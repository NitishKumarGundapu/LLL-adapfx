Available number of GPU = 2 < n_gpus = 12
Continue training with 2 GPUs
2023-07-28 14:23:04,915 - 0:00:08 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[6, 14], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[31457.28, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=2, n_train_epochs={'sst': 10, 'srl': 10, 'woz.en': 10}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[11010, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[11010, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-07-28 14:23:04,916 - 0:00:08 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-07-28 14:23:04,916 - 0:00:08 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-28 14:23:10,967 - 0:00:14 - 6.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
[0]
2023-07-28 14:23:27,581 - 0:00:31 - 16.6s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 69200
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-07-28 14:24:44,303 - 0:01:47 - 76.7s - INFO - __main__ - progress 0.578 , lr 5.9E-05 , loss 3.566 , qa loss 3.566 , lm loss 0.000 , avg batch size 4.0
2023-07-28 14:25:25,300 - 0:02:28 - 41.0s - INFO - __main__ - epoch 1/10 done , tot steps 1730 , lr 5.6E-05 , loss 2.21 , qa loss 2.21 , lm loss 0.00 , avg batch size 4.0
2023-07-28 14:26:32,579 - 0:03:36 - 67.3s - INFO - __main__ - progress 1.578 , lr 5.3E-05 , loss 0.343 , qa loss 0.343 , lm loss 0.000 , avg batch size 4.0
2023-07-28 14:27:18,557 - 0:04:22 - 46.0s - INFO - __main__ - epoch 2/10 done , tot steps 3460 , lr 5.0E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-07-28 14:28:39,498 - 0:05:43 - 80.9s - INFO - __main__ - progress 2.578 , lr 4.6E-05 , loss 0.338 , qa loss 0.338 , lm loss 0.000 , avg batch size 4.0
2023-07-28 14:29:28,960 - 0:06:32 - 49.5s - INFO - __main__ - epoch 3/10 done , tot steps 5190 , lr 4.4E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-07-28 14:30:47,831 - 0:07:51 - 78.9s - INFO - __main__ - progress 3.578 , lr 4.0E-05 , loss 0.321 , qa loss 0.321 , lm loss 0.000 , avg batch size 4.0
2023-07-28 14:31:36,787 - 0:08:40 - 49.0s - INFO - __main__ - epoch 4/10 done , tot steps 6920 , lr 3.8E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-07-28 14:32:52,958 - 0:09:56 - 76.2s - INFO - __main__ - progress 4.578 , lr 3.4E-05 , loss 0.330 , qa loss 0.330 , lm loss 0.000 , avg batch size 4.0
2023-07-28 14:33:40,333 - 0:10:43 - 47.4s - INFO - __main__ - epoch 5/10 done , tot steps 8650 , lr 3.1E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-07-28 14:34:57,391 - 0:12:00 - 77.1s - INFO - __main__ - progress 5.578 , lr 2.8E-05 , loss 0.316 , qa loss 0.316 , lm loss 0.000 , avg batch size 4.0
2023-07-28 14:35:42,213 - 0:12:45 - 44.8s - INFO - __main__ - epoch 6/10 done , tot steps 10380 , lr 2.5E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-07-28 14:36:59,762 - 0:14:03 - 77.5s - INFO - __main__ - progress 6.578 , lr 2.1E-05 , loss 0.316 , qa loss 0.316 , lm loss 0.000 , avg batch size 4.0
2023-07-28 14:37:47,492 - 0:14:51 - 47.7s - INFO - __main__ - epoch 7/10 done , tot steps 12110 , lr 1.9E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-07-28 14:39:03,944 - 0:16:07 - 76.5s - INFO - __main__ - progress 7.578 , lr 1.5E-05 , loss 0.314 , qa loss 0.314 , lm loss 0.000 , avg batch size 4.0
2023-07-28 14:39:51,529 - 0:16:55 - 47.6s - INFO - __main__ - epoch 8/10 done , tot steps 13840 , lr 1.3E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-07-28 14:41:07,866 - 0:18:11 - 76.3s - INFO - __main__ - progress 8.578 , lr 8.9E-06 , loss 0.315 , qa loss 0.315 , lm loss 0.000 , avg batch size 4.0
2023-07-28 14:41:54,717 - 0:18:58 - 46.9s - INFO - __main__ - epoch 9/10 done , tot steps 15570 , lr 6.3E-06 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-07-28 14:43:10,317 - 0:20:13 - 75.6s - INFO - __main__ - progress 9.578 , lr 2.7E-06 , loss 0.303 , qa loss 0.303 , lm loss 0.000 , avg batch size 4.0
2023-07-28 14:43:48,470 - 0:20:52 - 38.2s - INFO - __main__ - epoch 10/10 done , tot steps 17300 , lr 3.1E-08 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-07-28 14:43:49,669 - 0:20:53 - 1.2s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-07-28 14:43:49,669 - 0:20:53 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-28 14:43:49,830 - 0:20:53 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-07-28 14:44:01,049 - 0:21:04 - 11.2s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 64140
2023-07-28 14:45:38,879 - 0:22:42 - 97.8s - INFO - __main__ - progress 0.624 , lr 5.9E-05 , loss 5.894 , qa loss 5.894 , lm loss 0.000 , avg batch size 4.0
2023-07-28 14:46:25,814 - 0:23:29 - 46.9s - INFO - __main__ - epoch 1/10 done , tot steps 1604 , lr 5.6E-05 , loss 4.43 , qa loss 4.43 , lm loss 0.00 , avg batch size 4.0
2023-07-28 14:48:05,103 - 0:25:08 - 99.3s - INFO - __main__ - progress 1.624 , lr 5.2E-05 , loss 1.939 , qa loss 1.939 , lm loss 0.000 , avg batch size 4.0
2023-07-28 14:48:58,993 - 0:26:02 - 53.9s - INFO - __main__ - epoch 2/10 done , tot steps 3208 , lr 5.0E-05 , loss 1.92 , qa loss 1.92 , lm loss 0.00 , avg batch size 4.0
2023-07-28 14:50:40,090 - 0:27:43 - 101.1s - INFO - __main__ - progress 2.624 , lr 4.6E-05 , loss 1.851 , qa loss 1.851 , lm loss 0.000 , avg batch size 4.0
2023-07-28 14:51:35,393 - 0:28:38 - 55.3s - INFO - __main__ - epoch 3/10 done , tot steps 4812 , lr 4.4E-05 , loss 1.85 , qa loss 1.85 , lm loss 0.00 , avg batch size 4.0
2023-07-28 14:53:19,046 - 0:30:22 - 103.7s - INFO - __main__ - progress 3.624 , lr 4.0E-05 , loss 1.776 , qa loss 1.776 , lm loss 0.000 , avg batch size 4.0
2023-07-28 14:54:12,050 - 0:31:15 - 53.0s - INFO - __main__ - epoch 4/10 done , tot steps 6416 , lr 3.8E-05 , loss 1.80 , qa loss 1.80 , lm loss 0.00 , avg batch size 4.0
2023-07-28 14:55:54,602 - 0:32:58 - 102.6s - INFO - __main__ - progress 4.624 , lr 3.4E-05 , loss 1.788 , qa loss 1.788 , lm loss 0.000 , avg batch size 4.0
2023-07-28 14:56:48,208 - 0:33:51 - 53.6s - INFO - __main__ - epoch 5/10 done , tot steps 8020 , lr 3.1E-05 , loss 1.79 , qa loss 1.79 , lm loss 0.00 , avg batch size 4.0
2023-07-28 14:58:30,630 - 0:35:34 - 102.4s - INFO - __main__ - progress 5.624 , lr 2.7E-05 , loss 1.759 , qa loss 1.759 , lm loss 0.000 , avg batch size 4.0
2023-07-28 14:59:36,481 - 0:36:40 - 65.9s - INFO - __main__ - epoch 6/10 done , tot steps 9624 , lr 2.5E-05 , loss 1.76 , qa loss 1.76 , lm loss 0.00 , avg batch size 4.0
2023-07-28 15:01:22,136 - 0:38:25 - 105.7s - INFO - __main__ - progress 6.624 , lr 2.1E-05 , loss 1.727 , qa loss 1.727 , lm loss 0.000 , avg batch size 4.0
2023-07-28 15:02:25,786 - 0:39:29 - 63.7s - INFO - __main__ - epoch 7/10 done , tot steps 11228 , lr 1.9E-05 , loss 1.73 , qa loss 1.73 , lm loss 0.00 , avg batch size 4.0
2023-07-28 15:04:00,937 - 0:41:04 - 95.2s - INFO - __main__ - progress 7.624 , lr 1.5E-05 , loss 1.744 , qa loss 1.744 , lm loss 0.000 , avg batch size 4.0
2023-07-28 15:04:49,532 - 0:41:53 - 48.6s - INFO - __main__ - epoch 8/10 done , tot steps 12832 , lr 1.3E-05 , loss 1.74 , qa loss 1.74 , lm loss 0.00 , avg batch size 4.0
2023-07-28 15:06:19,221 - 0:43:22 - 89.7s - INFO - __main__ - progress 8.624 , lr 8.6E-06 , loss 1.730 , qa loss 1.730 , lm loss 0.000 , avg batch size 4.0
2023-07-28 15:07:06,434 - 0:44:10 - 47.2s - INFO - __main__ - epoch 9/10 done , tot steps 14436 , lr 6.3E-06 , loss 1.72 , qa loss 1.72 , lm loss 0.00 , avg batch size 4.0
2023-07-28 15:08:48,711 - 0:45:52 - 102.3s - INFO - __main__ - progress 9.624 , lr 2.4E-06 , loss 1.703 , qa loss 1.703 , lm loss 0.000 , avg batch size 4.0
2023-07-28 15:09:47,200 - 0:46:50 - 58.5s - INFO - __main__ - epoch 10/10 done , tot steps 16040 , lr 3.1E-08 , loss 1.70 , qa loss 1.70 , lm loss 0.00 , avg batch size 4.0
2023-07-28 15:09:48,450 - 0:46:52 - 1.3s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-07-28 15:09:48,451 - 0:46:52 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-28 15:09:48,625 - 0:46:52 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-07-28 15:09:57,579 - 0:47:01 - 9.0s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 25360
2023-07-28 15:10:55,686 - 0:47:59 - 58.1s - INFO - __main__ - epoch 1/10 done , tot steps 634 , lr 5.6E-05 , loss 7.02 , qa loss 7.02 , lm loss 0.00 , avg batch size 4.0
2023-07-28 15:12:00,298 - 0:49:03 - 64.6s - INFO - __main__ - epoch 2/10 done , tot steps 1268 , lr 5.0E-05 , loss 1.33 , qa loss 1.33 , lm loss 0.00 , avg batch size 4.0
2023-07-28 15:13:01,011 - 0:50:04 - 60.7s - INFO - __main__ - epoch 3/10 done , tot steps 1902 , lr 4.4E-05 , loss 1.08 , qa loss 1.08 , lm loss 0.00 , avg batch size 4.0
2023-07-28 15:14:02,639 - 0:51:06 - 61.6s - INFO - __main__ - epoch 4/10 done , tot steps 2536 , lr 3.8E-05 , loss 0.94 , qa loss 0.94 , lm loss 0.00 , avg batch size 4.0
2023-07-28 15:15:03,646 - 0:52:07 - 61.0s - INFO - __main__ - epoch 5/10 done , tot steps 3170 , lr 3.1E-05 , loss 0.86 , qa loss 0.86 , lm loss 0.00 , avg batch size 4.0
2023-07-28 15:16:03,331 - 0:53:06 - 59.7s - INFO - __main__ - epoch 6/10 done , tot steps 3804 , lr 2.5E-05 , loss 0.80 , qa loss 0.80 , lm loss 0.00 , avg batch size 4.0
2023-07-28 15:17:08,929 - 0:54:12 - 65.6s - INFO - __main__ - epoch 7/10 done , tot steps 4438 , lr 1.9E-05 , loss 0.76 , qa loss 0.76 , lm loss 0.00 , avg batch size 4.0
2023-07-28 15:18:11,804 - 0:55:15 - 62.9s - INFO - __main__ - epoch 8/10 done , tot steps 5072 , lr 1.3E-05 , loss 0.73 , qa loss 0.73 , lm loss 0.00 , avg batch size 4.0
2023-07-28 15:19:12,865 - 0:56:16 - 61.1s - INFO - __main__ - epoch 9/10 done , tot steps 5706 , lr 6.3E-06 , loss 0.70 , qa loss 0.70 , lm loss 0.00 , avg batch size 4.0
2023-07-28 15:20:14,467 - 0:57:18 - 61.6s - INFO - __main__ - epoch 10/10 done , tot steps 6340 , lr 3.0E-08 , loss 0.70 , qa loss 0.70 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 00:57:10
CPU Execution time: 00:50:47
