................................................................................................................................
 Training Adapter Fusion at rf 4
................................................................................................................................
Available number of GPU = 10 < n_gpus = 12
Continue training with 10 GPUs
2023-08-14 18:39:59,342 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0, 1, 2, 5, 6, 7, 8, 12, 14, 15], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='0,1,2,3,4,5,6,7,8,9,10', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[20971.52, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=10, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[7340, 11927, 11927, 11927, 11927, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[7340, 11927, 11927, 11927, 11927, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-14 18:39:59,342 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-14 18:39:59,342 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-14 18:40:01,715 - 0:00:07 - 2.4s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
The Flat  : True

[0]
2023-08-14 18:40:12,335 - 0:00:17 - 10.6s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union               591,744       0.476       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-14 18:41:08,205 - 0:01:13 - 55.9s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 4.262 , qa loss 4.262 , lm loss 0.000 , avg batch size 4.0
2023-08-14 18:41:40,215 - 0:01:45 - 32.0s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 2.61 , qa loss 2.61 , lm loss 0.00 , avg batch size 4.0
2023-08-14 18:42:36,963 - 0:02:42 - 56.7s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.320 , qa loss 0.320 , lm loss 0.000 , avg batch size 4.0
2023-08-14 18:43:08,435 - 0:03:13 - 31.5s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-14 18:44:05,712 - 0:04:11 - 57.3s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.312 , qa loss 0.312 , lm loss 0.000 , avg batch size 4.0
2023-08-14 18:44:37,464 - 0:04:42 - 31.8s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-14 18:45:35,145 - 0:05:40 - 57.7s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.305 , qa loss 0.305 , lm loss 0.000 , avg batch size 4.0
2023-08-14 18:46:05,771 - 0:06:11 - 30.6s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-14 18:47:03,515 - 0:07:08 - 57.7s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.287 , qa loss 0.287 , lm loss 0.000 , avg batch size 4.0
2023-08-14 18:47:34,720 - 0:07:40 - 31.2s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-14 18:48:32,109 - 0:08:37 - 57.4s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.288 , qa loss 0.288 , lm loss 0.000 , avg batch size 4.0
2023-08-14 18:49:03,426 - 0:09:08 - 31.3s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-14 18:50:02,724 - 0:10:08 - 59.3s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.288 , qa loss 0.288 , lm loss 0.000 , avg batch size 4.0
2023-08-14 18:50:34,332 - 0:10:39 - 31.6s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-14 18:51:32,241 - 0:11:37 - 57.9s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.285 , qa loss 0.285 , lm loss 0.000 , avg batch size 4.0
2023-08-14 18:52:03,367 - 0:12:08 - 31.1s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-14 18:53:00,502 - 0:13:05 - 57.1s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.278 , qa loss 0.278 , lm loss 0.000 , avg batch size 4.0
2023-08-14 18:53:31,242 - 0:13:36 - 30.7s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-14 18:54:29,095 - 0:14:34 - 57.9s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.287 , qa loss 0.287 , lm loss 0.000 , avg batch size 4.0
2023-08-14 18:54:59,965 - 0:15:05 - 30.9s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-14 18:55:57,096 - 0:16:02 - 57.1s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.281 , qa loss 0.281 , lm loss 0.000 , avg batch size 4.0
2023-08-14 18:56:28,309 - 0:16:33 - 31.2s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-14 18:57:25,356 - 0:17:30 - 57.0s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.281 , qa loss 0.281 , lm loss 0.000 , avg batch size 4.0
2023-08-14 18:57:56,682 - 0:18:01 - 31.3s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-14 18:58:54,157 - 0:18:59 - 57.5s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.278 , qa loss 0.278 , lm loss 0.000 , avg batch size 4.0
2023-08-14 18:59:24,862 - 0:19:30 - 30.7s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:00:21,442 - 0:20:26 - 56.6s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.279 , qa loss 0.279 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:00:52,750 - 0:20:58 - 31.3s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:01:50,162 - 0:21:55 - 57.4s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.280 , qa loss 0.280 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:02:21,235 - 0:22:26 - 31.1s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:03:18,104 - 0:23:23 - 56.9s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.277 , qa loss 0.277 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:03:49,173 - 0:23:54 - 31.1s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:04:45,298 - 0:24:50 - 56.1s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.269 , qa loss 0.269 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:05:16,231 - 0:25:21 - 30.9s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:06:13,398 - 0:26:18 - 57.2s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.269 , qa loss 0.269 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:06:44,709 - 0:26:49 - 31.3s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:07:42,236 - 0:27:47 - 57.5s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.268 , qa loss 0.268 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:08:13,965 - 0:28:19 - 31.7s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:09:11,725 - 0:29:17 - 57.8s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.271 , qa loss 0.271 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:09:43,923 - 0:29:49 - 32.2s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:09:45,058 - 0:29:50 - 1.1s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-14 19:09:45,058 - 0:29:50 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-14 19:09:45,185 - 0:29:50 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union               591,744       0.476       1       1
srl                      union               591,744       0.476       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-14 19:09:53,939 - 0:29:59 - 8.8s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-14 19:11:18,807 - 0:31:24 - 84.9s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 5.964 , qa loss 5.964 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:12:01,731 - 0:32:07 - 42.9s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 4.45 , qa loss 4.45 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:13:27,944 - 0:33:33 - 86.2s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 1.853 , qa loss 1.853 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:14:10,134 - 0:34:15 - 42.2s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 1.82 , qa loss 1.82 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:15:34,515 - 0:35:39 - 84.4s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 1.733 , qa loss 1.733 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:16:17,241 - 0:36:22 - 42.7s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 1.74 , qa loss 1.74 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:17:42,572 - 0:37:47 - 85.3s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 1.669 , qa loss 1.669 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:18:24,735 - 0:38:30 - 42.2s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 1.67 , qa loss 1.67 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:19:49,755 - 0:39:55 - 85.0s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 1.639 , qa loss 1.639 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:20:32,656 - 0:40:37 - 42.9s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 1.63 , qa loss 1.63 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:21:56,956 - 0:42:02 - 84.3s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 1.607 , qa loss 1.607 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:22:39,292 - 0:42:44 - 42.3s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 1.60 , qa loss 1.60 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:24:04,380 - 0:44:09 - 85.1s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 1.542 , qa loss 1.542 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:24:47,553 - 0:44:52 - 43.2s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 1.55 , qa loss 1.55 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:26:12,471 - 0:46:17 - 84.9s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 1.521 , qa loss 1.521 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:26:55,596 - 0:47:00 - 43.1s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 1.53 , qa loss 1.53 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:28:20,243 - 0:48:25 - 84.6s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 1.487 , qa loss 1.487 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:29:02,193 - 0:49:07 - 42.0s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 1.49 , qa loss 1.49 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:30:27,386 - 0:50:32 - 85.2s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 1.487 , qa loss 1.487 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:31:10,241 - 0:51:15 - 42.9s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 1.49 , qa loss 1.49 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:32:35,326 - 0:52:40 - 85.1s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 1.456 , qa loss 1.456 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:33:18,970 - 0:53:24 - 43.6s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 1.45 , qa loss 1.45 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:34:42,990 - 0:54:48 - 84.0s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 1.471 , qa loss 1.471 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:35:26,428 - 0:55:31 - 43.4s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 1.45 , qa loss 1.45 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:36:52,713 - 0:56:58 - 86.3s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 1.418 , qa loss 1.418 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:37:34,121 - 0:57:39 - 41.4s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 1.42 , qa loss 1.42 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:38:59,420 - 0:59:04 - 85.3s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 1.417 , qa loss 1.417 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:39:41,177 - 0:59:46 - 41.8s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 1.41 , qa loss 1.41 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:41:03,951 - 1:01:09 - 82.8s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 1.402 , qa loss 1.402 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:41:47,383 - 1:01:52 - 43.4s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 1.40 , qa loss 1.40 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:43:13,725 - 1:03:19 - 86.3s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 1.405 , qa loss 1.405 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:43:56,057 - 1:04:01 - 42.3s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 1.41 , qa loss 1.41 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:45:21,441 - 1:05:26 - 85.4s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 1.385 , qa loss 1.385 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:46:04,085 - 1:06:09 - 42.6s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 1.39 , qa loss 1.39 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:47:28,440 - 1:07:33 - 84.4s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 1.373 , qa loss 1.373 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:48:12,357 - 1:08:17 - 43.9s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 1.38 , qa loss 1.38 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:49:37,219 - 1:09:42 - 84.9s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 1.373 , qa loss 1.373 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:50:20,289 - 1:10:25 - 43.1s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 1.37 , qa loss 1.37 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:51:45,294 - 1:11:50 - 85.0s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 1.380 , qa loss 1.380 , lm loss 0.000 , avg batch size 4.0
2023-08-14 19:52:28,945 - 1:12:34 - 43.7s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 1.38 , qa loss 1.38 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:52:29,902 - 1:12:35 - 1.0s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-14 19:52:29,903 - 1:12:35 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-14 19:52:30,029 - 1:12:35 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union               591,744       0.476       0       0
srl                      union               591,744       0.476       1       1
woz_en                   union               591,744       0.476       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-14 19:52:37,832 - 1:12:43 - 7.8s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-14 19:53:20,014 - 1:13:25 - 42.2s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 7.26 , qa loss 7.26 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:54:02,980 - 1:14:08 - 43.0s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.88 , qa loss 0.88 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:54:46,215 - 1:14:51 - 43.2s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.69 , qa loss 0.69 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:55:29,854 - 1:15:35 - 43.6s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:56:13,199 - 1:16:18 - 43.3s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.55 , qa loss 0.55 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:56:54,722 - 1:17:00 - 41.5s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:57:36,865 - 1:17:42 - 42.1s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:58:19,739 - 1:18:25 - 42.9s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:59:02,464 - 1:19:07 - 42.7s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-08-14 19:59:45,016 - 1:19:50 - 42.6s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:00:27,705 - 1:20:32 - 42.7s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:01:11,304 - 1:21:16 - 43.6s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:01:54,598 - 1:21:59 - 43.3s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:02:36,415 - 1:22:41 - 41.8s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:03:19,648 - 1:23:24 - 43.2s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:04:02,934 - 1:24:08 - 43.3s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:04:45,685 - 1:24:50 - 42.8s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:05:28,692 - 1:25:33 - 43.0s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:06:11,808 - 1:26:17 - 43.1s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:06:55,680 - 1:27:00 - 43.9s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:26:57
CPU Execution time: 01:13:13
................................................................................................................................
Training Adapter + Prefix at prefix length 30
................................................................................................................................
2023-08-14 20:07:02,717 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-14 20:07:13,747 - 0:00:15 - 11.0s - INFO - __main__ - task: sst, epoch: 20
2023-08-14 20:07:13,748 - 0:00:15 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-14 20:07:18,030 - 0:00:20 - 4.3s - INFO - __main__ - len of test dataset: 1821
2023-08-14 20:07:28,543 - 0:00:30 - 10.5s - INFO - __main__ - score: {'sst': OrderedDict([('em', 76.16694124107633), ('nf1', 76.16694124107633), ('nem', 76.16694124107633)]), 'srl': None, 'woz.en': None}
2023-08-14 20:07:38,960 - 0:00:40 - 10.4s - INFO - __main__ - task: srl, epoch: 20
2023-08-14 20:07:38,961 - 0:00:40 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-14 20:07:43,011 - 0:00:45 - 4.1s - INFO - __main__ - len of test dataset: 2201
2023-08-14 20:22:53,785 - 0:15:55 - 910.8s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 10.81326669695593), ('nf1', 23.90627411827871), ('nem', 12.630622444343482)]), 'woz.en': None}
2023-08-14 20:23:04,255 - 0:16:06 - 10.5s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-14 20:23:04,255 - 0:16:06 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-14 20:23:08,312 - 0:16:10 - 4.1s - INFO - __main__ - len of test dataset: 1646
2023-08-14 20:32:53,734 - 0:25:55 - 585.4s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 13.001215066828674), ('nf1', 84.58059086856157), ('nem', 66.82867557715674), ('joint_goal_em', 39.79343863912515), ('turn_request_em', 83.71810449574727), ('turn_goal_em', 74.726609963548), ('avg_dialogue', 61.75577156743621)])}
................................................................................................................................
 Training Adapter Fusion at rf 4
................................................................................................................................
Available number of GPU = 10 < n_gpus = 12
Continue training with 10 GPUs
2023-08-14 20:32:59,823 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0, 1, 2, 5, 6, 7, 8, 12, 14, 15], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='0,1,2,3,4,5,6,7', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[20971.52, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=10, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[7340, 11927, 11927, 11927, 11927, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[7340, 11927, 11927, 11927, 11927, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-14 20:32:59,823 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-14 20:32:59,823 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-14 20:33:02,464 - 0:00:07 - 2.6s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [0, 1, 2, 3, 4, 5, 6, 7]
The Flat  : True

[0]
2023-08-14 20:33:13,565 - 0:00:18 - 11.1s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             2,366,976       1.902       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-14 20:34:20,865 - 0:01:25 - 67.3s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.353 , qa loss 1.353 , lm loss 0.000 , avg batch size 4.0
2023-08-14 20:34:56,854 - 0:02:01 - 36.0s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 0.89 , qa loss 0.89 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:36:01,213 - 0:03:05 - 64.4s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.222 , qa loss 0.222 , lm loss 0.000 , avg batch size 4.0
2023-08-14 20:36:36,935 - 0:03:41 - 35.7s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:37:40,748 - 0:04:45 - 63.8s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.200 , qa loss 0.200 , lm loss 0.000 , avg batch size 4.0
2023-08-14 20:38:16,347 - 0:05:20 - 35.6s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:39:20,519 - 0:06:25 - 64.2s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.190 , qa loss 0.190 , lm loss 0.000 , avg batch size 4.0
2023-08-14 20:39:56,013 - 0:07:00 - 35.5s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:41:00,435 - 0:08:05 - 64.4s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.184 , qa loss 0.184 , lm loss 0.000 , avg batch size 4.0
2023-08-14 20:41:36,133 - 0:08:40 - 35.7s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:42:39,947 - 0:09:44 - 63.8s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.161 , qa loss 0.161 , lm loss 0.000 , avg batch size 4.0
2023-08-14 20:43:16,079 - 0:10:20 - 36.1s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:44:19,698 - 0:11:24 - 63.6s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.157 , qa loss 0.157 , lm loss 0.000 , avg batch size 4.0
2023-08-14 20:44:54,957 - 0:11:59 - 35.3s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:45:58,221 - 0:13:02 - 63.3s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.148 , qa loss 0.148 , lm loss 0.000 , avg batch size 4.0
2023-08-14 20:46:33,328 - 0:13:37 - 35.1s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:47:36,156 - 0:14:40 - 62.8s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.151 , qa loss 0.151 , lm loss 0.000 , avg batch size 4.0
2023-08-14 20:48:11,415 - 0:15:16 - 35.3s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:49:14,918 - 0:16:19 - 63.5s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.133 , qa loss 0.133 , lm loss 0.000 , avg batch size 4.0
2023-08-14 20:49:50,698 - 0:16:55 - 35.8s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:50:54,474 - 0:17:59 - 63.8s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.123 , qa loss 0.123 , lm loss 0.000 , avg batch size 4.0
2023-08-14 20:51:30,187 - 0:18:34 - 35.7s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:52:34,196 - 0:19:38 - 64.0s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.125 , qa loss 0.125 , lm loss 0.000 , avg batch size 4.0
2023-08-14 20:53:09,252 - 0:20:13 - 35.1s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:54:12,960 - 0:21:17 - 63.7s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.115 , qa loss 0.115 , lm loss 0.000 , avg batch size 4.0
2023-08-14 20:54:48,735 - 0:21:53 - 35.8s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:55:52,717 - 0:22:57 - 64.0s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.096 , qa loss 0.096 , lm loss 0.000 , avg batch size 4.0
2023-08-14 20:56:28,074 - 0:23:32 - 35.4s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:57:31,716 - 0:24:36 - 63.6s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.108 , qa loss 0.108 , lm loss 0.000 , avg batch size 4.0
2023-08-14 20:58:07,483 - 0:25:12 - 35.8s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-14 20:59:10,507 - 0:26:15 - 63.0s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.101 , qa loss 0.101 , lm loss 0.000 , avg batch size 4.0
2023-08-14 20:59:46,349 - 0:26:50 - 35.8s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:00:50,492 - 0:27:55 - 64.1s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.085 , qa loss 0.085 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:01:26,324 - 0:28:30 - 35.8s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:02:30,865 - 0:29:35 - 64.5s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.087 , qa loss 0.087 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:03:06,352 - 0:30:10 - 35.5s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:04:09,342 - 0:31:13 - 63.0s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.078 , qa loss 0.078 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:04:45,278 - 0:31:49 - 35.9s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:05:49,149 - 0:32:53 - 63.9s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.082 , qa loss 0.082 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:06:26,058 - 0:33:30 - 36.9s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:06:27,238 - 0:33:31 - 1.2s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-14 21:06:27,238 - 0:33:31 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-14 21:06:27,359 - 0:33:31 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             2,366,976       1.902       1       1
srl                      union             2,366,976       1.902       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-14 21:06:36,370 - 0:33:41 - 9.0s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-14 21:08:10,695 - 0:35:15 - 94.3s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 2.630 , qa loss 2.630 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:08:57,491 - 0:36:02 - 46.8s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.19 , qa loss 2.19 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:10:28,833 - 0:37:33 - 91.3s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 1.373 , qa loss 1.373 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:11:16,174 - 0:38:20 - 47.3s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 1.34 , qa loss 1.34 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:12:49,831 - 0:39:54 - 93.7s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 1.217 , qa loss 1.217 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:13:37,994 - 0:40:42 - 48.2s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 1.20 , qa loss 1.20 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:15:10,722 - 0:42:15 - 92.7s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 1.090 , qa loss 1.090 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:15:58,497 - 0:43:03 - 47.8s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 1.07 , qa loss 1.07 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:17:32,285 - 0:44:36 - 93.8s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 1.001 , qa loss 1.001 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:18:18,934 - 0:45:23 - 46.6s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.99 , qa loss 0.99 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:19:52,266 - 0:46:56 - 93.3s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.936 , qa loss 0.936 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:20:40,330 - 0:47:44 - 48.1s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.94 , qa loss 0.94 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:22:12,494 - 0:49:17 - 92.2s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.869 , qa loss 0.869 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:23:01,109 - 0:50:05 - 48.6s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.86 , qa loss 0.86 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:24:33,021 - 0:51:37 - 91.9s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.850 , qa loss 0.850 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:25:20,518 - 0:52:25 - 47.5s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.84 , qa loss 0.84 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:26:53,111 - 0:53:57 - 92.6s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.784 , qa loss 0.784 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:27:40,506 - 0:54:45 - 47.4s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.79 , qa loss 0.79 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:29:13,365 - 0:56:18 - 92.9s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.754 , qa loss 0.754 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:29:59,820 - 0:57:04 - 46.5s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.76 , qa loss 0.76 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:31:31,802 - 0:58:36 - 92.0s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.704 , qa loss 0.704 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:32:18,946 - 0:59:23 - 47.1s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.71 , qa loss 0.71 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:33:51,554 - 1:00:56 - 92.6s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.704 , qa loss 0.704 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:34:39,241 - 1:01:43 - 47.7s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.69 , qa loss 0.69 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:36:11,531 - 1:03:16 - 92.3s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.667 , qa loss 0.667 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:36:59,073 - 1:04:03 - 47.5s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.66 , qa loss 0.66 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:38:32,458 - 1:05:37 - 93.4s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.642 , qa loss 0.642 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:39:19,072 - 1:06:23 - 46.6s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.64 , qa loss 0.64 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:40:52,394 - 1:07:57 - 93.3s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.612 , qa loss 0.612 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:41:39,727 - 1:08:44 - 47.3s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.62 , qa loss 0.62 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:43:11,196 - 1:10:15 - 91.5s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.600 , qa loss 0.600 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:43:59,673 - 1:11:04 - 48.5s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.60 , qa loss 0.60 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:45:31,502 - 1:12:36 - 91.8s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.593 , qa loss 0.593 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:46:18,746 - 1:13:23 - 47.2s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:47:50,878 - 1:14:55 - 92.1s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.574 , qa loss 0.574 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:48:37,605 - 1:15:42 - 46.7s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:50:10,650 - 1:17:15 - 93.0s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.562 , qa loss 0.562 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:50:58,120 - 1:18:02 - 47.5s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:52:31,817 - 1:19:36 - 93.7s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.540 , qa loss 0.540 , lm loss 0.000 , avg batch size 4.0
2023-08-14 21:53:19,086 - 1:20:23 - 47.3s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.55 , qa loss 0.55 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:53:20,270 - 1:20:24 - 1.2s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-14 21:53:20,270 - 1:20:24 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-14 21:53:20,397 - 1:20:25 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             2,366,976       1.902       0       0
srl                      union             2,366,976       1.902       1       1
woz_en                   union             2,366,976       1.902       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-14 21:53:31,791 - 1:20:36 - 11.4s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-14 21:54:21,336 - 1:21:25 - 49.5s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 2.24 , qa loss 2.24 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:55:10,781 - 1:22:15 - 49.4s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:55:59,089 - 1:23:03 - 48.3s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:56:47,980 - 1:23:52 - 48.9s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:57:37,418 - 1:24:42 - 49.4s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:58:26,125 - 1:25:30 - 48.7s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-14 21:59:14,392 - 1:26:19 - 48.3s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:00:01,835 - 1:27:06 - 47.4s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:00:50,721 - 1:27:55 - 48.9s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:01:39,350 - 1:28:43 - 48.6s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:02:28,726 - 1:29:33 - 49.4s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:03:18,140 - 1:30:22 - 49.4s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:04:06,249 - 1:31:10 - 48.1s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:04:55,079 - 1:31:59 - 48.8s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:05:44,475 - 1:32:49 - 49.4s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:06:33,523 - 1:33:38 - 49.0s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:07:22,074 - 1:34:26 - 48.6s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:08:10,429 - 1:35:15 - 48.4s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:08:59,333 - 1:36:03 - 48.9s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:09:48,197 - 1:36:52 - 48.9s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:36:49
CPU Execution time: 01:22:41
................................................................................................................................
Training Adapter + Prefix at prefix length 30
................................................................................................................................
2023-08-14 22:09:55,624 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-14 22:10:06,141 - 0:00:15 - 10.5s - INFO - __main__ - task: sst, epoch: 20
2023-08-14 22:10:06,142 - 0:00:15 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-14 22:10:09,523 - 0:00:18 - 3.4s - INFO - __main__ - len of test dataset: 1821
2023-08-14 22:10:19,727 - 0:00:29 - 10.2s - INFO - __main__ - score: {'sst': OrderedDict([('em', 88.79736408566721), ('nf1', 88.79736408566721), ('nem', 88.79736408566721)]), 'srl': None, 'woz.en': None}
2023-08-14 22:10:30,481 - 0:00:39 - 10.8s - INFO - __main__ - task: srl, epoch: 20
2023-08-14 22:10:30,482 - 0:00:39 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-14 22:10:34,636 - 0:00:43 - 4.2s - INFO - __main__ - len of test dataset: 2201
2023-08-14 22:28:38,776 - 0:18:48 - 1084.1s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 29.62289868241708), ('nf1', 49.458598986081256), ('nem', 33.48477964561563)]), 'woz.en': None}
2023-08-14 22:28:49,226 - 0:18:58 - 10.4s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-14 22:28:49,226 - 0:18:58 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-14 22:28:53,366 - 0:19:02 - 4.1s - INFO - __main__ - len of test dataset: 1646
2023-08-14 22:38:07,105 - 0:28:16 - 553.7s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 16.767922235722963), ('nf1', 94.32714308777496), ('nem', 86.81652490886998), ('joint_goal_em', 84.32563791008505), ('turn_request_em', 92.40583232077765), ('turn_goal_em', 91.43377885783718), ('avg_dialogue', 88.36573511543135)])}
................................................................................................................................
 Training Adapter Fusion at rf 4
................................................................................................................................
Available number of GPU = 10 < n_gpus = 12
Continue training with 10 GPUs
2023-08-14 22:38:13,191 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0, 1, 2, 5, 6, 7, 8, 12, 14, 15], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='0,1,2,3,4,5', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[20971.52, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=10, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[7340, 11927, 11927, 11927, 11927, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[7340, 11927, 11927, 11927, 11927, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-14 22:38:13,191 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-14 22:38:13,191 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-14 22:38:16,286 - 0:00:07 - 3.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [0, 1, 2, 3, 4, 5]
The Flat  : True

[0]
2023-08-14 22:38:27,482 - 0:00:19 - 11.2s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-14 22:39:40,496 - 0:01:32 - 73.0s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.115 , qa loss 1.115 , lm loss 0.000 , avg batch size 4.0
2023-08-14 22:40:21,266 - 0:02:12 - 40.8s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 0.74 , qa loss 0.74 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:41:32,731 - 0:03:24 - 71.5s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.223 , qa loss 0.223 , lm loss 0.000 , avg batch size 4.0
2023-08-14 22:42:17,518 - 0:04:09 - 44.8s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:43:28,306 - 0:05:19 - 70.8s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.197 , qa loss 0.197 , lm loss 0.000 , avg batch size 4.0
2023-08-14 22:44:10,720 - 0:06:02 - 42.4s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:45:23,332 - 0:07:14 - 72.6s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.176 , qa loss 0.176 , lm loss 0.000 , avg batch size 4.0
2023-08-14 22:46:06,279 - 0:07:57 - 42.9s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:47:14,713 - 0:09:06 - 68.4s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.170 , qa loss 0.170 , lm loss 0.000 , avg batch size 4.0
2023-08-14 22:47:53,423 - 0:09:45 - 38.7s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:49:00,892 - 0:10:52 - 67.5s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.149 , qa loss 0.149 , lm loss 0.000 , avg batch size 4.0
2023-08-14 22:49:40,005 - 0:11:31 - 39.1s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:50:50,795 - 0:12:42 - 70.8s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.142 , qa loss 0.142 , lm loss 0.000 , avg batch size 4.0
2023-08-14 22:51:30,359 - 0:13:21 - 39.6s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:52:38,826 - 0:14:30 - 68.5s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.133 , qa loss 0.133 , lm loss 0.000 , avg batch size 4.0
2023-08-14 22:53:18,808 - 0:15:10 - 40.0s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:54:27,657 - 0:16:19 - 68.8s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.111 , qa loss 0.111 , lm loss 0.000 , avg batch size 4.0
2023-08-14 22:55:07,435 - 0:16:59 - 39.8s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:56:17,043 - 0:18:08 - 69.6s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.116 , qa loss 0.116 , lm loss 0.000 , avg batch size 4.0
2023-08-14 22:56:57,865 - 0:18:49 - 40.8s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:58:07,182 - 0:19:58 - 69.3s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.107 , qa loss 0.107 , lm loss 0.000 , avg batch size 4.0
2023-08-14 22:58:47,623 - 0:20:39 - 40.4s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-14 22:59:57,003 - 0:21:48 - 69.4s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.097 , qa loss 0.097 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:00:37,108 - 0:22:28 - 40.1s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:01:45,476 - 0:23:37 - 68.4s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.088 , qa loss 0.088 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:02:25,970 - 0:24:17 - 40.5s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:03:35,301 - 0:25:26 - 69.3s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.083 , qa loss 0.083 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:04:14,839 - 0:26:06 - 39.5s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:05:23,970 - 0:27:15 - 69.1s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.076 , qa loss 0.076 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:06:04,210 - 0:27:55 - 40.2s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:07:13,697 - 0:29:05 - 69.5s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.072 , qa loss 0.072 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:07:53,738 - 0:29:45 - 40.0s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:09:03,784 - 0:30:55 - 70.0s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.062 , qa loss 0.062 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:09:43,494 - 0:31:35 - 39.7s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:10:52,808 - 0:32:44 - 69.3s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.057 , qa loss 0.057 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:11:32,713 - 0:33:24 - 39.9s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:12:42,732 - 0:34:34 - 70.0s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.054 , qa loss 0.054 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:13:22,756 - 0:35:14 - 40.0s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:14:33,099 - 0:36:24 - 70.3s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.056 , qa loss 0.056 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:15:14,289 - 0:37:05 - 41.2s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:15:15,524 - 0:37:07 - 1.2s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-14 23:15:15,524 - 0:37:07 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-14 23:15:15,646 - 0:37:07 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             3,550,464       2.853       1       1
srl                      union             3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-14 23:15:25,026 - 0:37:16 - 9.4s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-14 23:17:03,304 - 0:38:54 - 98.3s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 2.178 , qa loss 2.178 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:17:57,199 - 0:39:48 - 53.9s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 1.83 , qa loss 1.83 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:19:38,052 - 0:41:29 - 100.9s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 1.081 , qa loss 1.081 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:20:29,691 - 0:42:21 - 51.6s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 1.05 , qa loss 1.05 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:22:10,019 - 0:44:01 - 100.3s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.899 , qa loss 0.899 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:23:02,555 - 0:44:54 - 52.5s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.90 , qa loss 0.90 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:24:41,658 - 0:46:33 - 99.1s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.804 , qa loss 0.804 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:25:34,970 - 0:47:26 - 53.3s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.79 , qa loss 0.79 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:27:15,147 - 0:49:06 - 100.2s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.728 , qa loss 0.728 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:28:07,519 - 0:49:59 - 52.4s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.73 , qa loss 0.73 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:29:45,778 - 0:51:37 - 98.3s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.666 , qa loss 0.666 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:30:38,547 - 0:52:30 - 52.8s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.66 , qa loss 0.66 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:32:17,925 - 0:54:09 - 99.4s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.612 , qa loss 0.612 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:33:09,168 - 0:55:00 - 51.2s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:34:49,136 - 0:56:40 - 100.0s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.554 , qa loss 0.554 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:35:40,727 - 0:57:32 - 51.6s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.55 , qa loss 0.55 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:37:19,857 - 0:59:11 - 99.1s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.524 , qa loss 0.524 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:38:11,395 - 1:00:03 - 51.5s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:39:48,851 - 1:01:40 - 97.5s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.472 , qa loss 0.472 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:40:41,179 - 1:02:32 - 52.3s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:42:20,033 - 1:04:11 - 98.9s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.452 , qa loss 0.452 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:43:12,488 - 1:05:04 - 52.5s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:44:51,493 - 1:06:43 - 99.0s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.430 , qa loss 0.430 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:45:44,815 - 1:07:36 - 53.3s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:47:24,768 - 1:09:16 - 100.0s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.386 , qa loss 0.386 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:48:16,978 - 1:10:08 - 52.2s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:49:54,441 - 1:11:46 - 97.5s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.369 , qa loss 0.369 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:50:47,183 - 1:12:38 - 52.7s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:52:26,779 - 1:14:18 - 99.6s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.353 , qa loss 0.353 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:53:19,084 - 1:15:10 - 52.3s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:54:59,974 - 1:16:51 - 100.9s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.329 , qa loss 0.329 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:55:51,817 - 1:17:43 - 51.8s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-14 23:57:29,580 - 1:19:21 - 97.8s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.331 , qa loss 0.331 , lm loss 0.000 , avg batch size 4.0
2023-08-14 23:58:21,969 - 1:20:13 - 52.4s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:00:01,049 - 1:21:52 - 99.1s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.316 , qa loss 0.316 , lm loss 0.000 , avg batch size 4.0
2023-08-15 00:00:52,738 - 1:22:44 - 51.7s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:02:32,845 - 1:24:24 - 100.1s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.310 , qa loss 0.310 , lm loss 0.000 , avg batch size 4.0
2023-08-15 00:03:25,243 - 1:25:16 - 52.4s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:05:06,472 - 1:26:58 - 101.2s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.301 , qa loss 0.301 , lm loss 0.000 , avg batch size 4.0
2023-08-15 00:05:58,054 - 1:27:49 - 51.6s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:05:59,359 - 1:27:50 - 1.3s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-15 00:05:59,360 - 1:27:50 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 00:05:59,490 - 1:27:51 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             3,550,464       2.853       0       0
srl                      union             3,550,464       2.853       1       1
woz_en                   union             3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-15 00:06:07,950 - 1:27:59 - 8.5s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-15 00:07:02,346 - 1:28:53 - 54.4s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 1.89 , qa loss 1.89 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:07:56,452 - 1:29:48 - 54.1s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:08:50,089 - 1:30:41 - 53.6s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:09:43,042 - 1:31:34 - 53.0s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:10:36,557 - 1:32:28 - 53.5s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:11:29,867 - 1:33:21 - 53.3s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:12:22,137 - 1:34:13 - 52.3s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:13:16,767 - 1:35:08 - 54.6s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:14:10,230 - 1:36:01 - 53.5s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:15:03,938 - 1:36:55 - 53.7s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:15:57,278 - 1:37:48 - 53.3s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:16:50,835 - 1:38:42 - 53.6s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:17:42,159 - 1:39:33 - 51.3s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:18:35,030 - 1:40:26 - 52.9s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:19:28,340 - 1:41:19 - 53.3s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:20:21,519 - 1:42:13 - 53.2s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:21:15,093 - 1:43:06 - 53.6s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:22:07,916 - 1:43:59 - 52.8s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:23:00,801 - 1:44:52 - 52.9s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:23:54,391 - 1:45:46 - 53.6s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:45:42
CPU Execution time: 01:33:01
................................................................................................................................
Training Adapter + Prefix at prefix length 30
................................................................................................................................
2023-08-15 00:24:02,182 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-15 00:24:14,017 - 0:00:16 - 11.8s - INFO - __main__ - task: sst, epoch: 20
2023-08-15 00:24:14,018 - 0:00:16 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-15 00:24:18,318 - 0:00:21 - 4.3s - INFO - __main__ - len of test dataset: 1821
2023-08-15 00:24:29,467 - 0:00:32 - 11.1s - INFO - __main__ - score: {'sst': OrderedDict([('em', 89.34651290499725), ('nf1', 89.34651290499725), ('nem', 89.34651290499725)]), 'srl': None, 'woz.en': None}
2023-08-15 00:24:40,649 - 0:00:43 - 11.2s - INFO - __main__ - task: srl, epoch: 20
2023-08-15 00:24:40,650 - 0:00:43 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-15 00:24:44,986 - 0:00:47 - 4.3s - INFO - __main__ - len of test dataset: 2201
2023-08-15 00:43:51,983 - 0:19:54 - 1147.0s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 38.70967741935484), ('nf1', 60.255634180870274), ('nem', 43.66197183098591)]), 'woz.en': None}
2023-08-15 00:44:02,969 - 0:20:05 - 11.0s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-15 00:44:02,970 - 0:20:05 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-15 00:44:07,227 - 0:20:10 - 4.3s - INFO - __main__ - len of test dataset: 1646
2023-08-15 00:54:01,958 - 0:30:04 - 594.7s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 17.071688942891857), ('nf1', 94.62232515695457), ('nem', 87.363304981774), ('joint_goal_em', 87.363304981774), ('turn_request_em', 92.34507897934387), ('turn_goal_em', 91.98055893074118), ('avg_dialogue', 89.85419198055894)])}
................................................................................................................................
 Training Adapter Fusion at rf 4
................................................................................................................................
Available number of GPU = 8 < n_gpus = 12
Continue training with 8 GPUs
2023-08-15 00:54:07,818 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0, 1, 2, 5, 6, 8, 12, 15], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='0,1,2,3', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[23592.96, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=8, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[8257, 11927, 11927, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[8257, 11927, 11927, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-15 00:54:07,819 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-15 00:54:07,819 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 00:54:10,765 - 0:00:07 - 2.9s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [0, 1, 2, 3]
The Flat  : True

[0]
2023-08-15 00:54:20,995 - 0:00:17 - 10.2s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             4,733,952       3.804       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-15 00:55:28,045 - 0:01:24 - 67.1s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 0.999 , qa loss 0.999 , lm loss 0.000 , avg batch size 4.0
2023-08-15 00:56:08,256 - 0:02:04 - 40.2s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 0.69 , qa loss 0.69 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:57:19,691 - 0:03:16 - 71.4s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.215 , qa loss 0.215 , lm loss 0.000 , avg batch size 4.0
2023-08-15 00:57:59,944 - 0:03:56 - 40.3s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-15 00:59:10,891 - 0:05:07 - 70.9s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.189 , qa loss 0.189 , lm loss 0.000 , avg batch size 4.0
2023-08-15 00:59:51,222 - 0:05:47 - 40.3s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:01:01,736 - 0:06:58 - 70.5s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.164 , qa loss 0.164 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:01:42,004 - 0:07:38 - 40.3s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:02:51,911 - 0:08:48 - 69.9s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.147 , qa loss 0.147 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:03:33,380 - 0:09:29 - 41.5s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:04:43,818 - 0:10:40 - 70.4s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.131 , qa loss 0.131 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:05:24,102 - 0:11:20 - 40.3s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:06:34,343 - 0:12:30 - 70.2s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.126 , qa loss 0.126 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:07:14,693 - 0:13:11 - 40.3s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:08:25,602 - 0:14:22 - 70.9s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.096 , qa loss 0.096 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:09:05,550 - 0:15:02 - 39.9s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:10:15,511 - 0:16:12 - 70.0s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.095 , qa loss 0.095 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:10:55,140 - 0:16:51 - 39.6s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:12:05,473 - 0:18:02 - 70.3s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.087 , qa loss 0.087 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:12:45,778 - 0:18:42 - 40.3s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:13:56,690 - 0:19:53 - 70.9s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.070 , qa loss 0.070 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:14:37,242 - 0:20:33 - 40.6s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:15:48,754 - 0:21:45 - 71.5s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.065 , qa loss 0.065 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:16:29,906 - 0:22:26 - 41.2s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:17:41,241 - 0:23:37 - 71.3s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.058 , qa loss 0.058 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:18:21,450 - 0:24:18 - 40.2s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:19:32,171 - 0:25:28 - 70.7s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.055 , qa loss 0.055 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:20:12,566 - 0:26:09 - 40.4s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:21:23,591 - 0:27:20 - 71.0s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.045 , qa loss 0.045 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:22:04,109 - 0:28:00 - 40.5s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:23:15,391 - 0:29:11 - 71.3s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.048 , qa loss 0.048 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:23:56,035 - 0:29:52 - 40.6s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:25:06,516 - 0:31:03 - 70.5s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.037 , qa loss 0.037 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:25:46,704 - 0:31:43 - 40.2s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:26:57,480 - 0:32:54 - 70.8s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.034 , qa loss 0.034 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:27:37,749 - 0:33:34 - 40.3s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:28:49,759 - 0:34:46 - 72.0s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.038 , qa loss 0.038 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:29:34,402 - 0:35:30 - 44.6s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:30:46,136 - 0:36:42 - 71.7s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.026 , qa loss 0.026 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:31:27,813 - 0:37:24 - 41.7s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:31:28,970 - 0:37:25 - 1.2s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-15 01:31:28,970 - 0:37:25 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 01:31:29,099 - 0:37:25 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             4,733,952       3.804       1       1
srl                      union             4,733,952       3.804       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-15 01:31:37,221 - 0:37:33 - 8.1s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-15 01:33:19,646 - 0:39:16 - 102.4s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 2.074 , qa loss 2.074 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:34:13,127 - 0:40:09 - 53.5s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 1.70 , qa loss 1.70 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:35:57,281 - 0:41:53 - 104.2s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.911 , qa loss 0.911 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:36:49,656 - 0:42:46 - 52.4s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.88 , qa loss 0.88 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:38:35,271 - 0:44:31 - 105.6s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.731 , qa loss 0.731 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:39:28,862 - 0:45:25 - 53.6s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.71 , qa loss 0.71 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:41:12,852 - 0:47:09 - 104.0s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.627 , qa loss 0.627 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:42:06,332 - 0:48:02 - 53.5s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:43:50,381 - 0:49:46 - 104.0s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.557 , qa loss 0.557 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:44:43,921 - 0:50:40 - 53.5s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.55 , qa loss 0.55 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:46:26,997 - 0:52:23 - 103.1s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.499 , qa loss 0.499 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:47:21,248 - 0:53:17 - 54.3s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:49:06,597 - 0:55:03 - 105.3s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.431 , qa loss 0.431 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:49:59,865 - 0:55:56 - 53.3s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:51:44,351 - 0:57:40 - 104.5s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.390 , qa loss 0.390 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:52:36,545 - 0:58:33 - 52.2s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:54:19,176 - 1:00:15 - 102.6s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.361 , qa loss 0.361 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:55:14,390 - 1:01:10 - 55.2s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:56:59,539 - 1:02:56 - 105.1s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.315 , qa loss 0.315 , lm loss 0.000 , avg batch size 4.0
2023-08-15 01:57:52,965 - 1:03:49 - 53.4s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-15 01:59:38,347 - 1:05:34 - 105.4s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.294 , qa loss 0.294 , lm loss 0.000 , avg batch size 4.0
2023-08-15 02:00:32,062 - 1:06:28 - 53.7s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:02:16,825 - 1:08:13 - 104.8s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.270 , qa loss 0.270 , lm loss 0.000 , avg batch size 4.0
2023-08-15 02:03:09,714 - 1:09:06 - 52.9s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:04:56,512 - 1:10:53 - 106.8s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.263 , qa loss 0.263 , lm loss 0.000 , avg batch size 4.0
2023-08-15 02:05:49,041 - 1:11:45 - 52.5s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:07:34,449 - 1:13:31 - 105.4s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.235 , qa loss 0.235 , lm loss 0.000 , avg batch size 4.0
2023-08-15 02:08:28,380 - 1:14:24 - 53.9s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:10:13,512 - 1:16:10 - 105.1s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.220 , qa loss 0.220 , lm loss 0.000 , avg batch size 4.0
2023-08-15 02:11:07,067 - 1:17:03 - 53.6s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:12:54,207 - 1:18:50 - 107.1s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.201 , qa loss 0.201 , lm loss 0.000 , avg batch size 4.0
2023-08-15 02:13:46,714 - 1:19:43 - 52.5s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:15:30,660 - 1:21:27 - 103.9s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.184 , qa loss 0.184 , lm loss 0.000 , avg batch size 4.0
2023-08-15 02:16:25,047 - 1:22:21 - 54.4s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:18:09,996 - 1:24:06 - 104.9s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.177 , qa loss 0.177 , lm loss 0.000 , avg batch size 4.0
2023-08-15 02:19:03,984 - 1:25:00 - 54.0s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:20:49,155 - 1:26:45 - 105.2s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.167 , qa loss 0.167 , lm loss 0.000 , avg batch size 4.0
2023-08-15 02:21:43,019 - 1:27:39 - 53.9s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:23:25,028 - 1:29:21 - 102.0s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.161 , qa loss 0.161 , lm loss 0.000 , avg batch size 4.0
2023-08-15 02:24:19,263 - 1:30:15 - 54.2s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:24:20,473 - 1:30:17 - 1.2s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-15 02:24:20,474 - 1:30:17 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 02:24:20,597 - 1:30:17 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             4,733,952       3.804       0       0
srl                      union             4,733,952       3.804       1       1
woz_en                   union             4,733,952       3.804       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-15 02:24:27,790 - 1:30:24 - 7.2s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-15 02:25:25,088 - 1:31:21 - 57.3s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 1.79 , qa loss 1.79 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:26:21,996 - 1:32:18 - 56.9s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:27:18,015 - 1:33:14 - 56.0s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:28:15,174 - 1:34:11 - 57.2s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:29:11,579 - 1:35:08 - 56.4s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:30:07,371 - 1:36:03 - 55.8s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:31:03,689 - 1:37:00 - 56.3s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:32:00,189 - 1:37:56 - 56.5s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:32:57,468 - 1:38:54 - 57.3s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:33:54,312 - 1:39:50 - 56.8s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:34:50,624 - 1:40:47 - 56.3s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:35:46,884 - 1:41:43 - 56.3s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:36:42,545 - 1:42:39 - 55.7s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:37:38,440 - 1:43:35 - 55.9s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:38:33,748 - 1:44:30 - 55.3s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:39:29,955 - 1:45:26 - 56.2s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:40:25,716 - 1:46:22 - 55.8s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:41:21,149 - 1:47:17 - 55.4s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:42:16,879 - 1:48:13 - 55.7s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 02:43:13,037 - 1:49:09 - 56.2s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:49:06
CPU Execution time: 01:34:23
................................................................................................................................
Training Adapter + Prefix at prefix length 30
................................................................................................................................
2023-08-15 02:43:21,569 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-15 02:43:33,110 - 0:00:17 - 11.5s - INFO - __main__ - task: sst, epoch: 20
2023-08-15 02:43:33,112 - 0:00:17 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-15 02:43:37,556 - 0:00:21 - 4.4s - INFO - __main__ - len of test dataset: 1821
2023-08-15 02:43:48,806 - 0:00:33 - 11.3s - INFO - __main__ - score: {'sst': OrderedDict([('em', 90.77429983525536), ('nf1', 90.77429983525536), ('nem', 90.77429983525536)]), 'srl': None, 'woz.en': None}
2023-08-15 02:44:00,013 - 0:00:44 - 11.2s - INFO - __main__ - task: srl, epoch: 20
2023-08-15 02:44:00,014 - 0:00:44 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-15 02:44:04,130 - 0:00:48 - 4.1s - INFO - __main__ - len of test dataset: 2201
2023-08-15 03:06:10,215 - 0:22:54 - 1326.1s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 44.161744661517496), ('nf1', 65.39835107796296), ('nem', 50.02271694684235)]), 'woz.en': None}
2023-08-15 03:06:21,208 - 0:23:05 - 11.0s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-15 03:06:21,209 - 0:23:05 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-15 03:06:25,368 - 0:23:09 - 4.2s - INFO - __main__ - len of test dataset: 1646
2023-08-15 03:17:05,424 - 0:33:49 - 640.1s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 16.950182260024302), ('nf1', 94.22802895767657), ('nem', 86.81652490886998), ('joint_goal_em', 85.47995139732684), ('turn_request_em', 92.46658566221141), ('turn_goal_em', 91.31227217496962), ('avg_dialogue', 88.97326852976913)])}
................................................................................................................................
 Training Adapter Fusion at rf 4
................................................................................................................................
Available number of GPU = 7 < n_gpus = 12
Continue training with 7 GPUs
2023-08-15 03:17:12,394 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0, 1, 2, 5, 8, 12, 15], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='4,5,6,7,8,9,10,11', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[24903.68, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=7, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-15 03:17:12,395 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-15 03:17:12,395 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 03:17:15,223 - 0:00:08 - 2.8s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [4, 5, 6, 7, 8, 9, 10, 11]
The Flat  : True

[0]
2023-08-15 03:17:26,283 - 0:00:19 - 11.1s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             2,366,976       1.902       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-15 03:18:35,965 - 0:01:29 - 69.7s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.653 , qa loss 1.653 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:19:14,285 - 0:02:07 - 38.3s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.07 , qa loss 1.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:20:21,595 - 0:03:14 - 67.3s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.241 , qa loss 0.241 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:20:59,654 - 0:03:52 - 38.1s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:22:07,282 - 0:05:00 - 67.6s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.212 , qa loss 0.212 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:22:45,543 - 0:05:38 - 38.3s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:23:52,742 - 0:06:45 - 67.2s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.169 , qa loss 0.169 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:24:30,408 - 0:07:23 - 37.7s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:25:38,059 - 0:08:31 - 67.7s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.151 , qa loss 0.151 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:26:16,048 - 0:09:09 - 38.0s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:27:23,874 - 0:10:17 - 67.8s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.155 , qa loss 0.155 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:28:02,747 - 0:10:55 - 38.9s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:29:10,020 - 0:12:03 - 67.3s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.119 , qa loss 0.119 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:29:47,975 - 0:12:41 - 38.0s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:30:54,723 - 0:13:47 - 66.7s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.105 , qa loss 0.105 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:31:32,608 - 0:14:25 - 37.9s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:32:39,626 - 0:15:32 - 67.0s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.096 , qa loss 0.096 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:33:17,859 - 0:16:11 - 38.2s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:34:25,526 - 0:17:18 - 67.7s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.078 , qa loss 0.078 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:35:03,280 - 0:17:56 - 37.8s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:36:10,586 - 0:19:03 - 67.3s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.074 , qa loss 0.074 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:36:48,785 - 0:19:41 - 38.2s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:37:56,872 - 0:20:50 - 68.1s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.061 , qa loss 0.061 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:38:35,074 - 0:21:28 - 38.2s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:39:43,217 - 0:22:36 - 68.1s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.064 , qa loss 0.064 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:40:21,118 - 0:23:14 - 37.9s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:41:28,318 - 0:24:21 - 67.2s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.044 , qa loss 0.044 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:42:06,520 - 0:24:59 - 38.2s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:43:13,935 - 0:26:07 - 67.4s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.056 , qa loss 0.056 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:43:52,026 - 0:26:45 - 38.1s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:45:00,083 - 0:27:53 - 68.1s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.043 , qa loss 0.043 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:45:37,886 - 0:28:31 - 37.8s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:46:46,028 - 0:29:39 - 68.1s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.031 , qa loss 0.031 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:47:24,363 - 0:30:17 - 38.3s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:48:32,944 - 0:31:26 - 68.6s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.027 , qa loss 0.027 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:49:11,230 - 0:32:04 - 38.3s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:50:18,414 - 0:33:11 - 67.2s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.025 , qa loss 0.025 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:50:56,466 - 0:33:49 - 38.1s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:52:04,137 - 0:34:57 - 67.7s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.021 , qa loss 0.021 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:52:42,542 - 0:35:35 - 38.4s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:52:43,679 - 0:35:36 - 1.1s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-15 03:52:43,679 - 0:35:36 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 03:52:43,834 - 0:35:37 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             2,366,976       1.902       1       1
srl                      union             2,366,976       1.902       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-15 03:52:52,396 - 0:35:45 - 8.6s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-15 03:54:30,182 - 0:37:23 - 97.8s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 2.741 , qa loss 2.741 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:55:19,557 - 0:38:12 - 49.4s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.11 , qa loss 2.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:56:57,917 - 0:39:51 - 98.4s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.869 , qa loss 0.869 , lm loss 0.000 , avg batch size 4.0
2023-08-15 03:57:47,943 - 0:40:41 - 50.0s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.86 , qa loss 0.86 , lm loss 0.00 , avg batch size 4.0
2023-08-15 03:59:23,256 - 0:42:16 - 95.3s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.752 , qa loss 0.752 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:00:14,945 - 0:43:08 - 51.7s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.73 , qa loss 0.73 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:01:52,786 - 0:44:45 - 97.8s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.634 , qa loss 0.634 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:02:42,511 - 0:45:35 - 49.7s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.64 , qa loss 0.64 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:04:20,917 - 0:47:14 - 98.4s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.583 , qa loss 0.583 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:05:11,973 - 0:48:05 - 51.1s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:06:50,151 - 0:49:43 - 98.2s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.545 , qa loss 0.545 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:07:40,298 - 0:50:33 - 50.1s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.55 , qa loss 0.55 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:09:18,277 - 0:52:11 - 98.0s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.510 , qa loss 0.510 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:10:09,047 - 0:53:02 - 50.8s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:11:47,721 - 0:54:40 - 98.7s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.471 , qa loss 0.471 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:12:37,602 - 0:55:30 - 49.9s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:14:15,892 - 0:57:09 - 98.3s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.440 , qa loss 0.440 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:15:05,142 - 0:57:58 - 49.3s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:16:42,375 - 0:59:35 - 97.2s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.433 , qa loss 0.433 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:17:32,947 - 1:00:26 - 50.6s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:19:11,588 - 1:02:04 - 98.6s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.387 , qa loss 0.387 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:20:01,642 - 1:02:54 - 50.1s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:21:39,913 - 1:04:33 - 98.3s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.383 , qa loss 0.383 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:22:30,150 - 1:05:23 - 50.2s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:24:06,536 - 1:06:59 - 96.4s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.342 , qa loss 0.342 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:24:58,790 - 1:07:51 - 52.3s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:26:36,269 - 1:09:29 - 97.5s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.348 , qa loss 0.348 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:27:27,361 - 1:10:20 - 51.1s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:29:03,722 - 1:11:56 - 96.4s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.346 , qa loss 0.346 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:29:54,666 - 1:12:47 - 50.9s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:31:33,581 - 1:14:26 - 98.9s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.319 , qa loss 0.319 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:32:22,818 - 1:15:16 - 49.2s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:34:00,509 - 1:16:53 - 97.7s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.314 , qa loss 0.314 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:34:50,538 - 1:17:43 - 50.0s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:36:28,659 - 1:19:21 - 98.1s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.290 , qa loss 0.290 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:37:17,961 - 1:20:11 - 49.3s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:38:55,959 - 1:21:49 - 98.0s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.289 , qa loss 0.289 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:39:46,092 - 1:22:39 - 50.1s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:41:22,617 - 1:24:15 - 96.5s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.293 , qa loss 0.293 , lm loss 0.000 , avg batch size 4.0
2023-08-15 04:42:15,485 - 1:25:08 - 52.9s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:42:16,879 - 1:25:10 - 1.4s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-15 04:42:16,879 - 1:25:10 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 04:42:17,012 - 1:25:10 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             2,366,976       1.902       0       0
srl                      union             2,366,976       1.902       1       1
woz_en                   union             2,366,976       1.902       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-15 04:42:25,034 - 1:25:18 - 8.0s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-15 04:43:16,438 - 1:26:09 - 51.4s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 2.74 , qa loss 2.74 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:44:08,878 - 1:27:02 - 52.4s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:45:00,719 - 1:27:53 - 51.8s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:45:52,145 - 1:28:45 - 51.4s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:46:43,444 - 1:29:36 - 51.3s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:47:34,600 - 1:30:27 - 51.2s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:48:25,853 - 1:31:19 - 51.3s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:49:18,147 - 1:32:11 - 52.3s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:50:09,586 - 1:33:02 - 51.4s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:51:00,291 - 1:33:53 - 50.7s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:51:51,570 - 1:34:44 - 51.3s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:52:42,511 - 1:35:35 - 50.9s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:53:33,344 - 1:36:26 - 50.8s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:54:23,583 - 1:37:16 - 50.2s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:55:13,501 - 1:38:06 - 49.9s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:56:04,210 - 1:38:57 - 50.7s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:56:54,825 - 1:39:48 - 50.6s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:57:45,251 - 1:40:38 - 50.4s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:58:34,477 - 1:41:27 - 49.2s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 04:59:25,249 - 1:42:18 - 50.8s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:42:13
CPU Execution time: 01:28:16
................................................................................................................................
Training Adapter + Prefix at prefix length 30
................................................................................................................................
2023-08-15 04:59:32,268 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-15 04:59:43,624 - 0:00:15 - 11.4s - INFO - __main__ - task: sst, epoch: 20
2023-08-15 04:59:43,625 - 0:00:15 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-15 04:59:46,580 - 0:00:18 - 3.0s - INFO - __main__ - len of test dataset: 1821
2023-08-15 04:59:56,995 - 0:00:29 - 10.4s - INFO - __main__ - score: {'sst': OrderedDict([('em', 89.29159802306425), ('nf1', 89.29159802306425), ('nem', 89.29159802306425)]), 'srl': None, 'woz.en': None}
2023-08-15 05:00:08,618 - 0:00:40 - 11.6s - INFO - __main__ - task: srl, epoch: 20
2023-08-15 05:00:08,619 - 0:00:40 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-15 05:00:11,799 - 0:00:43 - 3.2s - INFO - __main__ - len of test dataset: 2201
2023-08-15 05:22:37,624 - 0:23:09 - 1345.8s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 47.61472058155384), ('nf1', 66.88461133937307), ('nem', 52.56701499318491)]), 'woz.en': None}
2023-08-15 05:22:48,665 - 0:23:20 - 11.0s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-15 05:22:48,665 - 0:23:20 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-15 05:22:51,805 - 0:23:23 - 3.1s - INFO - __main__ - len of test dataset: 1646
2023-08-15 05:34:18,239 - 0:34:50 - 686.4s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 16.767922235722963), ('nf1', 93.58521153478627), ('nem', 85.60145808019442), ('joint_goal_em', 82.68529769137302), ('turn_request_em', 91.98055893074118), ('turn_goal_em', 90.09720534629405), ('avg_dialogue', 87.3329283110571)])}
................................................................................................................................
 Training Adapter Fusion at rf 4
................................................................................................................................
Available number of GPU = 7 < n_gpus = 12
Continue training with 7 GPUs
2023-08-15 05:34:24,282 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0, 1, 2, 5, 8, 12, 15], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='6,7,8,9,10,11', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[24903.68, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=7, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-15 05:34:24,282 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-15 05:34:24,282 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 05:34:26,869 - 0:00:07 - 2.6s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [6, 7, 8, 9, 10, 11]
The Flat  : True

[0]
2023-08-15 05:34:37,988 - 0:00:18 - 11.1s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-15 05:35:48,692 - 0:01:28 - 70.7s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.271 , qa loss 1.271 , lm loss 0.000 , avg batch size 4.0
2023-08-15 05:36:28,652 - 0:02:08 - 40.0s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 0.85 , qa loss 0.85 , lm loss 0.00 , avg batch size 4.0
2023-08-15 05:37:39,980 - 0:03:20 - 71.3s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.246 , qa loss 0.246 , lm loss 0.000 , avg batch size 4.0
2023-08-15 05:38:20,993 - 0:04:01 - 41.0s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-15 05:39:31,708 - 0:05:11 - 70.7s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.198 , qa loss 0.198 , lm loss 0.000 , avg batch size 4.0
2023-08-15 05:40:11,694 - 0:05:51 - 40.0s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-15 05:41:22,893 - 0:07:03 - 71.2s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.164 , qa loss 0.164 , lm loss 0.000 , avg batch size 4.0
2023-08-15 05:42:03,159 - 0:07:43 - 40.3s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-15 05:43:13,581 - 0:08:53 - 70.4s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.142 , qa loss 0.142 , lm loss 0.000 , avg batch size 4.0
2023-08-15 05:43:53,986 - 0:09:34 - 40.4s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-15 05:45:03,724 - 0:10:43 - 69.7s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.109 , qa loss 0.109 , lm loss 0.000 , avg batch size 4.0
2023-08-15 05:45:43,841 - 0:11:24 - 40.1s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 05:46:54,591 - 0:12:34 - 70.8s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.111 , qa loss 0.111 , lm loss 0.000 , avg batch size 4.0
2023-08-15 05:47:35,259 - 0:13:15 - 40.7s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 05:48:45,622 - 0:14:25 - 70.4s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.089 , qa loss 0.089 , lm loss 0.000 , avg batch size 4.0
2023-08-15 05:49:26,767 - 0:15:07 - 41.1s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-15 05:50:37,087 - 0:16:17 - 70.3s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.076 , qa loss 0.076 , lm loss 0.000 , avg batch size 4.0
2023-08-15 05:51:17,389 - 0:16:57 - 40.3s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 05:52:26,785 - 0:18:07 - 69.4s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.062 , qa loss 0.062 , lm loss 0.000 , avg batch size 4.0
2023-08-15 05:53:07,093 - 0:18:47 - 40.3s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 05:54:17,707 - 0:19:57 - 70.6s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.061 , qa loss 0.061 , lm loss 0.000 , avg batch size 4.0
2023-08-15 05:54:58,175 - 0:20:38 - 40.5s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 05:56:08,392 - 0:21:48 - 70.2s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.048 , qa loss 0.048 , lm loss 0.000 , avg batch size 4.0
2023-08-15 05:56:49,300 - 0:22:29 - 40.9s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-15 05:57:59,384 - 0:23:39 - 70.1s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.036 , qa loss 0.036 , lm loss 0.000 , avg batch size 4.0
2023-08-15 05:58:40,235 - 0:24:20 - 40.9s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-08-15 05:59:50,482 - 0:25:30 - 70.2s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.033 , qa loss 0.033 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:00:30,461 - 0:26:10 - 40.0s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:01:41,437 - 0:27:21 - 71.0s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.027 , qa loss 0.027 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:02:23,124 - 0:28:03 - 41.7s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:03:34,375 - 0:29:14 - 71.3s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.026 , qa loss 0.026 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:04:15,944 - 0:29:56 - 41.6s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:05:27,343 - 0:31:07 - 71.4s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.020 , qa loss 0.020 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:06:09,405 - 0:31:49 - 42.1s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:07:21,439 - 0:33:01 - 72.0s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.017 , qa loss 0.017 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:08:04,265 - 0:33:44 - 42.8s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:09:17,416 - 0:34:57 - 73.2s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.015 , qa loss 0.015 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:09:59,929 - 0:35:40 - 42.5s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:11:12,452 - 0:36:52 - 72.5s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.015 , qa loss 0.015 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:11:56,359 - 0:37:36 - 43.9s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:11:57,564 - 0:37:37 - 1.2s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-15 06:11:57,565 - 0:37:37 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 06:11:57,698 - 0:37:37 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             3,550,464       2.853       1       1
srl                      union             3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-15 06:12:07,095 - 0:37:47 - 9.4s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-15 06:13:52,955 - 0:39:33 - 105.9s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 2.097 , qa loss 2.097 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:14:46,635 - 0:40:26 - 53.7s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 1.66 , qa loss 1.66 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:16:32,405 - 0:42:12 - 105.8s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.806 , qa loss 0.806 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:17:25,577 - 0:43:05 - 53.2s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.78 , qa loss 0.78 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:19:08,591 - 0:44:48 - 103.0s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.664 , qa loss 0.664 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:20:02,561 - 0:45:42 - 54.0s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:21:46,811 - 0:47:27 - 104.2s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.569 , qa loss 0.569 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:22:39,470 - 0:48:19 - 52.7s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.57 , qa loss 0.57 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:24:24,044 - 0:50:04 - 104.6s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.510 , qa loss 0.510 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:25:17,434 - 0:50:57 - 53.4s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:27:01,517 - 0:52:41 - 104.1s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.442 , qa loss 0.442 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:27:54,432 - 0:53:34 - 52.9s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:29:38,619 - 0:55:18 - 104.2s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.410 , qa loss 0.410 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:30:31,344 - 0:56:11 - 52.7s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:32:14,473 - 0:57:54 - 103.1s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.377 , qa loss 0.377 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:33:08,611 - 0:58:48 - 54.1s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:34:52,357 - 1:00:32 - 103.7s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.348 , qa loss 0.348 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:35:45,573 - 1:01:25 - 53.2s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:37:28,863 - 1:03:09 - 103.3s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.317 , qa loss 0.317 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:38:19,892 - 1:04:00 - 51.0s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:39:59,683 - 1:05:39 - 99.8s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.297 , qa loss 0.297 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:40:52,364 - 1:06:32 - 52.7s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:42:34,604 - 1:08:14 - 102.2s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.269 , qa loss 0.269 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:43:27,258 - 1:09:07 - 52.7s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:45:10,316 - 1:10:50 - 103.1s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.252 , qa loss 0.252 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:46:03,044 - 1:11:43 - 52.7s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:47:44,659 - 1:13:24 - 101.6s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.240 , qa loss 0.240 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:48:36,710 - 1:14:16 - 52.1s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:50:18,532 - 1:15:58 - 101.8s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.226 , qa loss 0.226 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:51:10,914 - 1:16:51 - 52.4s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:52:53,542 - 1:18:33 - 102.6s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.201 , qa loss 0.201 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:53:46,372 - 1:19:26 - 52.8s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:55:28,204 - 1:21:08 - 101.8s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.200 , qa loss 0.200 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:56:20,087 - 1:22:00 - 51.9s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-15 06:58:02,637 - 1:23:42 - 102.5s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.176 , qa loss 0.176 , lm loss 0.000 , avg batch size 4.0
2023-08-15 06:58:54,449 - 1:24:34 - 51.8s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:00:35,851 - 1:26:16 - 101.4s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.178 , qa loss 0.178 , lm loss 0.000 , avg batch size 4.0
2023-08-15 07:01:28,250 - 1:27:08 - 52.4s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:03:09,135 - 1:28:49 - 100.9s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.172 , qa loss 0.172 , lm loss 0.000 , avg batch size 4.0
2023-08-15 07:04:04,411 - 1:29:44 - 55.3s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:04:05,673 - 1:29:45 - 1.3s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-15 07:04:05,674 - 1:29:45 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 07:04:05,799 - 1:29:46 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             3,550,464       2.853       0       0
srl                      union             3,550,464       2.853       1       1
woz_en                   union             3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-15 07:04:14,025 - 1:29:54 - 8.2s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-15 07:05:09,322 - 1:30:49 - 55.3s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 2.18 , qa loss 2.18 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:06:05,465 - 1:31:45 - 56.1s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:07:00,542 - 1:32:40 - 55.1s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:07:56,314 - 1:33:36 - 55.8s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:08:51,005 - 1:34:31 - 54.7s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:09:46,149 - 1:35:26 - 55.1s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:10:41,406 - 1:36:21 - 55.3s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:11:36,854 - 1:37:17 - 55.4s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:12:31,611 - 1:38:11 - 54.8s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:13:26,892 - 1:39:07 - 55.3s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:14:20,974 - 1:40:01 - 54.1s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:15:15,103 - 1:40:55 - 54.1s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:16:10,279 - 1:41:50 - 55.2s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:17:04,916 - 1:42:45 - 54.6s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:17:59,776 - 1:43:40 - 54.9s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:18:53,505 - 1:44:33 - 53.7s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:19:48,384 - 1:45:28 - 54.9s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:20:42,405 - 1:46:22 - 54.0s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:21:36,697 - 1:47:16 - 54.3s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 07:22:31,993 - 1:48:12 - 55.3s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:48:08
CPU Execution time: 01:34:34
................................................................................................................................
Training Adapter + Prefix at prefix length 30
................................................................................................................................
2023-08-15 07:22:39,762 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-15 07:22:52,020 - 0:00:17 - 12.3s - INFO - __main__ - task: sst, epoch: 20
2023-08-15 07:22:52,021 - 0:00:17 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-15 07:22:56,326 - 0:00:21 - 4.3s - INFO - __main__ - len of test dataset: 1821
2023-08-15 07:23:06,464 - 0:00:31 - 10.1s - INFO - __main__ - score: {'sst': OrderedDict([('em', 89.73091707852828), ('nf1', 89.73091707852828), ('nem', 89.73091707852828)]), 'srl': None, 'woz.en': None}
2023-08-15 07:23:18,290 - 0:00:43 - 11.8s - INFO - __main__ - task: srl, epoch: 20
2023-08-15 07:23:18,290 - 0:00:43 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-15 07:23:22,650 - 0:00:47 - 4.4s - INFO - __main__ - len of test dataset: 2201
2023-08-15 07:45:54,312 - 0:23:19 - 1351.7s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 50.2044525215811), ('nf1', 68.89306552017962), ('nem', 55.474784189005)]), 'woz.en': None}
2023-08-15 07:46:05,841 - 0:23:31 - 11.5s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-15 07:46:05,841 - 0:23:31 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-15 07:46:10,164 - 0:23:35 - 4.3s - INFO - __main__ - len of test dataset: 1646
2023-08-15 07:57:51,591 - 0:35:16 - 701.4s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 17.01093560145808), ('nf1', 93.96676733554013), ('nem', 86.75577156743621), ('joint_goal_em', 83.96111786148238), ('turn_request_em', 92.2235722964763), ('turn_goal_em', 91.31227217496962), ('avg_dialogue', 88.09234507897935)])}
................................................................................................................................
 Training Adapter Fusion at rf 4
................................................................................................................................
Available number of GPU = 7 < n_gpus = 12
Continue training with 7 GPUs
2023-08-15 07:57:58,071 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0, 1, 2, 5, 8, 12, 15], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='8,9,10,11', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[24903.68, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=7, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-15 07:57:58,071 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-15 07:57:58,072 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 07:58:01,212 - 0:00:08 - 3.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [8, 9, 10, 11]
The Flat  : True

[0]
2023-08-15 07:58:12,304 - 0:00:19 - 11.1s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             4,733,952       3.804       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-15 07:59:22,253 - 0:01:29 - 69.9s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.165 , qa loss 1.165 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:00:04,461 - 0:02:11 - 42.2s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 0.78 , qa loss 0.78 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:01:17,929 - 0:03:24 - 73.5s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.214 , qa loss 0.214 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:02:00,415 - 0:04:07 - 42.5s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:03:13,855 - 0:05:20 - 73.4s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.183 , qa loss 0.183 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:03:58,296 - 0:06:05 - 44.4s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:05:12,196 - 0:07:19 - 73.9s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.157 , qa loss 0.157 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:05:55,348 - 0:08:02 - 43.2s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:07:09,892 - 0:09:16 - 74.5s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.122 , qa loss 0.122 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:07:52,510 - 0:09:59 - 42.6s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:09:07,689 - 0:11:14 - 75.2s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.110 , qa loss 0.110 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:09:50,384 - 0:11:57 - 42.7s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:11:04,823 - 0:13:11 - 74.4s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.092 , qa loss 0.092 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:11:47,644 - 0:13:54 - 42.8s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:13:02,516 - 0:15:09 - 74.9s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.074 , qa loss 0.074 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:13:45,034 - 0:15:51 - 42.5s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:14:58,544 - 0:17:05 - 73.5s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.063 , qa loss 0.063 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:15:41,043 - 0:17:47 - 42.5s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:16:55,050 - 0:19:01 - 74.0s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.050 , qa loss 0.050 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:17:37,996 - 0:19:44 - 42.9s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:18:52,120 - 0:20:59 - 74.1s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.055 , qa loss 0.055 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:19:34,410 - 0:21:41 - 42.3s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:20:48,996 - 0:22:55 - 74.6s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.034 , qa loss 0.034 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:21:31,005 - 0:23:37 - 42.0s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:22:45,096 - 0:24:52 - 74.1s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.024 , qa loss 0.024 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:23:28,092 - 0:25:35 - 43.0s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:24:40,733 - 0:26:47 - 72.6s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.032 , qa loss 0.032 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:25:23,928 - 0:27:30 - 43.2s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:26:37,395 - 0:28:44 - 73.5s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.020 , qa loss 0.020 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:27:20,082 - 0:29:27 - 42.7s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:28:34,758 - 0:30:41 - 74.7s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.018 , qa loss 0.018 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:29:17,016 - 0:31:23 - 42.3s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:30:31,009 - 0:32:37 - 74.0s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.012 , qa loss 0.012 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:31:14,626 - 0:33:21 - 43.6s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:32:29,091 - 0:34:36 - 74.5s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.010 , qa loss 0.010 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:33:11,262 - 0:35:18 - 42.2s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:34:25,774 - 0:36:32 - 74.5s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.005 , qa loss 0.005 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:35:08,268 - 0:37:15 - 42.5s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:36:22,134 - 0:38:29 - 73.9s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.007 , qa loss 0.007 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:37:05,625 - 0:39:12 - 43.5s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:37:06,869 - 0:39:13 - 1.2s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-15 08:37:06,870 - 0:39:13 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 08:37:06,988 - 0:39:13 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             4,733,952       3.804       1       1
srl                      union             4,733,952       3.804       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-15 08:37:15,561 - 0:39:22 - 8.6s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-15 08:39:00,837 - 0:41:07 - 105.3s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 1.998 , qa loss 1.998 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:39:56,536 - 0:42:03 - 55.7s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 1.58 , qa loss 1.58 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:41:41,652 - 0:43:48 - 105.1s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.745 , qa loss 0.745 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:42:36,983 - 0:44:43 - 55.3s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.73 , qa loss 0.73 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:44:23,628 - 0:46:30 - 106.6s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.592 , qa loss 0.592 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:45:18,051 - 0:47:24 - 54.4s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:47:02,075 - 0:49:09 - 104.0s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.514 , qa loss 0.514 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:47:57,688 - 0:50:04 - 55.6s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:49:43,407 - 0:51:50 - 105.7s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.437 , qa loss 0.437 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:50:39,257 - 0:52:46 - 55.9s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:52:28,260 - 0:54:35 - 109.0s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.378 , qa loss 0.378 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:53:23,259 - 0:55:30 - 55.0s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:55:09,902 - 0:57:16 - 106.6s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.327 , qa loss 0.327 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:56:03,671 - 0:58:10 - 53.8s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-15 08:57:49,328 - 0:59:56 - 105.7s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.299 , qa loss 0.299 , lm loss 0.000 , avg batch size 4.0
2023-08-15 08:58:44,349 - 1:00:51 - 55.0s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:00:29,402 - 1:02:36 - 105.1s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.273 , qa loss 0.273 , lm loss 0.000 , avg batch size 4.0
2023-08-15 09:01:26,116 - 1:03:33 - 56.7s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:03:13,982 - 1:05:20 - 107.9s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.251 , qa loss 0.251 , lm loss 0.000 , avg batch size 4.0
2023-08-15 09:04:08,926 - 1:06:15 - 54.9s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:05:55,645 - 1:08:02 - 106.7s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.211 , qa loss 0.211 , lm loss 0.000 , avg batch size 4.0
2023-08-15 09:06:51,080 - 1:08:58 - 55.4s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:08:37,889 - 1:10:44 - 106.8s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.196 , qa loss 0.196 , lm loss 0.000 , avg batch size 4.0
2023-08-15 09:09:32,456 - 1:11:39 - 54.6s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:11:20,353 - 1:13:27 - 107.9s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.182 , qa loss 0.182 , lm loss 0.000 , avg batch size 4.0
2023-08-15 09:12:14,614 - 1:14:21 - 54.3s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:14:00,130 - 1:16:07 - 105.5s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.174 , qa loss 0.174 , lm loss 0.000 , avg batch size 4.0
2023-08-15 09:14:56,525 - 1:17:03 - 56.4s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:16:42,372 - 1:18:49 - 105.8s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.145 , qa loss 0.145 , lm loss 0.000 , avg batch size 4.0
2023-08-15 09:17:38,722 - 1:19:45 - 56.4s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:19:25,828 - 1:21:32 - 107.1s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.142 , qa loss 0.142 , lm loss 0.000 , avg batch size 4.0
2023-08-15 09:20:21,070 - 1:22:28 - 55.2s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:22:05,453 - 1:24:12 - 104.4s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.128 , qa loss 0.128 , lm loss 0.000 , avg batch size 4.0
2023-08-15 09:23:02,186 - 1:25:09 - 56.7s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:24:48,827 - 1:26:55 - 106.6s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.123 , qa loss 0.123 , lm loss 0.000 , avg batch size 4.0
2023-08-15 09:25:44,764 - 1:27:51 - 55.9s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:27:30,565 - 1:29:37 - 105.8s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.117 , qa loss 0.117 , lm loss 0.000 , avg batch size 4.0
2023-08-15 09:28:26,081 - 1:30:33 - 55.5s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:30:12,940 - 1:32:19 - 106.9s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.110 , qa loss 0.110 , lm loss 0.000 , avg batch size 4.0
2023-08-15 09:31:09,020 - 1:33:15 - 56.1s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:31:10,263 - 1:33:17 - 1.2s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-15 09:31:10,264 - 1:33:17 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 09:31:10,393 - 1:33:17 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             4,733,952       3.804       0       0
srl                      union             4,733,952       3.804       1       1
woz_en                   union             4,733,952       3.804       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-15 09:31:18,322 - 1:33:25 - 7.9s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-15 09:32:17,163 - 1:34:24 - 58.8s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 1.77 , qa loss 1.77 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:33:16,187 - 1:35:23 - 59.0s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:34:16,033 - 1:36:22 - 59.8s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:35:16,142 - 1:37:23 - 60.1s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:36:16,016 - 1:38:22 - 59.9s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:37:14,255 - 1:39:21 - 58.2s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:38:13,959 - 1:40:20 - 59.7s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:39:13,231 - 1:41:20 - 59.3s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:40:16,196 - 1:42:23 - 63.0s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:41:16,556 - 1:43:23 - 60.4s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:42:15,868 - 1:44:22 - 59.3s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:43:15,494 - 1:45:22 - 59.6s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:44:15,199 - 1:46:22 - 59.7s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:45:14,332 - 1:47:21 - 59.1s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:46:13,814 - 1:48:20 - 59.5s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:47:11,320 - 1:49:18 - 57.5s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:48:08,126 - 1:50:15 - 56.8s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:49:05,848 - 1:51:12 - 57.7s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:50:02,864 - 1:52:09 - 57.0s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 09:51:00,892 - 1:53:07 - 58.0s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:53:04
CPU Execution time: 01:39:24
................................................................................................................................
Training Adapter + Prefix at prefix length 30
................................................................................................................................
2023-08-15 09:51:08,610 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-15 09:51:21,026 - 0:00:17 - 12.4s - INFO - __main__ - task: sst, epoch: 20
2023-08-15 09:51:21,027 - 0:00:17 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-15 09:51:24,556 - 0:00:20 - 3.5s - INFO - __main__ - len of test dataset: 1821
2023-08-15 09:51:33,966 - 0:00:30 - 9.4s - INFO - __main__ - score: {'sst': OrderedDict([('em', 91.15870400878639), ('nf1', 91.15870400878639), ('nem', 91.15870400878639)]), 'srl': None, 'woz.en': None}
2023-08-15 09:51:45,604 - 0:00:41 - 11.6s - INFO - __main__ - task: srl, epoch: 20
2023-08-15 09:51:45,604 - 0:00:41 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-15 09:51:49,749 - 0:00:46 - 4.1s - INFO - __main__ - len of test dataset: 2201
2023-08-15 10:13:01,575 - 0:21:57 - 1271.8s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 49.613811903680144), ('nf1', 69.1801453620501), ('nem', 55.156746933212176)]), 'woz.en': None}
2023-08-15 10:13:12,725 - 0:22:09 - 11.1s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-15 10:13:12,725 - 0:22:09 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-15 10:13:16,955 - 0:22:13 - 4.2s - INFO - __main__ - len of test dataset: 1646
2023-08-15 10:23:47,778 - 0:32:44 - 630.8s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 16.889428918590525), ('nf1', 94.3562489218625), ('nem', 86.99878493317132), ('joint_goal_em', 85.29769137302551), ('turn_request_em', 92.16281895504252), ('turn_goal_em', 91.91980558930742), ('avg_dialogue', 88.73025516403402)])}
................................................................................................................................
 Training Adapter Fusion at rf 4
................................................................................................................................
Available number of GPU = 7 < n_gpus = 12
Continue training with 7 GPUs
2023-08-15 10:23:54,206 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0, 1, 2, 5, 8, 12, 15], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='0,2,4,6,8,10', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[24903.68, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=7, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-15 10:23:54,207 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-15 10:23:54,207 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 10:23:56,679 - 0:00:07 - 2.5s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [0, 2, 4, 6, 8, 10]
The Flat  : True

[0]
2023-08-15 10:24:05,662 - 0:00:16 - 9.0s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-15 10:25:16,204 - 0:01:27 - 70.5s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.476 , qa loss 1.476 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:25:56,761 - 0:02:07 - 40.6s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 0.95 , qa loss 0.95 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:27:08,303 - 0:03:19 - 71.5s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.212 , qa loss 0.212 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:27:47,501 - 0:03:58 - 39.2s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:28:58,395 - 0:05:09 - 70.9s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.172 , qa loss 0.172 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:29:38,053 - 0:05:48 - 39.7s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:30:49,008 - 0:06:59 - 71.0s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.153 , qa loss 0.153 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:31:28,765 - 0:07:39 - 39.8s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:32:39,054 - 0:08:49 - 70.3s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.133 , qa loss 0.133 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:33:18,981 - 0:09:29 - 39.9s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:34:29,188 - 0:10:40 - 70.2s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.116 , qa loss 0.116 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:35:09,730 - 0:11:20 - 40.5s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:36:21,085 - 0:12:31 - 71.4s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.094 , qa loss 0.094 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:37:01,121 - 0:13:11 - 40.0s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:38:11,353 - 0:14:22 - 70.2s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.082 , qa loss 0.082 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:38:51,283 - 0:15:02 - 39.9s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:40:00,410 - 0:16:11 - 69.1s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.074 , qa loss 0.074 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:40:41,163 - 0:16:52 - 40.8s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:41:51,770 - 0:18:02 - 70.6s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.062 , qa loss 0.062 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:42:32,176 - 0:18:43 - 40.4s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:43:43,058 - 0:19:53 - 70.9s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.051 , qa loss 0.051 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:44:23,632 - 0:20:34 - 40.6s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:45:34,433 - 0:21:45 - 70.8s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.047 , qa loss 0.047 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:46:14,411 - 0:22:25 - 40.0s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:47:24,126 - 0:23:34 - 69.7s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.038 , qa loss 0.038 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:48:04,368 - 0:24:15 - 40.2s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:49:14,280 - 0:25:25 - 69.9s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.042 , qa loss 0.042 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:49:54,572 - 0:26:05 - 40.3s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:51:06,022 - 0:27:16 - 71.4s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.022 , qa loss 0.022 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:51:46,031 - 0:27:56 - 40.0s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:52:59,471 - 0:29:10 - 73.4s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.025 , qa loss 0.025 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:53:40,468 - 0:29:51 - 41.0s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:54:50,949 - 0:31:01 - 70.5s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.018 , qa loss 0.018 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:55:30,666 - 0:31:41 - 39.7s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:56:40,963 - 0:32:51 - 70.3s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.020 , qa loss 0.020 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:57:21,021 - 0:33:31 - 40.1s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-08-15 10:58:31,798 - 0:34:42 - 70.8s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.020 , qa loss 0.020 , lm loss 0.000 , avg batch size 4.0
2023-08-15 10:59:11,270 - 0:35:22 - 39.5s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:00:21,633 - 0:36:32 - 70.4s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.014 , qa loss 0.014 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:01:02,481 - 0:37:13 - 40.8s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:01:03,679 - 0:37:14 - 1.2s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-15 11:01:03,680 - 0:37:14 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 11:01:03,798 - 0:37:14 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             3,550,464       2.853       1       1
srl                      union             3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-15 11:01:10,541 - 0:37:21 - 6.7s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-15 11:02:56,625 - 0:39:07 - 106.1s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 2.363 , qa loss 2.363 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:03:47,380 - 0:39:58 - 50.8s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 1.80 , qa loss 1.80 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:05:34,804 - 0:41:45 - 107.4s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.727 , qa loss 0.727 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:06:29,615 - 0:42:40 - 54.8s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.71 , qa loss 0.71 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:08:11,106 - 0:44:21 - 101.5s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.594 , qa loss 0.594 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:09:03,407 - 0:45:14 - 52.3s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:10:43,986 - 0:46:54 - 100.6s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.493 , qa loss 0.493 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:11:36,755 - 0:47:47 - 52.8s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:13:21,290 - 0:49:32 - 104.5s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.445 , qa loss 0.445 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:14:14,156 - 0:50:25 - 52.9s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:15:57,172 - 0:52:08 - 103.0s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.391 , qa loss 0.391 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:16:54,221 - 0:53:05 - 57.0s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:18:43,805 - 0:54:54 - 109.6s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.345 , qa loss 0.345 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:19:36,322 - 0:55:47 - 52.5s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:21:17,468 - 0:57:28 - 101.1s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.296 , qa loss 0.296 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:22:15,511 - 0:58:26 - 58.0s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:24:04,029 - 1:00:14 - 108.5s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.271 , qa loss 0.271 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:24:58,332 - 1:01:09 - 54.3s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:26:39,377 - 1:02:50 - 101.0s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.241 , qa loss 0.241 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:27:37,375 - 1:03:48 - 58.0s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:29:19,722 - 1:05:30 - 102.3s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.218 , qa loss 0.218 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:30:10,639 - 1:06:21 - 50.9s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:31:52,071 - 1:08:02 - 101.4s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.196 , qa loss 0.196 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:32:44,095 - 1:08:54 - 52.0s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:34:25,773 - 1:10:36 - 101.7s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.177 , qa loss 0.177 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:35:18,190 - 1:11:29 - 52.4s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:36:59,304 - 1:13:10 - 101.1s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.167 , qa loss 0.167 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:37:52,061 - 1:14:02 - 52.8s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:39:32,855 - 1:15:43 - 100.8s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.146 , qa loss 0.146 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:40:25,203 - 1:16:36 - 52.3s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:42:06,697 - 1:18:17 - 101.5s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.138 , qa loss 0.138 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:42:59,562 - 1:19:10 - 52.9s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:44:41,255 - 1:20:52 - 101.7s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.134 , qa loss 0.134 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:45:32,394 - 1:21:43 - 51.1s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:47:13,266 - 1:23:24 - 100.9s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.122 , qa loss 0.122 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:48:05,148 - 1:24:16 - 51.9s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:49:45,955 - 1:25:56 - 100.8s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.111 , qa loss 0.111 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:50:38,884 - 1:26:49 - 52.9s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:52:19,984 - 1:28:30 - 101.1s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.112 , qa loss 0.112 , lm loss 0.000 , avg batch size 4.0
2023-08-15 11:53:12,743 - 1:29:23 - 52.8s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:53:13,970 - 1:29:24 - 1.2s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-15 11:53:13,971 - 1:29:24 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 11:53:14,103 - 1:29:24 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             3,550,464       2.853       0       0
srl                      union             3,550,464       2.853       1       1
woz_en                   union             3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-15 11:53:20,134 - 1:29:31 - 6.0s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-15 11:54:15,627 - 1:30:26 - 55.5s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 2.05 , qa loss 2.05 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:55:10,979 - 1:31:21 - 55.4s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:56:05,571 - 1:32:16 - 54.6s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:57:00,811 - 1:33:11 - 55.2s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:57:57,170 - 1:34:08 - 56.4s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:58:52,286 - 1:35:03 - 55.1s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-15 11:59:47,850 - 1:35:58 - 55.6s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-15 12:00:42,925 - 1:36:53 - 55.1s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 12:01:37,840 - 1:37:48 - 54.9s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 12:02:31,219 - 1:38:42 - 53.4s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-15 12:03:24,362 - 1:39:35 - 53.1s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-15 12:04:17,196 - 1:40:28 - 52.8s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-15 12:05:11,121 - 1:41:21 - 53.9s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-15 12:06:04,073 - 1:42:14 - 53.0s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 12:06:56,940 - 1:43:07 - 52.9s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 12:07:52,723 - 1:44:03 - 55.8s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 12:08:46,652 - 1:44:57 - 53.9s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 12:09:40,343 - 1:45:51 - 53.7s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 12:10:33,116 - 1:46:43 - 52.8s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 12:11:26,951 - 1:47:37 - 53.8s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:47:33
CPU Execution time: 01:32:10
................................................................................................................................
Training Adapter + Prefix at prefix length 30
................................................................................................................................
2023-08-15 12:11:34,119 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-15 12:11:46,012 - 0:00:16 - 11.9s - INFO - __main__ - task: sst, epoch: 20
2023-08-15 12:11:46,013 - 0:00:16 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-15 12:11:50,272 - 0:00:20 - 4.3s - INFO - __main__ - len of test dataset: 1821
2023-08-15 12:12:00,784 - 0:00:31 - 10.5s - INFO - __main__ - score: {'sst': OrderedDict([('em', 90.49972542559034), ('nf1', 90.49972542559034), ('nem', 90.49972542559034)]), 'srl': None, 'woz.en': None}
2023-08-15 12:12:12,501 - 0:00:42 - 11.7s - INFO - __main__ - task: srl, epoch: 20
2023-08-15 12:12:12,502 - 0:00:42 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-15 12:12:16,619 - 0:00:46 - 4.1s - INFO - __main__ - len of test dataset: 2201
2023-08-15 12:37:43,363 - 0:26:13 - 1526.7s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 47.751022262607904), ('nf1', 66.8673406147237), ('nem', 53.339391185824624)]), 'woz.en': None}
2023-08-15 12:37:54,884 - 0:26:25 - 11.5s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-15 12:37:54,884 - 0:26:25 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-15 12:37:58,749 - 0:26:29 - 3.9s - INFO - __main__ - len of test dataset: 1646
2023-08-15 12:50:56,020 - 0:39:26 - 777.3s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 17.01093560145808), ('nf1', 94.4804075363006), ('nem', 86.99878493317132), ('joint_goal_em', 84.62940461725394), ('turn_request_em', 92.89185905224787), ('turn_goal_em', 91.55528554070473), ('avg_dialogue', 88.76063183475091)])}
................................................................................................................................
 Training Adapter Fusion at rf 4
................................................................................................................................
Available number of GPU = 7 < n_gpus = 12
Continue training with 7 GPUs
2023-08-15 12:51:02,552 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0, 1, 2, 7, 8, 12, 15], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='1,3,5,7,9,11', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[24903.68, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=7, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-15 12:51:02,552 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-15 12:51:02,552 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 12:51:05,166 - 0:00:07 - 2.6s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [1, 3, 5, 7, 9, 11]
The Flat  : True

[0]
2023-08-15 12:51:14,689 - 0:00:17 - 9.5s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-15 12:52:23,559 - 0:01:25 - 68.9s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.242 , qa loss 1.242 , lm loss 0.000 , avg batch size 4.0
2023-08-15 12:53:04,766 - 0:02:07 - 41.2s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 0.82 , qa loss 0.82 , lm loss 0.00 , avg batch size 4.0
2023-08-15 12:54:17,033 - 0:03:19 - 72.3s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.223 , qa loss 0.223 , lm loss 0.000 , avg batch size 4.0
2023-08-15 12:54:57,497 - 0:03:59 - 40.5s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-15 12:56:10,486 - 0:05:12 - 73.0s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.170 , qa loss 0.170 , lm loss 0.000 , avg batch size 4.0
2023-08-15 12:56:52,910 - 0:05:55 - 42.4s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-15 12:58:06,268 - 0:07:08 - 73.4s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.162 , qa loss 0.162 , lm loss 0.000 , avg batch size 4.0
2023-08-15 12:58:47,440 - 0:07:49 - 41.2s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-15 12:59:59,923 - 0:09:02 - 72.5s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.132 , qa loss 0.132 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:00:40,519 - 0:09:42 - 40.6s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:01:51,549 - 0:10:53 - 71.0s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.121 , qa loss 0.121 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:02:32,311 - 0:11:34 - 40.8s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:03:44,352 - 0:12:46 - 72.0s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.109 , qa loss 0.109 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:04:24,808 - 0:13:27 - 40.5s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:05:35,743 - 0:14:38 - 70.9s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.081 , qa loss 0.081 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:06:17,957 - 0:15:20 - 42.2s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:07:31,156 - 0:16:33 - 73.2s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.076 , qa loss 0.076 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:08:12,582 - 0:17:14 - 41.4s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:09:23,868 - 0:18:26 - 71.3s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.068 , qa loss 0.068 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:10:04,989 - 0:19:07 - 41.1s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:11:17,596 - 0:20:20 - 72.6s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.059 , qa loss 0.059 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:11:59,034 - 0:21:01 - 41.4s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:13:12,933 - 0:22:15 - 73.9s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.040 , qa loss 0.040 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:13:53,938 - 0:22:56 - 41.0s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:15:07,293 - 0:24:09 - 73.4s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.042 , qa loss 0.042 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:15:49,116 - 0:24:51 - 41.8s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:17:01,795 - 0:26:04 - 72.7s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.038 , qa loss 0.038 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:17:43,587 - 0:26:45 - 41.8s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:18:56,721 - 0:27:59 - 73.1s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.032 , qa loss 0.032 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:19:37,917 - 0:28:40 - 41.2s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:20:49,800 - 0:29:52 - 71.9s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.026 , qa loss 0.026 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:21:31,422 - 0:30:33 - 41.6s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:22:44,400 - 0:31:46 - 73.0s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.026 , qa loss 0.026 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:23:26,139 - 0:32:28 - 41.7s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:24:38,749 - 0:33:41 - 72.6s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.022 , qa loss 0.022 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:25:19,764 - 0:34:22 - 41.0s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:26:32,231 - 0:35:34 - 72.5s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.020 , qa loss 0.020 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:27:13,505 - 0:36:15 - 41.3s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:28:25,308 - 0:37:27 - 71.8s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.009 , qa loss 0.009 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:29:07,205 - 0:38:09 - 41.9s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:29:08,404 - 0:38:10 - 1.2s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-15 13:29:08,405 - 0:38:10 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 13:29:08,531 - 0:38:10 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             3,550,464       2.853       1       1
srl                      union             3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-15 13:29:16,104 - 0:38:18 - 7.6s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-15 13:31:00,550 - 0:40:02 - 104.4s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 1.892 , qa loss 1.892 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:31:53,351 - 0:40:55 - 52.8s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 1.52 , qa loss 1.52 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:33:36,364 - 0:42:38 - 103.0s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.726 , qa loss 0.726 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:34:30,314 - 0:43:32 - 54.0s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.71 , qa loss 0.71 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:36:13,493 - 0:45:15 - 103.2s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.592 , qa loss 0.592 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:37:07,524 - 0:46:09 - 54.0s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:38:54,029 - 0:47:56 - 106.5s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.501 , qa loss 0.501 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:39:44,083 - 0:48:46 - 50.1s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:41:28,380 - 0:50:30 - 104.3s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.437 , qa loss 0.437 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:42:21,593 - 0:51:24 - 53.2s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:44:04,067 - 0:53:06 - 102.5s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.373 , qa loss 0.373 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:44:56,394 - 0:53:58 - 52.3s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:46:39,620 - 0:55:42 - 103.2s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.347 , qa loss 0.347 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:47:33,261 - 0:56:35 - 53.6s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:49:14,718 - 0:58:17 - 101.5s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.317 , qa loss 0.317 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:50:08,110 - 0:59:10 - 53.4s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:51:52,754 - 1:00:55 - 104.6s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.261 , qa loss 0.261 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:52:45,170 - 1:01:47 - 52.4s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:54:29,140 - 1:03:31 - 104.0s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.244 , qa loss 0.244 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:55:22,991 - 1:04:25 - 53.9s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:57:07,748 - 1:06:10 - 104.8s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.229 , qa loss 0.229 , lm loss 0.000 , avg batch size 4.0
2023-08-15 13:58:01,485 - 1:07:03 - 53.7s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-15 13:59:45,393 - 1:08:47 - 103.9s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.204 , qa loss 0.204 , lm loss 0.000 , avg batch size 4.0
2023-08-15 14:00:38,335 - 1:09:40 - 52.9s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:02:21,599 - 1:11:24 - 103.3s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.180 , qa loss 0.180 , lm loss 0.000 , avg batch size 4.0
2023-08-15 14:03:15,775 - 1:12:18 - 54.2s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:05:01,248 - 1:14:03 - 105.5s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.178 , qa loss 0.178 , lm loss 0.000 , avg batch size 4.0
2023-08-15 14:05:53,896 - 1:14:56 - 52.6s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:07:39,405 - 1:16:41 - 105.5s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.154 , qa loss 0.154 , lm loss 0.000 , avg batch size 4.0
2023-08-15 14:08:32,760 - 1:17:35 - 53.4s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:10:14,826 - 1:19:17 - 102.1s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.146 , qa loss 0.146 , lm loss 0.000 , avg batch size 4.0
2023-08-15 14:11:10,881 - 1:20:13 - 56.1s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:12:55,176 - 1:21:57 - 104.3s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.128 , qa loss 0.128 , lm loss 0.000 , avg batch size 4.0
2023-08-15 14:13:48,598 - 1:22:51 - 53.4s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:15:33,993 - 1:24:36 - 105.4s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.125 , qa loss 0.125 , lm loss 0.000 , avg batch size 4.0
2023-08-15 14:16:25,594 - 1:25:28 - 51.6s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:18:08,763 - 1:27:11 - 103.2s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.118 , qa loss 0.118 , lm loss 0.000 , avg batch size 4.0
2023-08-15 14:19:02,106 - 1:28:04 - 53.3s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:20:48,613 - 1:29:51 - 106.5s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.119 , qa loss 0.119 , lm loss 0.000 , avg batch size 4.0
2023-08-15 14:21:42,000 - 1:30:44 - 53.4s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:21:43,241 - 1:30:45 - 1.2s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-15 14:21:43,242 - 1:30:45 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 14:21:43,371 - 1:30:45 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             3,550,464       2.853       0       0
srl                      union             3,550,464       2.853       1       1
woz_en                   union             3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-15 14:21:50,044 - 1:30:52 - 6.7s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-15 14:22:46,703 - 1:31:49 - 56.7s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 2.28 , qa loss 2.28 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:23:43,692 - 1:32:46 - 57.0s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:24:40,426 - 1:33:42 - 56.7s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:25:36,786 - 1:34:39 - 56.4s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:26:33,699 - 1:35:36 - 56.9s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:27:30,114 - 1:36:32 - 56.4s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:28:27,990 - 1:37:30 - 57.9s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:29:24,164 - 1:38:26 - 56.2s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:30:21,743 - 1:39:24 - 57.6s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:31:18,510 - 1:40:20 - 56.8s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:32:15,723 - 1:41:18 - 57.2s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:33:13,240 - 1:42:15 - 57.5s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:34:10,644 - 1:43:13 - 57.4s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:35:07,077 - 1:44:09 - 56.4s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:36:03,820 - 1:45:06 - 56.7s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:37:01,398 - 1:46:03 - 57.6s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:37:57,653 - 1:47:00 - 56.3s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:38:54,088 - 1:47:56 - 56.4s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:39:51,592 - 1:48:54 - 57.5s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 14:40:49,553 - 1:49:51 - 58.0s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:49:48
CPU Execution time: 01:34:34
................................................................................................................................
Training Adapter + Prefix at prefix length 30
................................................................................................................................
2023-08-15 14:40:57,704 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-15 14:41:09,466 - 0:00:17 - 11.8s - INFO - __main__ - task: sst, epoch: 20
2023-08-15 14:41:09,467 - 0:00:17 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-15 14:41:13,931 - 0:00:21 - 4.5s - INFO - __main__ - len of test dataset: 1821
2023-08-15 14:41:24,107 - 0:00:31 - 10.2s - INFO - __main__ - score: {'sst': OrderedDict([('em', 90.55464030752334), ('nf1', 90.55464030752334), ('nem', 90.55464030752334)]), 'srl': None, 'woz.en': None}
2023-08-15 14:41:35,380 - 0:00:42 - 11.3s - INFO - __main__ - task: srl, epoch: 20
2023-08-15 14:41:35,380 - 0:00:42 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-15 14:41:39,549 - 0:00:47 - 4.2s - INFO - __main__ - len of test dataset: 2201
2023-08-15 15:07:56,214 - 0:27:03 - 1576.7s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 47.97819173103135), ('nf1', 68.45515482897434), ('nem', 53.702862335302136)]), 'woz.en': None}
2023-08-15 15:08:08,528 - 0:27:16 - 12.3s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-15 15:08:08,528 - 0:27:16 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-15 15:08:12,626 - 0:27:20 - 4.1s - INFO - __main__ - len of test dataset: 1646
2023-08-15 15:20:55,097 - 0:40:02 - 762.5s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 16.889428918590525), ('nf1', 94.4991703314911), ('nem', 87.24179829890643), ('joint_goal_em', 86.51275820170109), ('turn_request_em', 92.16281895504252), ('turn_goal_em', 91.91980558930742), ('avg_dialogue', 89.3377885783718)])}
--------------------------------------------The End Man!!!------------------------------------------------------
