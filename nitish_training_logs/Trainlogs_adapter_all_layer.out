Available number of GPU = 3 < n_gpus = 12
Continue training with 3 GPUs
2023-07-30 18:53:25,928 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[3, 13, 15], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[30146.56, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='model_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=3, n_train_epochs={'sst': 10, 'srl': 10, 'woz.en': 10}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[10551, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[10551, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-07-30 18:53:25,929 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-07-30 18:53:25,929 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-30 18:53:29,090 - 0:00:08 - 3.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
[0]
2023-07-30 18:53:39,927 - 0:00:19 - 10.8s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 69200
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-07-30 18:55:23,144 - 0:02:02 - 103.2s - INFO - __main__ - progress 0.578 , lr 5.9E-05 , loss 2.099 , qa loss 2.099 , lm loss 0.000 , avg batch size 4.0
2023-07-30 18:56:43,404 - 0:03:22 - 80.3s - INFO - __main__ - epoch 1/10 done , tot steps 1730 , lr 5.6E-05 , loss 1.32 , qa loss 1.32 , lm loss 0.00 , avg batch size 4.0
2023-07-30 18:58:45,811 - 0:05:24 - 122.4s - INFO - __main__ - progress 1.578 , lr 5.3E-05 , loss 0.219 , qa loss 0.219 , lm loss 0.000 , avg batch size 4.0
2023-07-30 19:00:08,362 - 0:06:47 - 82.6s - INFO - __main__ - epoch 2/10 done , tot steps 3460 , lr 5.0E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-07-30 19:02:11,908 - 0:08:51 - 123.5s - INFO - __main__ - progress 2.578 , lr 4.6E-05 , loss 0.189 , qa loss 0.189 , lm loss 0.000 , avg batch size 4.0
2023-07-30 19:03:32,921 - 0:10:12 - 81.0s - INFO - __main__ - epoch 3/10 done , tot steps 5190 , lr 4.4E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-07-30 19:05:36,662 - 0:12:15 - 123.7s - INFO - __main__ - progress 3.578 , lr 4.0E-05 , loss 0.172 , qa loss 0.172 , lm loss 0.000 , avg batch size 4.0
2023-07-30 19:06:57,327 - 0:13:36 - 80.7s - INFO - __main__ - epoch 4/10 done , tot steps 6920 , lr 3.8E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-07-30 19:09:01,599 - 0:15:40 - 124.3s - INFO - __main__ - progress 4.578 , lr 3.4E-05 , loss 0.150 , qa loss 0.150 , lm loss 0.000 , avg batch size 4.0
2023-07-30 19:10:22,982 - 0:17:02 - 81.4s - INFO - __main__ - epoch 5/10 done , tot steps 8650 , lr 3.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-07-30 19:12:24,067 - 0:19:03 - 121.1s - INFO - __main__ - progress 5.578 , lr 2.8E-05 , loss 0.143 , qa loss 0.143 , lm loss 0.000 , avg batch size 4.0
2023-07-30 19:13:40,280 - 0:20:19 - 76.2s - INFO - __main__ - epoch 6/10 done , tot steps 10380 , lr 2.5E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-30 19:15:15,659 - 0:21:54 - 95.4s - INFO - __main__ - progress 6.578 , lr 2.1E-05 , loss 0.137 , qa loss 0.137 , lm loss 0.000 , avg batch size 4.0
2023-07-30 19:16:27,331 - 0:23:06 - 71.7s - INFO - __main__ - epoch 7/10 done , tot steps 12110 , lr 1.9E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-07-30 19:18:32,945 - 0:25:12 - 125.6s - INFO - __main__ - progress 7.578 , lr 1.5E-05 , loss 0.119 , qa loss 0.119 , lm loss 0.000 , avg batch size 4.0
2023-07-30 19:19:55,686 - 0:26:34 - 82.7s - INFO - __main__ - epoch 8/10 done , tot steps 13840 , lr 1.3E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-07-30 19:22:02,546 - 0:28:41 - 126.9s - INFO - __main__ - progress 8.578 , lr 8.9E-06 , loss 0.122 , qa loss 0.122 , lm loss 0.000 , avg batch size 4.0
2023-07-30 19:23:24,533 - 0:30:03 - 82.0s - INFO - __main__ - epoch 9/10 done , tot steps 15570 , lr 6.3E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-07-30 19:25:28,907 - 0:32:08 - 124.4s - INFO - __main__ - progress 9.578 , lr 2.7E-06 , loss 0.100 , qa loss 0.100 , lm loss 0.000 , avg batch size 4.0
2023-07-30 19:26:51,931 - 0:33:31 - 83.0s - INFO - __main__ - epoch 10/10 done , tot steps 17300 , lr 3.1E-08 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-07-30 19:26:53,039 - 0:33:32 - 1.1s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-07-30 19:26:53,040 - 0:33:32 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-30 19:26:53,194 - 0:33:32 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-07-30 19:27:02,969 - 0:33:42 - 9.8s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 64140
2023-07-30 19:29:39,727 - 0:36:18 - 156.8s - INFO - __main__ - progress 0.624 , lr 5.9E-05 , loss 3.704 , qa loss 3.704 , lm loss 0.000 , avg batch size 4.0
2023-07-30 19:31:07,393 - 0:37:46 - 87.7s - INFO - __main__ - epoch 1/10 done , tot steps 1604 , lr 5.6E-05 , loss 2.66 , qa loss 2.66 , lm loss 0.00 , avg batch size 4.0
2023-07-30 19:33:41,017 - 0:40:20 - 153.6s - INFO - __main__ - progress 1.624 , lr 5.2E-05 , loss 0.765 , qa loss 0.765 , lm loss 0.000 , avg batch size 4.0
2023-07-30 19:34:54,572 - 0:41:33 - 73.6s - INFO - __main__ - epoch 2/10 done , tot steps 3208 , lr 5.0E-05 , loss 0.75 , qa loss 0.75 , lm loss 0.00 , avg batch size 4.0
2023-07-30 19:37:19,755 - 0:43:58 - 145.2s - INFO - __main__ - progress 2.624 , lr 4.6E-05 , loss 0.654 , qa loss 0.654 , lm loss 0.000 , avg batch size 4.0
2023-07-30 19:38:49,283 - 0:45:28 - 89.5s - INFO - __main__ - epoch 3/10 done , tot steps 4812 , lr 4.4E-05 , loss 0.64 , qa loss 0.64 , lm loss 0.00 , avg batch size 4.0
2023-07-30 19:41:26,044 - 0:48:05 - 156.8s - INFO - __main__ - progress 3.624 , lr 4.0E-05 , loss 0.571 , qa loss 0.571 , lm loss 0.000 , avg batch size 4.0
2023-07-30 19:42:58,343 - 0:49:37 - 92.3s - INFO - __main__ - epoch 4/10 done , tot steps 6416 , lr 3.8E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-07-30 19:45:37,742 - 0:52:16 - 159.4s - INFO - __main__ - progress 4.624 , lr 3.4E-05 , loss 0.528 , qa loss 0.528 , lm loss 0.000 , avg batch size 4.0
2023-07-30 19:47:06,544 - 0:53:45 - 88.8s - INFO - __main__ - epoch 5/10 done , tot steps 8020 , lr 3.1E-05 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-07-30 19:49:47,342 - 0:56:26 - 160.8s - INFO - __main__ - progress 5.624 , lr 2.7E-05 , loss 0.484 , qa loss 0.484 , lm loss 0.000 , avg batch size 4.0
2023-07-30 19:51:15,288 - 0:57:54 - 87.9s - INFO - __main__ - epoch 6/10 done , tot steps 9624 , lr 2.5E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-07-30 19:53:46,828 - 1:00:25 - 151.5s - INFO - __main__ - progress 6.624 , lr 2.1E-05 , loss 0.438 , qa loss 0.438 , lm loss 0.000 , avg batch size 4.0
2023-07-30 19:55:05,811 - 1:01:44 - 79.0s - INFO - __main__ - epoch 7/10 done , tot steps 11228 , lr 1.9E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-07-30 19:57:32,816 - 1:04:11 - 147.0s - INFO - __main__ - progress 7.624 , lr 1.5E-05 , loss 0.416 , qa loss 0.416 , lm loss 0.000 , avg batch size 4.0
2023-07-30 19:59:03,237 - 1:05:42 - 90.4s - INFO - __main__ - epoch 8/10 done , tot steps 12832 , lr 1.3E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-07-30 20:01:42,896 - 1:08:22 - 159.7s - INFO - __main__ - progress 8.624 , lr 8.6E-06 , loss 0.388 , qa loss 0.388 , lm loss 0.000 , avg batch size 4.0
2023-07-30 20:03:11,337 - 1:09:50 - 88.4s - INFO - __main__ - epoch 9/10 done , tot steps 14436 , lr 6.3E-06 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-07-30 20:05:51,641 - 1:12:30 - 160.3s - INFO - __main__ - progress 9.624 , lr 2.4E-06 , loss 0.392 , qa loss 0.392 , lm loss 0.000 , avg batch size 4.0
2023-07-30 20:07:19,992 - 1:13:59 - 88.4s - INFO - __main__ - epoch 10/10 done , tot steps 16040 , lr 3.1E-08 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-07-30 20:07:21,220 - 1:14:00 - 1.2s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-07-30 20:07:21,220 - 1:14:00 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-30 20:07:21,372 - 1:14:00 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-07-30 20:07:30,819 - 1:14:09 - 9.4s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 25360
2023-07-30 20:09:16,114 - 1:15:55 - 105.3s - INFO - __main__ - epoch 1/10 done , tot steps 634 , lr 5.6E-05 , loss 4.43 , qa loss 4.43 , lm loss 0.00 , avg batch size 4.0
2023-07-30 20:11:00,702 - 1:17:39 - 104.6s - INFO - __main__ - epoch 2/10 done , tot steps 1268 , lr 5.0E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-07-30 20:12:39,323 - 1:19:18 - 98.6s - INFO - __main__ - epoch 3/10 done , tot steps 1902 , lr 4.4E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-07-30 20:14:15,406 - 1:20:54 - 96.1s - INFO - __main__ - epoch 4/10 done , tot steps 2536 , lr 3.8E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-07-30 20:15:36,103 - 1:22:15 - 80.7s - INFO - __main__ - epoch 5/10 done , tot steps 3170 , lr 3.1E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-07-30 20:17:08,326 - 1:23:47 - 92.2s - INFO - __main__ - epoch 6/10 done , tot steps 3804 , lr 2.5E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-07-30 20:18:50,798 - 1:25:29 - 102.5s - INFO - __main__ - epoch 7/10 done , tot steps 4438 , lr 1.9E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-07-30 20:20:36,735 - 1:27:15 - 105.9s - INFO - __main__ - epoch 8/10 done , tot steps 5072 , lr 1.3E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-07-30 20:22:20,101 - 1:28:59 - 103.4s - INFO - __main__ - epoch 9/10 done , tot steps 5706 , lr 6.3E-06 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-30 20:24:04,594 - 1:30:43 - 104.5s - INFO - __main__ - epoch 10/10 done , tot steps 6340 , lr 3.0E-08 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:30:40
CPU Execution time: 01:23:43
