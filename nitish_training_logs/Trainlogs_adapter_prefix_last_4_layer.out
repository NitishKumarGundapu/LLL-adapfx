Available number of GPU = 1 < n_gpus = 12
Continue training with 1 GPUs
2023-07-28 16:40:37,762 - 0:00:07 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[13], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 10, 'srl': 10, 'woz.en': 10}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[11468], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[11468], unbound=0, use_sep=False, weight_decay=0.01)
2023-07-28 16:40:37,762 - 0:00:07 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-07-28 16:40:37,762 - 0:00:07 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-28 16:40:40,752 - 0:00:10 - 3.0s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
[0]
2023-07-28 16:40:54,875 - 0:00:24 - 14.1s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 69200
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2023-07-28 16:42:26,873 - 0:01:56 - 92.0s - INFO - __main__ - progress 0.578 , lr 5.9E-05 , loss 1.984 , qa loss 1.984 , lm loss 0.000 , avg batch size 4.0
2023-07-28 16:43:35,346 - 0:03:05 - 68.5s - INFO - __main__ - epoch 1/10 done , tot steps 1730 , lr 5.6E-05 , loss 1.26 , qa loss 1.26 , lm loss 0.00 , avg batch size 4.0
2023-07-28 16:45:16,722 - 0:04:46 - 101.4s - INFO - __main__ - progress 1.578 , lr 5.3E-05 , loss 0.236 , qa loss 0.236 , lm loss 0.000 , avg batch size 4.0
2023-07-28 16:46:13,820 - 0:05:43 - 57.1s - INFO - __main__ - epoch 2/10 done , tot steps 3460 , lr 5.0E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-07-28 16:47:47,478 - 0:07:17 - 93.7s - INFO - __main__ - progress 2.578 , lr 4.6E-05 , loss 0.217 , qa loss 0.217 , lm loss 0.000 , avg batch size 4.0
2023-07-28 16:48:45,956 - 0:08:15 - 58.5s - INFO - __main__ - epoch 3/10 done , tot steps 5190 , lr 4.4E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-07-28 16:50:17,423 - 0:09:47 - 91.5s - INFO - __main__ - progress 3.578 , lr 4.0E-05 , loss 0.195 , qa loss 0.195 , lm loss 0.000 , avg batch size 4.0
2023-07-28 16:51:16,100 - 0:10:46 - 58.7s - INFO - __main__ - epoch 4/10 done , tot steps 6920 , lr 3.8E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-07-28 16:52:46,239 - 0:12:16 - 90.1s - INFO - __main__ - progress 4.578 , lr 3.4E-05 , loss 0.188 , qa loss 0.188 , lm loss 0.000 , avg batch size 4.0
2023-07-28 16:53:44,907 - 0:13:14 - 58.7s - INFO - __main__ - epoch 5/10 done , tot steps 8650 , lr 3.1E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-07-28 16:55:17,237 - 0:14:47 - 92.3s - INFO - __main__ - progress 5.578 , lr 2.8E-05 , loss 0.179 , qa loss 0.179 , lm loss 0.000 , avg batch size 4.0
2023-07-28 16:56:15,091 - 0:15:45 - 57.9s - INFO - __main__ - epoch 6/10 done , tot steps 10380 , lr 2.5E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-07-28 16:57:46,876 - 0:17:16 - 91.8s - INFO - __main__ - progress 6.578 , lr 2.1E-05 , loss 0.176 , qa loss 0.176 , lm loss 0.000 , avg batch size 4.0
2023-07-28 16:58:45,060 - 0:18:15 - 58.2s - INFO - __main__ - epoch 7/10 done , tot steps 12110 , lr 1.9E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-07-28 17:00:17,318 - 0:19:47 - 92.3s - INFO - __main__ - progress 7.578 , lr 1.5E-05 , loss 0.171 , qa loss 0.171 , lm loss 0.000 , avg batch size 4.0
2023-07-28 17:01:15,328 - 0:20:45 - 58.0s - INFO - __main__ - epoch 8/10 done , tot steps 13840 , lr 1.3E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-07-28 17:02:44,677 - 0:22:14 - 89.3s - INFO - __main__ - progress 8.578 , lr 8.9E-06 , loss 0.169 , qa loss 0.169 , lm loss 0.000 , avg batch size 4.0
2023-07-28 17:03:43,825 - 0:23:13 - 59.1s - INFO - __main__ - epoch 9/10 done , tot steps 15570 , lr 6.3E-06 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-07-28 17:05:31,098 - 0:25:01 - 107.3s - INFO - __main__ - progress 9.578 , lr 2.7E-06 , loss 0.160 , qa loss 0.160 , lm loss 0.000 , avg batch size 4.0
2023-07-28 17:07:30,562 - 0:27:00 - 119.5s - INFO - __main__ - epoch 10/10 done , tot steps 17300 , lr 3.1E-08 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-28 17:07:31,901 - 0:27:01 - 1.3s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-07-28 17:07:31,902 - 0:27:01 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-28 17:07:32,047 - 0:27:02 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-07-28 17:07:45,408 - 0:27:15 - 13.4s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 64140
2023-07-28 17:12:09,964 - 0:31:39 - 264.6s - INFO - __main__ - progress 0.624 , lr 5.9E-05 , loss 3.351 , qa loss 3.351 , lm loss 0.000 , avg batch size 4.0
2023-07-28 17:14:45,963 - 0:34:15 - 156.0s - INFO - __main__ - epoch 1/10 done , tot steps 1604 , lr 5.6E-05 , loss 2.70 , qa loss 2.70 , lm loss 0.00 , avg batch size 4.0
2023-07-28 17:19:14,406 - 0:38:44 - 268.4s - INFO - __main__ - progress 1.624 , lr 5.2E-05 , loss 1.496 , qa loss 1.496 , lm loss 0.000 , avg batch size 4.0
2023-07-28 17:21:46,264 - 0:41:16 - 151.9s - INFO - __main__ - epoch 2/10 done , tot steps 3208 , lr 5.0E-05 , loss 1.47 , qa loss 1.47 , lm loss 0.00 , avg batch size 4.0
2023-07-28 17:26:22,854 - 0:45:52 - 276.6s - INFO - __main__ - progress 2.624 , lr 4.6E-05 , loss 1.389 , qa loss 1.389 , lm loss 0.000 , avg batch size 4.0
2023-07-28 17:28:53,166 - 0:48:23 - 150.3s - INFO - __main__ - epoch 3/10 done , tot steps 4812 , lr 4.4E-05 , loss 1.38 , qa loss 1.38 , lm loss 0.00 , avg batch size 4.0
2023-07-28 17:33:21,253 - 0:52:51 - 268.1s - INFO - __main__ - progress 3.624 , lr 4.0E-05 , loss 1.310 , qa loss 1.310 , lm loss 0.000 , avg batch size 4.0
2023-07-28 17:35:53,860 - 0:55:23 - 152.6s - INFO - __main__ - epoch 4/10 done , tot steps 6416 , lr 3.8E-05 , loss 1.30 , qa loss 1.30 , lm loss 0.00 , avg batch size 4.0
2023-07-28 17:40:21,110 - 0:59:51 - 267.3s - INFO - __main__ - progress 4.624 , lr 3.4E-05 , loss 1.254 , qa loss 1.254 , lm loss 0.000 , avg batch size 4.0
2023-07-28 17:42:51,059 - 1:02:21 - 149.9s - INFO - __main__ - epoch 5/10 done , tot steps 8020 , lr 3.1E-05 , loss 1.26 , qa loss 1.26 , lm loss 0.00 , avg batch size 4.0
2023-07-28 17:47:33,911 - 1:07:03 - 282.9s - INFO - __main__ - progress 5.624 , lr 2.7E-05 , loss 1.236 , qa loss 1.236 , lm loss 0.000 , avg batch size 4.0
2023-07-28 17:50:09,311 - 1:09:39 - 155.4s - INFO - __main__ - epoch 6/10 done , tot steps 9624 , lr 2.5E-05 , loss 1.23 , qa loss 1.23 , lm loss 0.00 , avg batch size 4.0
2023-07-28 17:54:36,007 - 1:14:05 - 266.7s - INFO - __main__ - progress 6.624 , lr 2.1E-05 , loss 1.187 , qa loss 1.187 , lm loss 0.000 , avg batch size 4.0
2023-07-28 17:57:06,245 - 1:16:36 - 150.2s - INFO - __main__ - epoch 7/10 done , tot steps 11228 , lr 1.9E-05 , loss 1.18 , qa loss 1.18 , lm loss 0.00 , avg batch size 4.0
2023-07-28 18:01:29,258 - 1:20:59 - 263.0s - INFO - __main__ - progress 7.624 , lr 1.5E-05 , loss 1.152 , qa loss 1.152 , lm loss 0.000 , avg batch size 4.0
2023-07-28 18:04:04,050 - 1:23:34 - 154.8s - INFO - __main__ - epoch 8/10 done , tot steps 12832 , lr 1.3E-05 , loss 1.17 , qa loss 1.17 , lm loss 0.00 , avg batch size 4.0
2023-07-28 18:08:48,469 - 1:28:18 - 284.4s - INFO - __main__ - progress 8.624 , lr 8.6E-06 , loss 1.171 , qa loss 1.171 , lm loss 0.000 , avg batch size 4.0
2023-07-28 18:11:21,117 - 1:30:51 - 152.6s - INFO - __main__ - epoch 9/10 done , tot steps 14436 , lr 6.3E-06 , loss 1.16 , qa loss 1.16 , lm loss 0.00 , avg batch size 4.0
2023-07-28 18:15:49,228 - 1:35:19 - 268.1s - INFO - __main__ - progress 9.624 , lr 2.4E-06 , loss 1.124 , qa loss 1.124 , lm loss 0.000 , avg batch size 4.0
2023-07-28 18:18:12,776 - 1:37:42 - 143.5s - INFO - __main__ - epoch 10/10 done , tot steps 16040 , lr 3.1E-08 , loss 1.14 , qa loss 1.14 , lm loss 0.00 , avg batch size 4.0
2023-07-28 18:18:14,286 - 1:37:44 - 1.5s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-07-28 18:18:14,286 - 1:37:44 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-28 18:18:14,448 - 1:37:44 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-07-28 18:18:24,239 - 1:37:54 - 9.8s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 25360
2023-07-28 18:20:09,178 - 1:39:39 - 104.9s - INFO - __main__ - epoch 1/10 done , tot steps 634 , lr 5.6E-05 , loss 3.00 , qa loss 3.00 , lm loss 0.00 , avg batch size 4.0
2023-07-28 18:21:53,828 - 1:41:23 - 104.7s - INFO - __main__ - epoch 2/10 done , tot steps 1268 , lr 5.0E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-07-28 18:23:38,299 - 1:43:08 - 104.5s - INFO - __main__ - epoch 3/10 done , tot steps 1902 , lr 4.4E-05 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-07-28 18:25:24,130 - 1:44:54 - 105.8s - INFO - __main__ - epoch 4/10 done , tot steps 2536 , lr 3.8E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-07-28 18:27:15,472 - 1:46:45 - 111.3s - INFO - __main__ - epoch 5/10 done , tot steps 3170 , lr 3.1E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-07-28 18:29:06,749 - 1:48:36 - 111.3s - INFO - __main__ - epoch 6/10 done , tot steps 3804 , lr 2.5E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-07-28 18:30:47,737 - 1:50:17 - 101.0s - INFO - __main__ - epoch 7/10 done , tot steps 4438 , lr 1.9E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-07-28 18:32:32,568 - 1:52:02 - 104.8s - INFO - __main__ - epoch 8/10 done , tot steps 5072 , lr 1.3E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-07-28 18:34:15,444 - 1:53:45 - 102.9s - INFO - __main__ - epoch 9/10 done , tot steps 5706 , lr 6.3E-06 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-07-28 18:36:00,724 - 1:55:30 - 105.3s - INFO - __main__ - epoch 10/10 done , tot steps 6340 , lr 3.0E-08 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:55:24
CPU Execution time: 01:49:35
