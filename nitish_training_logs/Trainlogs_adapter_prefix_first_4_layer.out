Available number of GPU = 1 < n_gpus = 12
Continue training with 1 GPUs
2023-07-29 19:32:19,952 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[14], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 10, 'srl': 10, 'woz.en': 10}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[11468], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[11468], unbound=0, use_sep=False, weight_decay=0.01)
2023-07-29 19:32:19,953 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-07-29 19:32:19,953 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-29 19:32:22,904 - 0:00:08 - 3.0s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
[0]
2023-07-29 19:32:33,984 - 0:00:19 - 11.1s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 69200
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2023-07-29 19:33:38,408 - 0:01:23 - 64.4s - INFO - __main__ - progress 0.578 , lr 5.9E-05 , loss 2.852 , qa loss 2.852 , lm loss 0.000 , avg batch size 4.0
2023-07-29 19:34:12,215 - 0:01:57 - 33.8s - INFO - __main__ - epoch 1/10 done , tot steps 1730 , lr 5.6E-05 , loss 1.79 , qa loss 1.79 , lm loss 0.00 , avg batch size 4.0
2023-07-29 19:35:19,488 - 0:03:04 - 67.3s - INFO - __main__ - progress 1.578 , lr 5.3E-05 , loss 0.291 , qa loss 0.291 , lm loss 0.000 , avg batch size 4.0
2023-07-29 19:36:11,398 - 0:03:56 - 51.9s - INFO - __main__ - epoch 2/10 done , tot steps 3460 , lr 5.0E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-07-29 19:37:36,230 - 0:05:21 - 84.8s - INFO - __main__ - progress 2.578 , lr 4.6E-05 , loss 0.244 , qa loss 0.244 , lm loss 0.000 , avg batch size 4.0
2023-07-29 19:38:27,397 - 0:06:12 - 51.2s - INFO - __main__ - epoch 3/10 done , tot steps 5190 , lr 4.4E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-07-29 19:39:52,169 - 0:07:37 - 84.8s - INFO - __main__ - progress 3.578 , lr 4.0E-05 , loss 0.230 , qa loss 0.230 , lm loss 0.000 , avg batch size 4.0
2023-07-29 19:40:44,322 - 0:08:29 - 52.2s - INFO - __main__ - epoch 4/10 done , tot steps 6920 , lr 3.8E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-07-29 19:42:08,468 - 0:09:53 - 84.1s - INFO - __main__ - progress 4.578 , lr 3.4E-05 , loss 0.196 , qa loss 0.196 , lm loss 0.000 , avg batch size 4.0
2023-07-29 19:43:00,959 - 0:10:46 - 52.5s - INFO - __main__ - epoch 5/10 done , tot steps 8650 , lr 3.1E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-07-29 19:44:25,298 - 0:12:10 - 84.3s - INFO - __main__ - progress 5.578 , lr 2.8E-05 , loss 0.194 , qa loss 0.194 , lm loss 0.000 , avg batch size 4.0
2023-07-29 19:45:16,758 - 0:13:02 - 51.5s - INFO - __main__ - epoch 6/10 done , tot steps 10380 , lr 2.5E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-07-29 19:46:43,111 - 0:14:28 - 86.4s - INFO - __main__ - progress 6.578 , lr 2.1E-05 , loss 0.182 , qa loss 0.182 , lm loss 0.000 , avg batch size 4.0
2023-07-29 19:47:34,781 - 0:15:20 - 51.7s - INFO - __main__ - epoch 7/10 done , tot steps 12110 , lr 1.9E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-07-29 19:48:59,608 - 0:16:45 - 84.8s - INFO - __main__ - progress 7.578 , lr 1.5E-05 , loss 0.167 , qa loss 0.167 , lm loss 0.000 , avg batch size 4.0
2023-07-29 19:49:50,130 - 0:17:35 - 50.5s - INFO - __main__ - epoch 8/10 done , tot steps 13840 , lr 1.3E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-07-29 19:51:14,204 - 0:18:59 - 84.1s - INFO - __main__ - progress 8.578 , lr 8.9E-06 , loss 0.165 , qa loss 0.165 , lm loss 0.000 , avg batch size 4.0
2023-07-29 19:52:04,938 - 0:19:50 - 50.7s - INFO - __main__ - epoch 9/10 done , tot steps 15570 , lr 6.3E-06 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-29 19:53:27,121 - 0:21:12 - 82.2s - INFO - __main__ - progress 9.578 , lr 2.7E-06 , loss 0.149 , qa loss 0.149 , lm loss 0.000 , avg batch size 4.0
2023-07-29 19:54:17,779 - 0:22:03 - 50.7s - INFO - __main__ - epoch 10/10 done , tot steps 17300 , lr 3.1E-08 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-07-29 19:54:19,157 - 0:22:04 - 1.4s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-07-29 19:54:19,158 - 0:22:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-29 19:54:19,294 - 0:22:04 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-07-29 19:54:28,058 - 0:22:13 - 8.8s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 64140
2023-07-29 19:56:26,225 - 0:24:11 - 118.2s - INFO - __main__ - progress 0.624 , lr 5.9E-05 , loss 5.000 , qa loss 5.000 , lm loss 0.000 , avg batch size 4.0
2023-07-29 19:57:30,884 - 0:25:16 - 64.7s - INFO - __main__ - epoch 1/10 done , tot steps 1604 , lr 5.6E-05 , loss 3.65 , qa loss 3.65 , lm loss 0.00 , avg batch size 4.0
2023-07-29 19:59:31,586 - 0:27:17 - 120.7s - INFO - __main__ - progress 1.624 , lr 5.2E-05 , loss 1.115 , qa loss 1.115 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:00:35,883 - 0:28:21 - 64.3s - INFO - __main__ - epoch 2/10 done , tot steps 3208 , lr 5.0E-05 , loss 1.06 , qa loss 1.06 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:02:35,360 - 0:30:20 - 119.5s - INFO - __main__ - progress 2.624 , lr 4.6E-05 , loss 0.906 , qa loss 0.906 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:03:40,337 - 0:31:25 - 65.0s - INFO - __main__ - epoch 3/10 done , tot steps 4812 , lr 4.4E-05 , loss 0.90 , qa loss 0.90 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:05:37,622 - 0:33:23 - 117.3s - INFO - __main__ - progress 3.624 , lr 4.0E-05 , loss 0.814 , qa loss 0.814 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:06:43,329 - 0:34:28 - 65.7s - INFO - __main__ - epoch 4/10 done , tot steps 6416 , lr 3.8E-05 , loss 0.79 , qa loss 0.79 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:08:41,954 - 0:36:27 - 118.6s - INFO - __main__ - progress 4.624 , lr 3.4E-05 , loss 0.738 , qa loss 0.738 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:09:30,733 - 0:37:16 - 48.8s - INFO - __main__ - epoch 5/10 done , tot steps 8020 , lr 3.1E-05 , loss 0.73 , qa loss 0.73 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:11:03,092 - 0:38:48 - 92.4s - INFO - __main__ - progress 5.624 , lr 2.7E-05 , loss 0.702 , qa loss 0.702 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:12:03,811 - 0:39:49 - 60.7s - INFO - __main__ - epoch 6/10 done , tot steps 9624 , lr 2.5E-05 , loss 0.70 , qa loss 0.70 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:14:06,638 - 0:41:52 - 122.8s - INFO - __main__ - progress 6.624 , lr 2.1E-05 , loss 0.671 , qa loss 0.671 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:15:13,019 - 0:42:58 - 66.4s - INFO - __main__ - epoch 7/10 done , tot steps 11228 , lr 1.9E-05 , loss 0.67 , qa loss 0.67 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:17:17,324 - 0:45:02 - 124.3s - INFO - __main__ - progress 7.624 , lr 1.5E-05 , loss 0.645 , qa loss 0.645 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:18:23,600 - 0:46:09 - 66.3s - INFO - __main__ - epoch 8/10 done , tot steps 12832 , lr 1.3E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:20:28,654 - 0:48:14 - 125.1s - INFO - __main__ - progress 8.624 , lr 8.6E-06 , loss 0.620 , qa loss 0.620 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:21:33,742 - 0:49:19 - 65.1s - INFO - __main__ - epoch 9/10 done , tot steps 14436 , lr 6.3E-06 , loss 0.62 , qa loss 0.62 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:23:36,274 - 0:51:21 - 122.5s - INFO - __main__ - progress 9.624 , lr 2.4E-06 , loss 0.610 , qa loss 0.610 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:24:43,239 - 0:52:28 - 67.0s - INFO - __main__ - epoch 10/10 done , tot steps 16040 , lr 3.1E-08 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:24:44,742 - 0:52:30 - 1.5s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-07-29 20:24:44,742 - 0:52:30 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-29 20:24:44,915 - 0:52:30 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-07-29 20:24:54,552 - 0:52:40 - 9.6s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 25360
2023-07-29 20:25:55,714 - 0:53:41 - 61.2s - INFO - __main__ - epoch 1/10 done , tot steps 634 , lr 5.6E-05 , loss 3.91 , qa loss 3.91 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:26:56,807 - 0:54:42 - 61.1s - INFO - __main__ - epoch 2/10 done , tot steps 1268 , lr 5.0E-05 , loss 0.98 , qa loss 0.98 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:27:57,250 - 0:55:42 - 60.4s - INFO - __main__ - epoch 3/10 done , tot steps 1902 , lr 4.4E-05 , loss 0.69 , qa loss 0.69 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:28:56,001 - 0:56:41 - 58.8s - INFO - __main__ - epoch 4/10 done , tot steps 2536 , lr 3.8E-05 , loss 0.57 , qa loss 0.57 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:29:54,148 - 0:57:39 - 58.1s - INFO - __main__ - epoch 5/10 done , tot steps 3170 , lr 3.1E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:31:09,505 - 0:58:54 - 75.4s - INFO - __main__ - epoch 6/10 done , tot steps 3804 , lr 2.5E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:32:10,572 - 0:59:56 - 61.1s - INFO - __main__ - epoch 7/10 done , tot steps 4438 , lr 1.9E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:33:11,163 - 1:00:56 - 60.6s - INFO - __main__ - epoch 8/10 done , tot steps 5072 , lr 1.3E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:34:09,511 - 1:01:55 - 58.3s - INFO - __main__ - epoch 9/10 done , tot steps 5706 , lr 6.3E-06 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:35:09,875 - 1:02:55 - 60.4s - INFO - __main__ - epoch 10/10 done , tot steps 6340 , lr 3.0E-08 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:02:51
CPU Execution time: 00:55:51
