................................................................................................................................
 Training Adapter Fusion at rf 4
................................................................................................................................
Available number of GPU = 5 < n_gpus = 12
Continue training with 5 GPUs
2023-08-15 17:02:31,523 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0, 1, 5, 8, 12], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[27525.12, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=5, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[9633, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[9633, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-15 17:02:31,524 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-15 17:02:31,524 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 17:02:34,855 - 0:00:08 - 3.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : []
The Flat  : True

[0]
2023-08-15 17:02:47,126 - 0:00:20 - 12.3s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             7,100,928       5.706       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-15 17:04:15,977 - 0:01:49 - 88.9s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.130 , qa loss 1.130 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:05:08,336 - 0:02:42 - 52.4s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 0.76 , qa loss 0.76 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:06:33,021 - 0:04:06 - 84.7s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.227 , qa loss 0.227 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:07:25,760 - 0:04:59 - 52.7s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:08:50,840 - 0:06:24 - 85.1s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.183 , qa loss 0.183 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:09:43,391 - 0:07:17 - 52.6s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:11:09,593 - 0:08:43 - 86.2s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.147 , qa loss 0.147 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:12:02,161 - 0:09:35 - 52.6s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:13:27,146 - 0:11:00 - 85.0s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.126 , qa loss 0.126 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:14:19,887 - 0:11:53 - 52.7s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:15:44,369 - 0:13:18 - 84.5s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.113 , qa loss 0.113 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:16:36,646 - 0:14:10 - 52.3s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:18:00,880 - 0:15:34 - 84.2s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.083 , qa loss 0.083 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:18:52,450 - 0:16:26 - 51.6s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:20:17,241 - 0:17:50 - 84.8s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.070 , qa loss 0.070 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:21:09,135 - 0:18:42 - 51.9s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:22:33,170 - 0:20:06 - 84.0s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.062 , qa loss 0.062 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:23:25,092 - 0:20:58 - 51.9s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:24:49,840 - 0:22:23 - 84.7s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.047 , qa loss 0.047 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:25:42,050 - 0:23:15 - 52.2s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:27:07,877 - 0:24:41 - 85.8s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.046 , qa loss 0.046 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:27:59,797 - 0:25:33 - 51.9s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:29:25,186 - 0:26:58 - 85.4s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.032 , qa loss 0.032 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:30:17,519 - 0:27:51 - 52.3s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:31:41,309 - 0:29:15 - 83.8s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.027 , qa loss 0.027 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:32:33,263 - 0:30:06 - 52.0s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:34:00,556 - 0:31:34 - 87.3s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.022 , qa loss 0.022 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:34:52,888 - 0:32:26 - 52.3s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:36:20,285 - 0:33:53 - 87.4s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.020 , qa loss 0.020 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:37:13,098 - 0:34:46 - 52.8s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:38:39,057 - 0:36:12 - 86.0s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.018 , qa loss 0.018 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:39:31,811 - 0:37:05 - 52.8s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:40:55,797 - 0:38:29 - 84.0s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.018 , qa loss 0.018 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:41:48,002 - 0:39:21 - 52.2s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:43:11,828 - 0:40:45 - 83.8s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.012 , qa loss 0.012 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:44:04,253 - 0:41:37 - 52.4s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:45:30,680 - 0:43:04 - 86.4s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.006 , qa loss 0.006 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:46:23,523 - 0:43:57 - 52.8s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:47:47,723 - 0:45:21 - 84.2s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.011 , qa loss 0.011 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:48:42,750 - 0:46:16 - 55.0s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:48:44,071 - 0:46:17 - 1.3s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-15 17:48:44,071 - 0:46:17 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 17:48:44,198 - 0:46:17 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             7,100,928       5.706       1       1
srl                      union             7,100,928       5.706       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-15 17:48:53,848 - 0:46:27 - 9.6s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-15 17:50:55,649 - 0:48:29 - 121.8s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 2.006 , qa loss 2.006 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:51:59,575 - 0:49:33 - 63.9s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 1.57 , qa loss 1.57 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:53:58,050 - 0:51:31 - 118.5s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.700 , qa loss 0.700 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:55:04,920 - 0:52:38 - 66.9s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.69 , qa loss 0.69 , lm loss 0.00 , avg batch size 4.0
2023-08-15 17:57:06,224 - 0:54:39 - 121.3s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.539 , qa loss 0.539 , lm loss 0.000 , avg batch size 4.0
2023-08-15 17:58:09,509 - 0:55:43 - 63.3s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:00:08,615 - 0:57:42 - 119.1s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.444 , qa loss 0.444 , lm loss 0.000 , avg batch size 4.0
2023-08-15 18:01:12,948 - 0:58:46 - 64.3s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:03:12,343 - 1:00:46 - 119.4s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.376 , qa loss 0.376 , lm loss 0.000 , avg batch size 4.0
2023-08-15 18:04:17,317 - 1:01:51 - 65.0s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:06:18,178 - 1:03:51 - 120.9s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.310 , qa loss 0.310 , lm loss 0.000 , avg batch size 4.0
2023-08-15 18:07:23,162 - 1:04:56 - 65.0s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:09:21,910 - 1:06:55 - 118.7s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.264 , qa loss 0.264 , lm loss 0.000 , avg batch size 4.0
2023-08-15 18:10:25,415 - 1:07:59 - 63.5s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:12:24,058 - 1:09:57 - 118.6s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.237 , qa loss 0.237 , lm loss 0.000 , avg batch size 4.0
2023-08-15 18:13:26,202 - 1:10:59 - 62.1s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:15:22,515 - 1:12:56 - 116.3s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.199 , qa loss 0.199 , lm loss 0.000 , avg batch size 4.0
2023-08-15 18:16:25,945 - 1:13:59 - 63.4s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:18:22,842 - 1:15:56 - 116.9s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.165 , qa loss 0.165 , lm loss 0.000 , avg batch size 4.0
2023-08-15 18:19:24,670 - 1:16:58 - 61.8s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:21:20,685 - 1:18:54 - 116.0s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.153 , qa loss 0.153 , lm loss 0.000 , avg batch size 4.0
2023-08-15 18:22:24,742 - 1:19:58 - 64.1s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:24:22,446 - 1:21:56 - 117.7s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.125 , qa loss 0.125 , lm loss 0.000 , avg batch size 4.0
2023-08-15 18:25:26,087 - 1:22:59 - 63.6s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:27:20,795 - 1:24:54 - 114.7s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.117 , qa loss 0.117 , lm loss 0.000 , avg batch size 4.0
2023-08-15 18:28:23,042 - 1:25:56 - 62.2s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:30:19,100 - 1:27:52 - 116.1s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.097 , qa loss 0.097 , lm loss 0.000 , avg batch size 4.0
2023-08-15 18:31:22,117 - 1:28:55 - 63.0s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:33:19,705 - 1:30:53 - 117.6s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.085 , qa loss 0.085 , lm loss 0.000 , avg batch size 4.0
2023-08-15 18:34:21,475 - 1:31:55 - 61.8s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:36:18,811 - 1:33:52 - 117.3s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.083 , qa loss 0.083 , lm loss 0.000 , avg batch size 4.0
2023-08-15 18:37:22,460 - 1:34:56 - 63.6s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:39:20,943 - 1:36:54 - 118.5s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.073 , qa loss 0.073 , lm loss 0.000 , avg batch size 4.0
2023-08-15 18:40:24,216 - 1:37:57 - 63.3s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:42:20,489 - 1:39:54 - 116.3s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.069 , qa loss 0.069 , lm loss 0.000 , avg batch size 4.0
2023-08-15 18:43:22,324 - 1:40:56 - 61.8s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:45:18,715 - 1:42:52 - 116.4s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.058 , qa loss 0.058 , lm loss 0.000 , avg batch size 4.0
2023-08-15 18:46:21,458 - 1:43:55 - 62.7s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:48:16,240 - 1:45:49 - 114.8s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.053 , qa loss 0.053 , lm loss 0.000 , avg batch size 4.0
2023-08-15 18:49:19,653 - 1:46:53 - 63.4s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:49:20,970 - 1:46:54 - 1.3s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-15 18:49:20,970 - 1:46:54 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-15 18:49:21,101 - 1:46:54 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      union             7,100,928       5.706       0       0
srl                      union             7,100,928       5.706       1       1
woz_en                   union             7,100,928       5.706       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-15 18:49:29,727 - 1:47:03 - 8.6s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-15 18:50:40,059 - 1:48:13 - 70.3s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 1.25 , qa loss 1.25 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:51:49,317 - 1:49:23 - 69.3s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:52:57,741 - 1:50:31 - 68.4s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:54:07,143 - 1:51:40 - 69.4s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:55:17,115 - 1:52:50 - 70.0s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:56:25,397 - 1:53:59 - 68.3s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:57:34,209 - 1:55:07 - 68.8s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:58:42,633 - 1:56:16 - 68.4s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-15 18:59:55,008 - 1:57:28 - 72.4s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-15 19:01:03,012 - 1:58:36 - 68.0s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-15 19:02:11,686 - 1:59:45 - 68.7s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-15 19:03:20,111 - 2:00:53 - 68.4s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-15 19:04:28,955 - 2:02:02 - 68.8s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 19:05:38,016 - 2:03:11 - 69.1s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-15 19:06:46,350 - 2:04:20 - 68.3s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-15 19:07:55,616 - 2:05:29 - 69.3s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-08-15 19:09:04,638 - 2:06:38 - 69.0s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-15 19:10:12,440 - 2:07:46 - 67.8s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-08-15 19:11:20,404 - 2:08:54 - 68.0s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-15 19:12:28,710 - 2:10:02 - 68.3s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 02:09:58
CPU Execution time: 01:56:33
................................................................................................................................
Training Adapter + Prefix at prefix length 30
................................................................................................................................
2023-08-15 19:12:36,190 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[0], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-15 19:12:47,965 - 0:00:16 - 11.8s - INFO - __main__ - task: sst, epoch: 20
2023-08-15 19:12:47,966 - 0:00:16 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-15 19:12:52,278 - 0:00:20 - 4.3s - INFO - __main__ - len of test dataset: 1821
2023-08-15 19:13:03,280 - 0:00:31 - 11.0s - INFO - __main__ - score: {'sst': OrderedDict([('em', 90.88412959912137), ('nf1', 90.88412959912137), ('nem', 90.88412959912137)]), 'srl': None, 'woz.en': None}
2023-08-15 19:13:14,695 - 0:00:43 - 11.4s - INFO - __main__ - task: srl, epoch: 20
2023-08-15 19:13:14,696 - 0:00:43 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-15 19:13:18,849 - 0:00:47 - 4.2s - INFO - __main__ - len of test dataset: 2201
2023-08-15 19:37:23,773 - 0:24:52 - 1444.9s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 48.9777373920945), ('nf1', 69.18285690991267), ('nem', 54.83870967741935)]), 'woz.en': None}
2023-08-15 19:37:35,532 - 0:25:04 - 11.8s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-15 19:37:35,532 - 0:25:04 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-15 19:37:39,648 - 0:25:08 - 4.1s - INFO - __main__ - len of test dataset: 1646
2023-08-15 19:50:22,391 - 0:37:50 - 762.7s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 16.889428918590525), ('nf1', 93.58110230528214), ('nem', 85.96597812879708), ('joint_goal_em', 82.62454434993924), ('turn_request_em', 91.79829890643985), ('turn_goal_em', 90.64398541919806), ('avg_dialogue', 87.21142162818956)])}
commands_1.sh: line 40: syntax error near unexpected token `done'
commands_1.sh: line 40: `done '
