................................................................................................................................
 Training Adapter + Prefix at prefix length 20
................................................................................................................................
Available number of GPU = 7 < n_gpus = 12
Continue training with 7 GPUs
2023-08-06 10:14:37,447 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[2, 5, 7, 11, 12, 13, 15], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout=[], lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[24903.68, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=7, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=20, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-06 10:14:37,448 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-06 10:14:37,448 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-06 10:14:40,645 - 0:00:08 - 3.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 20


The Reduction Facotr is : 4


The Bottleneck size is : 800


The leaving layers are : []


The Flat  : True

[0]
2023-08-06 10:14:53,440 - 0:00:20 - 12.8s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
Traceback (most recent call last):
  File "train_adapter_prefix_config.py", line 346, in <module>
    model = train([task_id], model, train_type=1)
  File "train_adapter_prefix_config.py", line 211, in train
    PrefixTuningConfig(cross_prefix=True, prefix_length=args.prefixlength, flat = True,non_linearity='relu',leave_out=list(map(int,args.leaveout.split(",")))),
AttributeError: 'list' object has no attribute 'split'
................................................................................................................................
Training Adapter + Prefix at prefix length 20
................................................................................................................................
2023-08-06 10:15:01,203 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[2], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout=[], lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-06 10:15:16,651 - 0:00:21 - 15.4s - INFO - __main__ - task: sst, epoch: 20
2023-08-06 10:15:16,651 - 0:00:21 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-06 10:15:22,412 - 0:00:27 - 5.8s - INFO - __main__ - len of test dataset: 1821
2023-08-06 10:15:32,344 - 0:00:37 - 9.9s - INFO - __main__ - score: {'sst': OrderedDict([('em', 83.80010982976387), ('nf1', 83.80010982976387), ('nem', 83.80010982976387)]), 'srl': None, 'woz.en': None}
2023-08-06 10:15:44,394 - 0:00:49 - 12.1s - INFO - __main__ - task: srl, epoch: 20
2023-08-06 10:15:44,394 - 0:00:49 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-06 10:15:50,546 - 0:00:55 - 6.2s - INFO - __main__ - len of test dataset: 2201
2023-08-06 10:43:52,013 - 0:28:56 - 1681.5s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 41.61744661517492), ('nf1', 61.98041948447113), ('nem', 46.706042707860064)]), 'woz.en': None}
2023-08-06 10:44:03,808 - 0:29:08 - 11.8s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-06 10:44:03,808 - 0:29:08 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-06 10:44:09,831 - 0:29:14 - 6.0s - INFO - __main__ - len of test dataset: 1646
2023-08-06 10:57:22,992 - 0:42:27 - 793.2s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 15.249088699878493), ('nf1', 91.10473371203483), ('nem', 81.10571081409478), ('joint_goal_em', 74.54434993924666), ('turn_request_em', 89.6719319562576), ('turn_goal_em', 86.87727825030377), ('avg_dialogue', 82.10814094775213)])}
................................................................................................................................
 Training Adapter + Prefix at prefix length 30
................................................................................................................................
Available number of GPU = 8 < n_gpus = 12
Continue training with 8 GPUs
2023-08-06 10:57:30,371 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[2, 5, 7, 9, 11, 12, 13, 15], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout=[], lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[23592.96, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=8, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[8257, 11927, 11927, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[8257, 11927, 11927, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-06 10:57:30,371 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-06 10:57:30,371 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-06 10:57:33,846 - 0:00:09 - 3.5s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30

The Reduction Facotr is : 4

The Bottleneck size is : 800

The leaving layers are : []

The Flat  : True

[0]
2023-08-06 10:57:47,075 - 0:00:22 - 13.2s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-06 10:59:17,634 - 0:01:52 - 90.6s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.167 , qa loss 2.167 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:00:11,782 - 0:02:47 - 54.1s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.41 , qa loss 1.41 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:01:37,228 - 0:04:12 - 85.4s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.356 , qa loss 0.356 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:02:30,919 - 0:05:06 - 53.7s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:03:56,320 - 0:06:31 - 85.4s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.297 , qa loss 0.297 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:04:50,610 - 0:07:25 - 54.3s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:06:18,158 - 0:08:53 - 87.5s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.278 , qa loss 0.278 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:07:11,727 - 0:09:47 - 53.6s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:08:38,948 - 0:11:14 - 87.2s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.266 , qa loss 0.266 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:09:31,668 - 0:12:06 - 52.7s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:10:57,008 - 0:13:32 - 85.3s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.251 , qa loss 0.251 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:11:52,943 - 0:14:28 - 55.9s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:13:19,686 - 0:15:55 - 86.7s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.246 , qa loss 0.246 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:14:13,817 - 0:16:49 - 54.1s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:15:40,073 - 0:18:15 - 86.3s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.226 , qa loss 0.226 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:16:33,911 - 0:19:09 - 53.8s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:18:00,049 - 0:20:35 - 86.1s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.236 , qa loss 0.236 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:18:54,297 - 0:21:29 - 54.2s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:20:19,883 - 0:22:55 - 85.6s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.228 , qa loss 0.228 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:21:16,357 - 0:23:51 - 56.5s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:22:49,379 - 0:25:24 - 93.0s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.224 , qa loss 0.224 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:23:47,826 - 0:26:23 - 58.4s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:25:20,680 - 0:27:56 - 92.9s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.222 , qa loss 0.222 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:26:17,695 - 0:28:53 - 57.0s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:27:48,325 - 0:30:23 - 90.6s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.214 , qa loss 0.214 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:28:47,253 - 0:31:22 - 58.9s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:30:16,841 - 0:32:52 - 89.6s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.206 , qa loss 0.206 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:31:15,254 - 0:33:50 - 58.4s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:32:46,728 - 0:35:22 - 91.5s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.222 , qa loss 0.222 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:33:45,087 - 0:36:20 - 58.4s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:35:15,684 - 0:37:51 - 90.6s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.205 , qa loss 0.205 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:36:14,277 - 0:38:49 - 58.6s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:37:46,840 - 0:40:22 - 92.6s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.207 , qa loss 0.207 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:38:41,996 - 0:41:17 - 55.2s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:40:07,996 - 0:42:43 - 86.0s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.206 , qa loss 0.206 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:41:02,913 - 0:43:38 - 54.9s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:42:28,595 - 0:45:03 - 85.7s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.199 , qa loss 0.199 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:43:22,306 - 0:45:57 - 53.7s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:44:47,871 - 0:47:23 - 85.6s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.187 , qa loss 0.187 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:45:43,716 - 0:48:19 - 55.8s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:45:45,155 - 0:48:20 - 1.4s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-06 11:45:45,155 - 0:48:20 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-06 11:45:45,291 - 0:48:20 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-08-06 11:45:56,554 - 0:48:31 - 11.3s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-06 11:47:54,480 - 0:50:29 - 117.9s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 7.173 , qa loss 7.173 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:48:57,930 - 0:51:33 - 63.5s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 5.71 , qa loss 5.71 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:50:58,603 - 0:53:33 - 120.7s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 2.163 , qa loss 2.163 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:52:04,503 - 0:54:39 - 65.9s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 1.99 , qa loss 1.99 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:54:02,545 - 0:56:37 - 118.0s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 1.388 , qa loss 1.388 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:55:09,746 - 0:57:45 - 67.2s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 1.33 , qa loss 1.33 , lm loss 0.00 , avg batch size 4.0
2023-08-06 11:57:11,554 - 0:59:46 - 121.8s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 1.114 , qa loss 1.114 , lm loss 0.000 , avg batch size 4.0
2023-08-06 11:58:17,242 - 1:00:52 - 65.7s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 1.11 , qa loss 1.11 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:00:18,205 - 1:02:53 - 121.0s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.968 , qa loss 0.968 , lm loss 0.000 , avg batch size 4.0
2023-08-06 12:01:24,334 - 1:03:59 - 66.1s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.95 , qa loss 0.95 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:03:25,168 - 1:06:00 - 120.8s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.885 , qa loss 0.885 , lm loss 0.000 , avg batch size 4.0
2023-08-06 12:04:33,014 - 1:07:08 - 67.8s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.86 , qa loss 0.86 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:06:29,427 - 1:09:04 - 116.4s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.807 , qa loss 0.807 , lm loss 0.000 , avg batch size 4.0
2023-08-06 12:07:33,001 - 1:10:08 - 63.6s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.80 , qa loss 0.80 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:09:27,460 - 1:12:02 - 114.5s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.769 , qa loss 0.769 , lm loss 0.000 , avg batch size 4.0
2023-08-06 12:10:31,401 - 1:13:06 - 63.9s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.76 , qa loss 0.76 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:12:26,610 - 1:15:01 - 115.2s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.721 , qa loss 0.721 , lm loss 0.000 , avg batch size 4.0
2023-08-06 12:13:30,386 - 1:16:05 - 63.8s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.73 , qa loss 0.73 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:15:28,003 - 1:18:03 - 117.6s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.671 , qa loss 0.671 , lm loss 0.000 , avg batch size 4.0
2023-08-06 12:16:33,195 - 1:19:08 - 65.2s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.68 , qa loss 0.68 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:18:31,404 - 1:21:06 - 118.2s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.659 , qa loss 0.659 , lm loss 0.000 , avg batch size 4.0
2023-08-06 12:19:38,031 - 1:22:13 - 66.6s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:21:38,001 - 1:24:13 - 120.0s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.643 , qa loss 0.643 , lm loss 0.000 , avg batch size 4.0
2023-08-06 12:22:43,563 - 1:25:18 - 65.6s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.64 , qa loss 0.64 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:24:45,675 - 1:27:21 - 122.1s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.601 , qa loss 0.601 , lm loss 0.000 , avg batch size 4.0
2023-08-06 12:25:51,427 - 1:28:26 - 65.8s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:27:53,338 - 1:30:28 - 121.9s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.601 , qa loss 0.601 , lm loss 0.000 , avg batch size 4.0
2023-08-06 12:29:00,232 - 1:31:35 - 66.9s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:31:01,711 - 1:33:37 - 121.5s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.573 , qa loss 0.573 , lm loss 0.000 , avg batch size 4.0
2023-08-06 12:32:07,720 - 1:34:43 - 66.0s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:34:08,940 - 1:36:44 - 121.2s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.569 , qa loss 0.569 , lm loss 0.000 , avg batch size 4.0
2023-08-06 12:35:16,748 - 1:37:52 - 67.8s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.57 , qa loss 0.57 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:37:19,682 - 1:39:55 - 122.9s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.559 , qa loss 0.559 , lm loss 0.000 , avg batch size 4.0
2023-08-06 12:38:27,691 - 1:41:03 - 68.0s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:40:30,749 - 1:43:06 - 123.1s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.541 , qa loss 0.541 , lm loss 0.000 , avg batch size 4.0
2023-08-06 12:41:36,991 - 1:44:12 - 66.2s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:43:35,020 - 1:46:10 - 118.0s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.542 , qa loss 0.542 , lm loss 0.000 , avg batch size 4.0
2023-08-06 12:44:39,066 - 1:47:14 - 64.0s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:46:31,575 - 1:49:06 - 112.5s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.523 , qa loss 0.523 , lm loss 0.000 , avg batch size 4.0
2023-08-06 12:47:39,024 - 1:50:14 - 67.4s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:47:40,466 - 1:50:15 - 1.4s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-06 12:47:40,467 - 1:50:15 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-06 12:47:40,626 - 1:50:15 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-08-06 12:47:50,890 - 1:50:26 - 10.3s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-06 12:48:59,389 - 1:51:34 - 68.5s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 4.99 , qa loss 4.99 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:50:05,381 - 1:52:40 - 66.0s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 1.00 , qa loss 1.00 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:51:12,663 - 1:53:47 - 67.3s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.67 , qa loss 0.67 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:52:19,840 - 1:54:55 - 67.2s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:53:28,093 - 1:56:03 - 68.3s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:54:35,510 - 1:57:10 - 67.4s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:55:42,725 - 1:58:18 - 67.2s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:56:49,611 - 1:59:24 - 66.9s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:57:56,516 - 2:00:31 - 66.9s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-06 12:59:02,836 - 2:01:38 - 66.3s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-06 13:00:10,166 - 2:02:45 - 67.3s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-06 13:01:16,798 - 2:03:52 - 66.6s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-06 13:02:23,759 - 2:04:59 - 67.0s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 13:03:30,290 - 2:06:05 - 66.5s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 13:04:35,758 - 2:07:11 - 65.5s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-06 13:05:41,797 - 2:08:17 - 66.0s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-06 13:06:47,734 - 2:09:23 - 65.9s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-06 13:07:53,802 - 2:10:29 - 66.1s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-06 13:08:59,124 - 2:11:34 - 65.3s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-06 13:10:07,242 - 2:12:42 - 68.1s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 02:12:38
CPU Execution time: 02:01:18
................................................................................................................................
Training Adapter + Prefix at prefix length 30
................................................................................................................................
2023-08-06 13:10:16,421 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[2], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout=[], lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-06 13:10:31,160 - 0:00:20 - 14.7s - INFO - __main__ - task: sst, epoch: 20
2023-08-06 13:10:31,160 - 0:00:20 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-06 13:10:36,658 - 0:00:26 - 5.5s - INFO - __main__ - len of test dataset: 1821
2023-08-06 13:10:47,588 - 0:00:36 - 10.9s - INFO - __main__ - score: {'sst': OrderedDict([('em', 82.42723778143876), ('nf1', 82.42723778143876), ('nem', 82.42723778143876)]), 'srl': None, 'woz.en': None}
2023-08-06 13:10:59,894 - 0:00:49 - 12.3s - INFO - __main__ - task: srl, epoch: 20
2023-08-06 13:10:59,894 - 0:00:49 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-06 13:11:05,510 - 0:00:54 - 5.6s - INFO - __main__ - len of test dataset: 2201
2023-08-06 13:43:27,252 - 0:33:16 - 1941.7s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 42.25352112676056), ('nf1', 61.96093911029903), ('nem', 47.342117219445704)]), 'woz.en': None}
2023-08-06 13:43:45,875 - 0:33:35 - 18.6s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-06 13:43:45,876 - 0:33:35 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-06 13:43:53,602 - 0:33:43 - 7.7s - INFO - __main__ - len of test dataset: 1646
2023-08-06 14:03:34,801 - 0:53:24 - 1181.2s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 16.038882138517618), ('nf1', 92.71112477370077), ('nem', 83.47509113001215), ('joint_goal_em', 80.25516403402187), ('turn_request_em', 91.00850546780073), ('turn_goal_em', 89.12515188335358), ('avg_dialogue', 85.63183475091131)])}
................................................................................................................................
 Training Adapter + Prefix at prefix length 40
................................................................................................................................
Available number of GPU = 8 < n_gpus = 12
Continue training with 8 GPUs
2023-08-06 14:03:46,605 - 0:00:08 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[2, 5, 7, 9, 11, 12, 13, 15], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout=[], lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[23592.96, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=8, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=40, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[8257, 11927, 11927, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[8257, 11927, 11927, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-06 14:03:46,605 - 0:00:08 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-06 14:03:46,606 - 0:00:08 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-06 14:03:51,197 - 0:00:13 - 4.6s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 40
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : []
The Flat  : True

[0]
2023-08-06 14:04:07,713 - 0:00:29 - 16.5s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-06 14:05:59,880 - 0:02:21 - 112.2s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.025 , qa loss 2.025 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:07:11,744 - 0:03:33 - 71.9s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.33 , qa loss 1.33 , lm loss 0.00 , avg batch size 4.0
2023-08-06 14:09:00,286 - 0:05:22 - 108.5s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.362 , qa loss 0.362 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:10:11,560 - 0:06:33 - 71.3s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-08-06 14:11:58,314 - 0:08:20 - 106.8s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.321 , qa loss 0.321 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:13:08,946 - 0:09:30 - 70.6s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-06 14:14:57,170 - 0:11:19 - 108.2s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.285 , qa loss 0.285 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:16:09,262 - 0:12:31 - 72.1s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-06 14:17:58,228 - 0:14:20 - 109.0s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.266 , qa loss 0.266 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:19:10,455 - 0:15:32 - 72.2s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-06 14:20:57,024 - 0:17:19 - 106.6s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.255 , qa loss 0.255 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:22:07,505 - 0:18:29 - 70.5s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-06 14:23:52,641 - 0:20:14 - 105.1s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.245 , qa loss 0.245 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:25:03,466 - 0:21:25 - 70.8s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-06 14:26:48,957 - 0:23:10 - 105.5s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.239 , qa loss 0.239 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:27:59,111 - 0:24:21 - 70.2s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-06 14:29:44,009 - 0:26:06 - 104.9s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.227 , qa loss 0.227 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:30:53,544 - 0:27:15 - 69.5s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-06 14:32:39,188 - 0:29:01 - 105.6s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.227 , qa loss 0.227 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:33:48,667 - 0:30:10 - 69.5s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-06 14:35:34,410 - 0:31:56 - 105.7s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.225 , qa loss 0.225 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:36:44,299 - 0:33:06 - 69.9s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-06 14:38:39,699 - 0:35:01 - 115.4s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.212 , qa loss 0.212 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:39:53,363 - 0:36:15 - 73.7s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-06 14:41:45,705 - 0:38:07 - 112.3s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.208 , qa loss 0.208 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:43:00,117 - 0:39:22 - 74.4s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-06 14:44:52,191 - 0:41:14 - 112.1s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.205 , qa loss 0.205 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:46:04,618 - 0:42:26 - 72.4s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 14:47:56,585 - 0:44:18 - 112.0s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.197 , qa loss 0.197 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:49:11,661 - 0:45:33 - 75.1s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 14:51:02,808 - 0:47:24 - 111.1s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.188 , qa loss 0.188 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:52:18,692 - 0:48:40 - 75.9s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-06 14:54:09,370 - 0:50:31 - 110.7s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.199 , qa loss 0.199 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:55:23,659 - 0:51:45 - 74.3s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 14:57:13,547 - 0:53:35 - 109.9s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.182 , qa loss 0.182 , lm loss 0.000 , avg batch size 4.0
2023-08-06 14:58:26,572 - 0:54:48 - 73.0s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-06 15:00:18,160 - 0:56:40 - 111.6s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.189 , qa loss 0.189 , lm loss 0.000 , avg batch size 4.0
2023-08-06 15:01:32,730 - 0:57:54 - 74.6s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-06 15:03:29,985 - 0:59:51 - 117.3s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.181 , qa loss 0.181 , lm loss 0.000 , avg batch size 4.0
2023-08-06 15:04:44,915 - 1:01:06 - 74.9s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-06 15:04:46,759 - 1:01:08 - 1.8s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-06 15:04:46,760 - 1:01:08 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-06 15:04:46,943 - 1:01:08 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-08-06 15:04:59,522 - 1:01:21 - 12.6s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-06 15:07:26,546 - 1:03:48 - 147.0s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 7.616 , qa loss 7.616 , lm loss 0.000 , avg batch size 4.0
2023-08-06 15:08:46,886 - 1:05:08 - 80.3s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 6.14 , qa loss 6.14 , lm loss 0.00 , avg batch size 4.0
2023-08-06 15:11:08,821 - 1:07:30 - 141.9s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 2.389 , qa loss 2.389 , lm loss 0.000 , avg batch size 4.0
2023-08-06 15:12:29,991 - 1:08:51 - 81.2s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 2.17 , qa loss 2.17 , lm loss 0.00 , avg batch size 4.0
2023-08-06 15:14:53,346 - 1:11:15 - 143.4s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 1.499 , qa loss 1.499 , lm loss 0.000 , avg batch size 4.0
2023-08-06 15:16:14,614 - 1:12:36 - 81.3s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 1.43 , qa loss 1.43 , lm loss 0.00 , avg batch size 4.0
2023-08-06 15:18:39,080 - 1:15:01 - 144.5s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 1.170 , qa loss 1.170 , lm loss 0.000 , avg batch size 4.0
2023-08-06 15:19:59,733 - 1:16:21 - 80.7s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 1.13 , qa loss 1.13 , lm loss 0.00 , avg batch size 4.0
2023-08-06 15:22:25,060 - 1:18:47 - 145.3s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.996 , qa loss 0.996 , lm loss 0.000 , avg batch size 4.0
2023-08-06 15:23:44,425 - 1:20:06 - 79.4s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.98 , qa loss 0.98 , lm loss 0.00 , avg batch size 4.0
2023-08-06 15:26:04,813 - 1:22:26 - 140.4s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.895 , qa loss 0.895 , lm loss 0.000 , avg batch size 4.0
2023-08-06 15:27:23,757 - 1:23:45 - 78.9s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.88 , qa loss 0.88 , lm loss 0.00 , avg batch size 4.0
2023-08-06 15:29:46,553 - 1:26:08 - 142.8s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.838 , qa loss 0.838 , lm loss 0.000 , avg batch size 4.0
2023-08-06 15:31:05,561 - 1:27:27 - 79.0s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.83 , qa loss 0.83 , lm loss 0.00 , avg batch size 4.0
2023-08-06 15:33:26,420 - 1:29:48 - 140.9s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.769 , qa loss 0.769 , lm loss 0.000 , avg batch size 4.0
2023-08-06 15:34:47,458 - 1:31:09 - 81.0s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.76 , qa loss 0.76 , lm loss 0.00 , avg batch size 4.0
2023-08-06 15:37:10,472 - 1:33:32 - 143.0s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.737 , qa loss 0.737 , lm loss 0.000 , avg batch size 4.0
2023-08-06 15:38:30,410 - 1:34:52 - 79.9s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.72 , qa loss 0.72 , lm loss 0.00 , avg batch size 4.0
2023-08-06 15:40:53,688 - 1:37:15 - 143.3s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.688 , qa loss 0.688 , lm loss 0.000 , avg batch size 4.0
2023-08-06 15:42:14,994 - 1:38:36 - 81.3s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.68 , qa loss 0.68 , lm loss 0.00 , avg batch size 4.0
2023-08-06 15:44:37,769 - 1:40:59 - 142.8s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.645 , qa loss 0.645 , lm loss 0.000 , avg batch size 4.0
2023-08-06 15:45:57,083 - 1:42:19 - 79.3s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-08-06 15:48:17,869 - 1:44:39 - 140.8s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.626 , qa loss 0.626 , lm loss 0.000 , avg batch size 4.0
2023-08-06 15:49:38,194 - 1:46:00 - 80.3s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-08-06 15:52:01,109 - 1:48:23 - 142.9s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.613 , qa loss 0.613 , lm loss 0.000 , avg batch size 4.0
2023-08-06 15:53:18,869 - 1:49:40 - 77.8s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.60 , qa loss 0.60 , lm loss 0.00 , avg batch size 4.0
2023-08-06 15:55:39,181 - 1:52:01 - 140.3s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.575 , qa loss 0.575 , lm loss 0.000 , avg batch size 4.0
2023-08-06 15:56:57,536 - 1:53:19 - 78.4s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-06 15:59:20,603 - 1:55:42 - 143.1s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.556 , qa loss 0.556 , lm loss 0.000 , avg batch size 4.0
2023-08-06 16:00:38,722 - 1:57:00 - 78.1s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:03:01,085 - 1:59:23 - 142.4s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.551 , qa loss 0.551 , lm loss 0.000 , avg batch size 4.0
2023-08-06 16:04:20,940 - 2:00:42 - 79.9s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:06:49,383 - 2:03:11 - 148.4s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.544 , qa loss 0.544 , lm loss 0.000 , avg batch size 4.0
2023-08-06 16:08:18,485 - 2:04:40 - 89.1s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:11:03,127 - 2:07:25 - 164.6s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.521 , qa loss 0.521 , lm loss 0.000 , avg batch size 4.0
2023-08-06 16:12:25,122 - 2:08:47 - 82.0s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:15:00,958 - 2:11:22 - 155.8s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.499 , qa loss 0.499 , lm loss 0.000 , avg batch size 4.0
2023-08-06 16:16:23,323 - 2:12:45 - 82.4s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:18:53,434 - 2:15:15 - 150.1s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.493 , qa loss 0.493 , lm loss 0.000 , avg batch size 4.0
2023-08-06 16:20:21,609 - 2:16:43 - 88.2s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:20:23,769 - 2:16:45 - 2.2s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-06 16:20:23,770 - 2:16:45 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-06 16:20:23,993 - 2:16:45 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-08-06 16:20:35,940 - 2:16:57 - 11.9s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-06 16:22:12,161 - 2:18:34 - 96.2s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 6.08 , qa loss 6.08 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:23:49,272 - 2:20:11 - 97.1s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 1.08 , qa loss 1.08 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:25:25,299 - 2:21:47 - 96.0s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.74 , qa loss 0.74 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:27:02,132 - 2:23:24 - 96.8s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:28:39,090 - 2:25:01 - 97.0s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:30:17,624 - 2:26:39 - 98.5s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:31:59,686 - 2:28:21 - 102.1s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:33:42,878 - 2:30:04 - 103.2s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:35:27,713 - 2:31:49 - 104.8s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:37:12,648 - 2:33:34 - 104.9s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:38:55,778 - 2:35:17 - 103.1s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:40:40,570 - 2:37:02 - 104.8s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:42:24,302 - 2:38:46 - 103.7s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:44:06,949 - 2:40:28 - 102.6s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:45:51,696 - 2:42:13 - 104.7s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:47:30,040 - 2:43:52 - 98.3s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:48:58,274 - 2:45:20 - 88.2s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:50:28,742 - 2:46:50 - 90.5s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:51:59,781 - 2:48:21 - 91.0s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-06 16:53:30,207 - 2:49:52 - 90.4s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 02:49:45
CPU Execution time: 02:17:32
................................................................................................................................
Training Adapter + Prefix at prefix length 40
................................................................................................................................
2023-08-06 16:53:44,822 - 0:00:10 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[2], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout=[], lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-06 16:54:09,562 - 0:00:34 - 24.7s - INFO - __main__ - task: sst, epoch: 20
2023-08-06 16:54:09,562 - 0:00:34 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-06 16:54:15,478 - 0:00:40 - 5.9s - INFO - __main__ - len of test dataset: 1821
2023-08-06 16:54:27,876 - 0:00:53 - 12.4s - INFO - __main__ - score: {'sst': OrderedDict([('em', 84.1845140032949), ('nf1', 84.1845140032949), ('nem', 84.1845140032949)]), 'srl': None, 'woz.en': None}
2023-08-06 16:54:46,882 - 0:01:12 - 19.0s - INFO - __main__ - task: srl, epoch: 20
2023-08-06 16:54:46,883 - 0:01:12 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-06 16:54:52,741 - 0:01:18 - 5.9s - INFO - __main__ - len of test dataset: 2201
2023-08-06 17:35:48,827 - 0:42:14 - 2456.1s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 42.61699227623807), ('nf1', 63.56440577016434), ('nem', 48.114493412085416)]), 'woz.en': None}
2023-08-06 17:36:07,853 - 0:42:33 - 19.0s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-06 17:36:07,853 - 0:42:33 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-06 17:36:14,217 - 0:42:39 - 6.4s - INFO - __main__ - len of test dataset: 1646
2023-08-06 17:51:59,254 - 0:58:24 - 945.0s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 15.613608748481166), ('nf1', 90.92065319586588), ('nem', 81.34872417982989), ('joint_goal_em', 75.09113001215067), ('turn_request_em', 89.42891859052247), ('turn_goal_em', 87.363304981774), ('avg_dialogue', 82.26002430133657)])}
................................................................................................................................
 Training Adapter + Prefix at prefix length 50
................................................................................................................................
Available number of GPU = 6 < n_gpus = 12
Continue training with 6 GPUs
2023-08-06 17:52:09,528 - 0:00:08 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[2, 5, 9, 12, 13, 15], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout=[], lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[26214.4, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=6, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=50, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[9175, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[9175, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-06 17:52:09,528 - 0:00:08 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-06 17:52:09,528 - 0:00:08 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-06 17:52:12,552 - 0:00:11 - 3.0s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 50
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : []
The Flat  : True

[0]
2023-08-06 17:52:26,057 - 0:00:24 - 13.5s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-06 17:53:56,719 - 0:01:55 - 90.7s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 3.033 , qa loss 3.033 , lm loss 0.000 , avg batch size 4.0
2023-08-06 17:54:51,334 - 0:02:49 - 54.6s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.91 , qa loss 1.91 , lm loss 0.00 , avg batch size 4.0
2023-08-06 17:56:18,400 - 0:04:16 - 87.1s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.323 , qa loss 0.323 , lm loss 0.000 , avg batch size 4.0
2023-08-06 17:57:16,390 - 0:05:14 - 58.0s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-06 17:58:55,287 - 0:06:53 - 98.9s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.280 , qa loss 0.280 , lm loss 0.000 , avg batch size 4.0
2023-08-06 17:59:47,610 - 0:07:46 - 52.3s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:01:11,485 - 0:09:10 - 83.9s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.256 , qa loss 0.256 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:02:03,237 - 0:10:01 - 51.8s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:03:28,345 - 0:11:26 - 85.1s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.249 , qa loss 0.249 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:04:21,605 - 0:12:20 - 53.3s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:05:53,572 - 0:13:52 - 92.0s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.241 , qa loss 0.241 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:06:48,979 - 0:14:47 - 55.4s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:08:17,490 - 0:16:16 - 88.5s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.232 , qa loss 0.232 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:09:11,322 - 0:17:09 - 53.8s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:10:37,763 - 0:18:36 - 86.4s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.223 , qa loss 0.223 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:11:31,697 - 0:19:30 - 53.9s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:12:59,894 - 0:20:58 - 88.2s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.215 , qa loss 0.215 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:13:52,991 - 0:21:51 - 53.1s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:15:20,425 - 0:23:18 - 87.4s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.207 , qa loss 0.207 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:16:12,795 - 0:24:11 - 52.4s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:17:38,855 - 0:25:37 - 86.1s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.199 , qa loss 0.199 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:18:31,086 - 0:26:29 - 52.2s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:19:56,368 - 0:27:54 - 85.3s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.199 , qa loss 0.199 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:20:48,973 - 0:28:47 - 52.6s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:22:14,433 - 0:30:12 - 85.5s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.189 , qa loss 0.189 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:23:06,885 - 0:31:05 - 52.5s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:24:32,272 - 0:32:30 - 85.4s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.198 , qa loss 0.198 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:25:24,691 - 0:33:23 - 52.4s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:26:48,988 - 0:34:47 - 84.3s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.191 , qa loss 0.191 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:27:40,426 - 0:35:38 - 51.4s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:29:05,535 - 0:37:04 - 85.1s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.185 , qa loss 0.185 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:29:58,227 - 0:37:56 - 52.7s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:31:24,918 - 0:39:23 - 86.7s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.185 , qa loss 0.185 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:32:17,049 - 0:40:15 - 52.1s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:33:42,521 - 0:41:41 - 85.5s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.175 , qa loss 0.175 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:34:34,158 - 0:42:32 - 51.6s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:35:58,942 - 0:43:57 - 84.8s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.188 , qa loss 0.188 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:36:51,557 - 0:44:50 - 52.6s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:38:15,607 - 0:46:14 - 84.1s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.180 , qa loss 0.180 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:39:09,274 - 0:47:07 - 53.7s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:39:10,754 - 0:47:09 - 1.5s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-06 18:39:10,754 - 0:47:09 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-06 18:39:10,897 - 0:47:09 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-08-06 18:39:21,785 - 0:47:20 - 10.9s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-06 18:41:17,677 - 0:49:16 - 115.9s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 8.163 , qa loss 8.163 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:42:19,084 - 0:50:17 - 61.4s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 6.59 , qa loss 6.59 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:44:12,799 - 0:52:11 - 113.7s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 2.473 , qa loss 2.473 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:45:14,685 - 0:53:13 - 61.9s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 2.21 , qa loss 2.21 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:47:08,219 - 0:55:06 - 113.5s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 1.445 , qa loss 1.445 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:48:11,204 - 0:56:09 - 63.0s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 1.37 , qa loss 1.37 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:50:06,592 - 0:58:05 - 115.4s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 1.156 , qa loss 1.156 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:51:08,393 - 0:59:06 - 61.8s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 1.12 , qa loss 1.12 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:53:04,517 - 1:01:03 - 116.1s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.953 , qa loss 0.953 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:54:05,211 - 1:02:03 - 60.7s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.95 , qa loss 0.95 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:56:00,475 - 1:03:59 - 115.3s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.879 , qa loss 0.879 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:57:02,870 - 1:05:01 - 62.4s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.87 , qa loss 0.87 , lm loss 0.00 , avg batch size 4.0
2023-08-06 18:58:57,810 - 1:06:56 - 114.9s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.799 , qa loss 0.799 , lm loss 0.000 , avg batch size 4.0
2023-08-06 18:59:59,482 - 1:07:58 - 61.7s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.80 , qa loss 0.80 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:01:57,471 - 1:09:56 - 118.0s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.763 , qa loss 0.763 , lm loss 0.000 , avg batch size 4.0
2023-08-06 19:02:58,904 - 1:10:57 - 61.4s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.76 , qa loss 0.76 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:04:51,898 - 1:12:50 - 113.0s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.703 , qa loss 0.703 , lm loss 0.000 , avg batch size 4.0
2023-08-06 19:05:54,097 - 1:13:52 - 62.2s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.70 , qa loss 0.70 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:07:45,646 - 1:15:44 - 111.5s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.688 , qa loss 0.688 , lm loss 0.000 , avg batch size 4.0
2023-08-06 19:08:49,432 - 1:16:47 - 63.8s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.68 , qa loss 0.68 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:10:43,305 - 1:18:41 - 113.9s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.665 , qa loss 0.665 , lm loss 0.000 , avg batch size 4.0
2023-08-06 19:11:45,962 - 1:19:44 - 62.7s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:13:39,281 - 1:21:37 - 113.3s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.611 , qa loss 0.611 , lm loss 0.000 , avg batch size 4.0
2023-08-06 19:14:42,278 - 1:22:40 - 63.0s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.62 , qa loss 0.62 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:16:39,011 - 1:24:37 - 116.7s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.603 , qa loss 0.603 , lm loss 0.000 , avg batch size 4.0
2023-08-06 19:17:40,790 - 1:25:39 - 61.8s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.60 , qa loss 0.60 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:19:36,965 - 1:27:35 - 116.2s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.585 , qa loss 0.585 , lm loss 0.000 , avg batch size 4.0
2023-08-06 19:20:38,652 - 1:28:37 - 61.7s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:22:53,627 - 1:30:52 - 135.0s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.556 , qa loss 0.556 , lm loss 0.000 , avg batch size 4.0
2023-08-06 19:24:15,807 - 1:32:14 - 82.2s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:26:12,791 - 1:34:11 - 117.0s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.536 , qa loss 0.536 , lm loss 0.000 , avg batch size 4.0
2023-08-06 19:27:35,511 - 1:35:34 - 82.7s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:30:18,577 - 1:38:17 - 163.1s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.524 , qa loss 0.524 , lm loss 0.000 , avg batch size 4.0
2023-08-06 19:31:52,606 - 1:39:51 - 94.0s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:34:33,246 - 1:42:31 - 160.6s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.501 , qa loss 0.501 , lm loss 0.000 , avg batch size 4.0
2023-08-06 19:36:07,202 - 1:44:05 - 94.0s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:38:19,982 - 1:46:18 - 132.8s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.499 , qa loss 0.499 , lm loss 0.000 , avg batch size 4.0
2023-08-06 19:39:22,169 - 1:47:20 - 62.2s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:41:14,891 - 1:49:13 - 112.7s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.495 , qa loss 0.495 , lm loss 0.000 , avg batch size 4.0
2023-08-06 19:42:18,973 - 1:50:17 - 64.1s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:42:20,451 - 1:50:19 - 1.5s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-06 19:42:20,452 - 1:50:19 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-06 19:42:20,655 - 1:50:19 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-08-06 19:42:30,830 - 1:50:29 - 10.2s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-06 19:43:38,601 - 1:51:37 - 67.8s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 5.33 , qa loss 5.33 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:44:45,740 - 1:52:44 - 67.1s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 1.13 , qa loss 1.13 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:45:54,038 - 1:53:52 - 68.3s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.75 , qa loss 0.75 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:47:01,521 - 1:55:00 - 67.5s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:48:10,062 - 1:56:08 - 68.5s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:49:17,557 - 1:57:16 - 67.5s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:50:24,702 - 1:58:23 - 67.1s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:51:31,411 - 1:59:29 - 66.7s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:52:38,210 - 2:00:36 - 66.8s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:53:42,904 - 2:01:41 - 64.7s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:54:49,816 - 2:02:48 - 66.9s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:55:54,953 - 2:03:53 - 65.1s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:57:04,142 - 2:05:02 - 69.2s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:58:11,822 - 2:06:10 - 67.7s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-06 19:59:17,714 - 2:07:16 - 65.9s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-06 20:00:26,027 - 2:08:24 - 68.3s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-06 20:01:33,712 - 2:09:32 - 67.7s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 20:02:37,642 - 2:10:36 - 63.9s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 20:03:44,442 - 2:11:42 - 66.8s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 20:04:52,543 - 2:12:51 - 68.1s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 02:12:44
CPU Execution time: 01:58:54
................................................................................................................................
Training Adapter + Prefix at prefix length 50
................................................................................................................................
2023-08-06 20:05:03,804 - 0:00:08 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[2], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout=[], lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-06 20:05:21,155 - 0:00:25 - 17.4s - INFO - __main__ - task: sst, epoch: 20
2023-08-06 20:05:21,156 - 0:00:25 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-06 20:05:26,873 - 0:00:31 - 5.7s - INFO - __main__ - len of test dataset: 1821
2023-08-06 20:05:36,712 - 0:00:40 - 9.8s - INFO - __main__ - score: {'sst': OrderedDict([('em', 83.47062053816585), ('nf1', 83.47062053816585), ('nem', 83.47062053816585)]), 'srl': None, 'woz.en': None}
2023-08-06 20:05:48,980 - 0:00:53 - 12.3s - INFO - __main__ - task: srl, epoch: 20
2023-08-06 20:05:48,981 - 0:00:53 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-06 20:05:54,264 - 0:00:58 - 5.3s - INFO - __main__ - len of test dataset: 2201
2023-08-06 20:39:43,227 - 0:34:47 - 2029.0s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 42.25352112676056), ('nf1', 62.780798740004364), ('nem', 47.66015447523853)]), 'woz.en': None}
2023-08-06 20:39:55,755 - 0:34:59 - 12.5s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-06 20:39:55,755 - 0:35:00 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-06 20:40:01,104 - 0:35:05 - 5.3s - INFO - __main__ - len of test dataset: 1646
2023-08-06 20:55:14,323 - 0:50:18 - 913.2s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 15.127582017010935), ('nf1', 90.90900897401507), ('nem', 81.53098420413123), ('joint_goal_em', 77.70352369380316), ('turn_request_em', 88.94289185905225), ('turn_goal_em', 88.33535844471446), ('avg_dialogue', 83.32320777642771)])}
................................................................................................................................
 Training Adapter + Prefix at prefix length 60
................................................................................................................................
Available number of GPU = 8 < n_gpus = 12
Continue training with 8 GPUs
2023-08-06 20:55:23,451 - 0:00:07 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[2, 5, 7, 9, 11, 12, 13, 15], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout=[], lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[23592.96, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=8, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=60, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[8257, 11927, 11927, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[8257, 11927, 11927, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-06 20:55:23,451 - 0:00:07 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-06 20:55:23,451 - 0:00:07 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-06 20:55:26,613 - 0:00:10 - 3.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 60
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : []
The Flat  : True

[0]
2023-08-06 20:55:39,645 - 0:00:23 - 13.0s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-06 20:57:13,341 - 0:01:57 - 93.7s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 4.506 , qa loss 4.506 , lm loss 0.000 , avg batch size 4.0
2023-08-06 20:58:10,518 - 0:02:54 - 57.2s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 2.75 , qa loss 2.75 , lm loss 0.00 , avg batch size 4.0
2023-08-06 20:59:41,065 - 0:04:24 - 90.5s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.308 , qa loss 0.308 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:00:39,125 - 0:05:22 - 58.1s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:02:10,571 - 0:06:54 - 91.4s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.277 , qa loss 0.277 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:03:08,170 - 0:07:51 - 57.6s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:04:39,309 - 0:09:22 - 91.1s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.258 , qa loss 0.258 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:05:37,735 - 0:10:21 - 58.4s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:07:09,289 - 0:11:52 - 91.6s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.239 , qa loss 0.239 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:08:07,163 - 0:12:50 - 57.9s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:09:36,629 - 0:14:20 - 89.5s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.227 , qa loss 0.227 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:10:34,088 - 0:15:17 - 57.5s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:12:04,007 - 0:16:47 - 89.9s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.219 , qa loss 0.219 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:13:01,788 - 0:17:45 - 57.8s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:14:32,731 - 0:19:16 - 90.9s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.211 , qa loss 0.211 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:15:30,046 - 0:20:13 - 57.3s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:16:59,376 - 0:21:43 - 89.3s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.209 , qa loss 0.209 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:17:57,914 - 0:22:41 - 58.5s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:19:28,747 - 0:24:12 - 90.8s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.208 , qa loss 0.208 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:20:26,774 - 0:25:10 - 58.0s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:21:58,512 - 0:26:42 - 91.7s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.196 , qa loss 0.196 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:22:55,937 - 0:27:39 - 57.4s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:24:24,490 - 0:29:08 - 88.6s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.203 , qa loss 0.203 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:25:22,437 - 0:30:06 - 57.9s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:26:53,147 - 0:31:36 - 90.7s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.188 , qa loss 0.188 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:27:49,660 - 0:32:33 - 56.5s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:29:21,341 - 0:34:05 - 91.7s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.186 , qa loss 0.186 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:30:18,676 - 0:35:02 - 57.3s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:31:47,774 - 0:36:31 - 89.1s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.181 , qa loss 0.181 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:32:44,288 - 0:37:27 - 56.5s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:34:15,121 - 0:38:58 - 90.8s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.175 , qa loss 0.175 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:35:11,563 - 0:39:55 - 56.4s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:36:42,421 - 0:41:26 - 90.9s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.174 , qa loss 0.174 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:37:39,571 - 0:42:23 - 57.1s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:39:07,763 - 0:43:51 - 88.2s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.174 , qa loss 0.174 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:40:03,191 - 0:44:46 - 55.4s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:41:32,708 - 0:46:16 - 89.5s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.171 , qa loss 0.171 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:42:29,831 - 0:47:13 - 57.1s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:44:02,066 - 0:48:45 - 92.2s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.173 , qa loss 0.173 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:45:00,387 - 0:49:44 - 58.3s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:45:01,685 - 0:49:45 - 1.3s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-06 21:45:01,686 - 0:49:45 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-06 21:45:01,850 - 0:49:45 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-08-06 21:45:12,939 - 0:49:56 - 11.1s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-06 21:47:13,108 - 0:51:56 - 120.2s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 6.844 , qa loss 6.844 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:48:17,108 - 0:53:00 - 64.0s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 5.28 , qa loss 5.28 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:50:16,990 - 0:55:00 - 119.9s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 1.881 , qa loss 1.881 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:51:22,704 - 0:56:06 - 65.7s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 1.73 , qa loss 1.73 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:53:21,589 - 0:58:05 - 118.9s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 1.258 , qa loss 1.258 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:54:26,983 - 0:59:10 - 65.4s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 1.22 , qa loss 1.22 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:56:26,133 - 1:01:09 - 119.1s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 1.044 , qa loss 1.044 , lm loss 0.000 , avg batch size 4.0
2023-08-06 21:57:31,166 - 1:02:14 - 65.0s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 1.02 , qa loss 1.02 , lm loss 0.00 , avg batch size 4.0
2023-08-06 21:59:31,031 - 1:04:14 - 119.9s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.920 , qa loss 0.920 , lm loss 0.000 , avg batch size 4.0
2023-08-06 22:00:35,805 - 1:05:19 - 64.8s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.91 , qa loss 0.91 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:02:33,805 - 1:07:17 - 118.0s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.852 , qa loss 0.852 , lm loss 0.000 , avg batch size 4.0
2023-08-06 22:03:40,303 - 1:08:23 - 66.5s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.83 , qa loss 0.83 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:05:38,605 - 1:10:22 - 118.3s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.760 , qa loss 0.760 , lm loss 0.000 , avg batch size 4.0
2023-08-06 22:06:44,571 - 1:11:28 - 66.0s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.76 , qa loss 0.76 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:08:45,093 - 1:13:28 - 120.5s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.733 , qa loss 0.733 , lm loss 0.000 , avg batch size 4.0
2023-08-06 22:09:50,416 - 1:14:34 - 65.3s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.72 , qa loss 0.72 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:11:47,272 - 1:16:30 - 116.9s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.677 , qa loss 0.677 , lm loss 0.000 , avg batch size 4.0
2023-08-06 22:12:51,542 - 1:17:35 - 64.3s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.69 , qa loss 0.69 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:14:50,190 - 1:19:33 - 118.6s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.650 , qa loss 0.650 , lm loss 0.000 , avg batch size 4.0
2023-08-06 22:15:55,752 - 1:20:39 - 65.6s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:17:54,614 - 1:22:38 - 118.9s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.606 , qa loss 0.606 , lm loss 0.000 , avg batch size 4.0
2023-08-06 22:18:59,191 - 1:23:42 - 64.6s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.62 , qa loss 0.62 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:20:59,039 - 1:25:42 - 119.8s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.600 , qa loss 0.600 , lm loss 0.000 , avg batch size 4.0
2023-08-06 22:22:03,334 - 1:26:47 - 64.3s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.60 , qa loss 0.60 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:24:02,774 - 1:28:46 - 119.4s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.597 , qa loss 0.597 , lm loss 0.000 , avg batch size 4.0
2023-08-06 22:25:08,586 - 1:29:52 - 65.8s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:27:06,396 - 1:31:50 - 117.8s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.571 , qa loss 0.571 , lm loss 0.000 , avg batch size 4.0
2023-08-06 22:28:11,399 - 1:32:55 - 65.0s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:30:10,529 - 1:34:54 - 119.1s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.554 , qa loss 0.554 , lm loss 0.000 , avg batch size 4.0
2023-08-06 22:31:15,712 - 1:35:59 - 65.2s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.55 , qa loss 0.55 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:33:14,739 - 1:37:58 - 119.0s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.530 , qa loss 0.530 , lm loss 0.000 , avg batch size 4.0
2023-08-06 22:34:20,025 - 1:39:03 - 65.3s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:36:17,825 - 1:41:01 - 117.8s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.516 , qa loss 0.516 , lm loss 0.000 , avg batch size 4.0
2023-08-06 22:37:22,858 - 1:42:06 - 65.0s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:39:21,547 - 1:44:05 - 118.7s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.519 , qa loss 0.519 , lm loss 0.000 , avg batch size 4.0
2023-08-06 22:40:26,198 - 1:45:09 - 64.7s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:42:25,268 - 1:47:08 - 119.1s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.492 , qa loss 0.492 , lm loss 0.000 , avg batch size 4.0
2023-08-06 22:43:30,846 - 1:48:14 - 65.6s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:45:29,171 - 1:50:12 - 118.3s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.508 , qa loss 0.508 , lm loss 0.000 , avg batch size 4.0
2023-08-06 22:46:34,193 - 1:51:17 - 65.0s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:46:35,485 - 1:51:19 - 1.3s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-06 22:46:35,485 - 1:51:19 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-06 22:46:35,674 - 1:51:19 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-08-06 22:46:45,781 - 1:51:29 - 10.1s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-06 22:47:58,070 - 1:52:41 - 72.3s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 5.96 , qa loss 5.96 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:49:06,857 - 1:53:50 - 68.8s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 1.14 , qa loss 1.14 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:50:17,053 - 1:55:00 - 70.2s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.79 , qa loss 0.79 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:51:26,091 - 1:56:09 - 69.0s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:52:37,051 - 1:57:20 - 71.0s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:53:45,766 - 1:58:29 - 68.7s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:54:55,083 - 1:59:38 - 69.3s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:56:05,343 - 2:00:49 - 70.3s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:57:15,190 - 2:01:58 - 69.8s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:58:25,341 - 2:03:09 - 70.2s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-06 22:59:35,243 - 2:04:18 - 69.9s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-06 23:00:44,567 - 2:05:28 - 69.3s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-06 23:01:53,607 - 2:06:37 - 69.0s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-06 23:03:03,301 - 2:07:46 - 69.7s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-06 23:04:11,579 - 2:08:55 - 68.3s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-06 23:05:22,166 - 2:10:05 - 70.6s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-06 23:06:31,302 - 2:11:14 - 69.1s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-06 23:07:39,799 - 2:12:23 - 68.5s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-06 23:08:49,879 - 2:13:33 - 70.1s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-06 23:10:00,673 - 2:14:44 - 70.8s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 02:14:38
CPU Execution time: 02:03:11
................................................................................................................................
Training Adapter + Prefix at prefix length 60
................................................................................................................................
2023-08-06 23:10:13,949 - 0:00:09 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[2], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout=[], lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-06 23:10:30,345 - 0:00:26 - 16.4s - INFO - __main__ - task: sst, epoch: 20
2023-08-06 23:10:30,346 - 0:00:26 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-06 23:10:35,602 - 0:00:31 - 5.3s - INFO - __main__ - len of test dataset: 1821
2023-08-06 23:10:47,587 - 0:00:43 - 12.0s - INFO - __main__ - score: {'sst': OrderedDict([('em', 83.08621636463481), ('nf1', 83.08621636463481), ('nem', 83.08621636463481)]), 'srl': None, 'woz.en': None}
2023-08-06 23:11:00,309 - 0:00:56 - 12.7s - INFO - __main__ - task: srl, epoch: 20
2023-08-06 23:11:00,310 - 0:00:56 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-06 23:11:06,535 - 0:01:02 - 6.2s - INFO - __main__ - len of test dataset: 2201
2023-08-06 23:47:27,720 - 0:37:23 - 2181.2s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 41.344843253066784), ('nf1', 61.30483666463245), ('nem', 46.61517492049069)]), 'woz.en': None}
2023-08-06 23:47:39,913 - 0:37:35 - 12.2s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-06 23:47:39,913 - 0:37:35 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-06 23:47:46,066 - 0:37:41 - 6.2s - INFO - __main__ - len of test dataset: 1646
2023-08-07 00:05:07,223 - 0:55:03 - 1041.2s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 14.823815309842042), ('nf1', 90.63329238602633), ('nem', 80.1944106925881), ('joint_goal_em', 74.30133657351155), ('turn_request_em', 88.45686512758202), ('turn_goal_em', 86.45200486026732), ('avg_dialogue', 81.37910085054679)])}
................................................................................................................................
 Training Adapter + Prefix at prefix length 80
................................................................................................................................
Available number of GPU = 5 < n_gpus = 12
Continue training with 5 GPUs
2023-08-07 00:05:20,102 - 0:00:10 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[2, 5, 7, 11, 13], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout=[], lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[27525.12, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=5, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=80, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[9633, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[9633, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-07 00:05:20,102 - 0:00:10 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-07 00:05:20,102 - 0:00:10 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-07 00:05:24,223 - 0:00:14 - 4.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 80
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : []
The Flat  : True

[0]
2023-08-07 00:05:38,760 - 0:00:29 - 14.5s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-07 00:07:24,172 - 0:02:14 - 105.4s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.402 , qa loss 2.402 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:08:28,018 - 0:03:18 - 63.8s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.54 , qa loss 1.54 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:10:06,866 - 0:04:57 - 98.8s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.332 , qa loss 0.332 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:11:02,916 - 0:05:53 - 56.0s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:12:29,413 - 0:07:19 - 86.5s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.275 , qa loss 0.275 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:13:24,456 - 0:08:14 - 55.0s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:15:01,905 - 0:09:52 - 97.4s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.253 , qa loss 0.253 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:16:03,596 - 0:10:53 - 61.7s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:17:40,751 - 0:12:31 - 97.2s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.231 , qa loss 0.231 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:18:43,193 - 0:13:33 - 62.4s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:20:24,877 - 0:15:15 - 101.7s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.224 , qa loss 0.224 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:21:28,860 - 0:16:19 - 64.0s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:23:06,821 - 0:17:57 - 98.0s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.216 , qa loss 0.216 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:24:09,017 - 0:18:59 - 62.2s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:25:45,478 - 0:20:35 - 96.5s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.218 , qa loss 0.218 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:26:47,972 - 0:21:38 - 62.5s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:28:26,981 - 0:23:17 - 99.0s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.209 , qa loss 0.209 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:29:28,218 - 0:24:18 - 61.2s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:31:03,209 - 0:25:53 - 95.0s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.220 , qa loss 0.220 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:32:02,142 - 0:26:52 - 58.9s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:33:35,728 - 0:28:25 - 93.6s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.197 , qa loss 0.197 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:34:40,198 - 0:29:30 - 64.5s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:36:14,427 - 0:31:04 - 94.2s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.198 , qa loss 0.198 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:37:12,034 - 0:32:02 - 57.6s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:38:47,323 - 0:33:37 - 95.3s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.186 , qa loss 0.186 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:39:48,384 - 0:34:38 - 61.1s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:41:24,412 - 0:36:14 - 96.0s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.185 , qa loss 0.185 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:42:25,991 - 0:37:16 - 61.6s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:44:05,888 - 0:38:56 - 99.9s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.173 , qa loss 0.173 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:45:08,489 - 0:39:58 - 62.6s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:46:48,222 - 0:41:38 - 99.7s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.171 , qa loss 0.171 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:47:51,534 - 0:42:41 - 63.3s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:49:31,358 - 0:44:21 - 99.8s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.169 , qa loss 0.169 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:50:36,871 - 0:45:27 - 65.5s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:52:16,697 - 0:47:06 - 99.8s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.171 , qa loss 0.171 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:53:26,081 - 0:48:16 - 69.4s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:54:59,208 - 0:49:49 - 93.1s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.181 , qa loss 0.181 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:55:57,163 - 0:50:47 - 58.0s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:57:32,164 - 0:52:22 - 95.0s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.166 , qa loss 0.166 , lm loss 0.000 , avg batch size 4.0
2023-08-07 00:58:31,403 - 0:53:21 - 59.2s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-07 00:58:32,851 - 0:53:23 - 1.4s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-07 00:58:32,852 - 0:53:23 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-07 00:58:33,025 - 0:53:23 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-08-07 00:58:44,909 - 0:53:35 - 11.9s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-07 01:00:47,018 - 0:55:37 - 122.1s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 7.396 , qa loss 7.396 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:01:54,354 - 0:56:44 - 67.3s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 5.77 , qa loss 5.77 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:03:55,385 - 0:58:45 - 121.0s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 2.080 , qa loss 2.080 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:05:01,343 - 0:59:51 - 66.0s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 1.89 , qa loss 1.89 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:07:03,624 - 1:01:53 - 122.3s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 1.329 , qa loss 1.329 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:08:09,369 - 1:02:59 - 65.7s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 1.28 , qa loss 1.28 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:10:09,545 - 1:04:59 - 120.2s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 1.077 , qa loss 1.077 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:11:16,503 - 1:06:06 - 67.0s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 1.05 , qa loss 1.05 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:13:18,069 - 1:08:08 - 121.6s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.940 , qa loss 0.940 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:14:25,528 - 1:09:15 - 67.5s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.93 , qa loss 0.93 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:16:28,943 - 1:11:19 - 123.4s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.858 , qa loss 0.858 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:17:33,669 - 1:12:23 - 64.7s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.85 , qa loss 0.85 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:19:36,770 - 1:14:27 - 123.1s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.802 , qa loss 0.802 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:20:44,266 - 1:15:34 - 67.5s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.80 , qa loss 0.80 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:22:46,069 - 1:17:36 - 121.8s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.735 , qa loss 0.735 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:23:53,302 - 1:18:43 - 67.2s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.74 , qa loss 0.74 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:25:54,551 - 1:20:44 - 121.2s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.703 , qa loss 0.703 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:27:01,074 - 1:21:51 - 66.5s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.69 , qa loss 0.69 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:29:04,284 - 1:23:54 - 123.2s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.681 , qa loss 0.681 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:30:09,397 - 1:24:59 - 65.1s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.67 , qa loss 0.67 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:32:10,726 - 1:27:00 - 121.3s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.644 , qa loss 0.644 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:33:16,198 - 1:28:06 - 65.5s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:35:18,185 - 1:30:08 - 122.0s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.604 , qa loss 0.604 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:36:22,442 - 1:31:12 - 64.3s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:38:26,118 - 1:33:16 - 123.7s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.607 , qa loss 0.607 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:39:31,691 - 1:34:21 - 65.6s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.60 , qa loss 0.60 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:41:34,938 - 1:36:25 - 123.2s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.561 , qa loss 0.561 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:42:41,503 - 1:37:31 - 66.6s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:44:43,982 - 1:39:34 - 122.5s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.535 , qa loss 0.535 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:45:50,314 - 1:40:40 - 66.3s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:47:52,003 - 1:42:42 - 121.7s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.524 , qa loss 0.524 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:48:59,131 - 1:43:49 - 67.1s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:51:01,535 - 1:45:51 - 122.4s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.503 , qa loss 0.503 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:52:07,055 - 1:46:57 - 65.5s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:54:07,333 - 1:48:57 - 120.3s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.497 , qa loss 0.497 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:55:13,050 - 1:50:03 - 65.7s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-07 01:57:13,938 - 1:52:04 - 120.9s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.501 , qa loss 0.501 , lm loss 0.000 , avg batch size 4.0
2023-08-07 01:58:21,003 - 1:53:11 - 67.1s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:00:23,610 - 1:55:13 - 122.6s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.484 , qa loss 0.484 , lm loss 0.000 , avg batch size 4.0
2023-08-07 02:01:30,445 - 1:56:20 - 66.8s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:01:31,880 - 1:56:22 - 1.4s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-07 02:01:31,881 - 1:56:22 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-07 02:01:32,051 - 1:56:22 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-08-07 02:01:42,960 - 1:56:33 - 10.9s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-07 02:02:58,998 - 1:57:49 - 76.0s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 5.68 , qa loss 5.68 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:04:13,478 - 1:59:03 - 74.5s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 1.03 , qa loss 1.03 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:05:26,006 - 2:00:16 - 72.5s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.70 , qa loss 0.70 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:06:40,228 - 2:01:30 - 74.2s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.57 , qa loss 0.57 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:07:52,986 - 2:02:43 - 72.8s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:09:06,648 - 2:03:56 - 73.7s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:10:20,791 - 2:05:11 - 74.1s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:11:34,525 - 2:06:24 - 73.7s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:12:47,756 - 2:07:38 - 73.2s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:14:01,373 - 2:08:51 - 73.6s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:15:15,443 - 2:10:05 - 74.1s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:16:29,018 - 2:11:19 - 73.6s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:17:43,180 - 2:12:33 - 74.2s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:18:58,419 - 2:13:48 - 75.2s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:20:13,013 - 2:15:03 - 74.6s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:21:26,720 - 2:16:16 - 73.7s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:22:39,349 - 2:17:29 - 72.6s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:23:53,102 - 2:18:43 - 73.8s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:25:08,127 - 2:19:58 - 75.0s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-07 02:26:22,624 - 2:21:12 - 74.5s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 02:21:03
CPU Execution time: 02:07:26
................................................................................................................................
Training Adapter + Prefix at prefix length 80
................................................................................................................................
2023-08-07 02:26:33,918 - 0:00:07 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[2], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout=[], lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-07 02:26:52,867 - 0:00:26 - 18.9s - INFO - __main__ - task: sst, epoch: 20
2023-08-07 02:26:52,867 - 0:00:26 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-07 02:26:58,704 - 0:00:32 - 5.8s - INFO - __main__ - len of test dataset: 1821
2023-08-07 02:27:09,764 - 0:00:43 - 11.1s - INFO - __main__ - score: {'sst': OrderedDict([('em', 83.14113124656782), ('nf1', 83.14113124656782), ('nem', 83.14113124656782)]), 'srl': None, 'woz.en': None}
2023-08-07 02:27:24,019 - 0:00:57 - 14.3s - INFO - __main__ - task: srl, epoch: 20
2023-08-07 02:27:24,020 - 0:00:57 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-07 02:27:29,561 - 0:01:03 - 5.5s - INFO - __main__ - len of test dataset: 2201
2023-08-07 03:04:02,544 - 0:37:36 - 2193.0s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 42.29895502044525), ('nf1', 62.19567521389512), ('nem', 47.296683325761016)]), 'woz.en': None}
2023-08-07 03:04:15,577 - 0:37:49 - 13.0s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-07 03:04:15,578 - 0:37:49 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-07 03:04:21,961 - 0:37:55 - 6.4s - INFO - __main__ - len of test dataset: 1646
2023-08-07 03:20:58,906 - 0:54:32 - 996.9s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 15.127582017010935), ('nf1', 90.64891433664219), ('nem', 80.61968408262454), ('joint_goal_em', 76.00243013365736), ('turn_request_em', 88.94289185905225), ('turn_goal_em', 87.12029161603888), ('avg_dialogue', 82.4726609963548)])}
................................................................................................................................
 Training Adapter + Prefix at prefix length 100
................................................................................................................................
Available number of GPU = 5 < n_gpus = 12
Continue training with 5 GPUs
2023-08-07 03:21:10,431 - 0:00:09 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[2, 5, 7, 12, 15], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout=[], lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[27525.12, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=5, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=100, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[9633, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[9633, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-07 03:21:10,431 - 0:00:09 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-07 03:21:10,431 - 0:00:09 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-07 03:21:14,126 - 0:00:12 - 3.7s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 100
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : []
The Flat  : True

[0]
2023-08-07 03:21:26,547 - 0:00:25 - 12.4s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-07 03:23:07,363 - 0:02:06 - 100.8s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.910 , qa loss 2.910 , lm loss 0.000 , avg batch size 4.0
2023-08-07 03:24:10,578 - 0:03:09 - 63.2s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.85 , qa loss 1.85 , lm loss 0.00 , avg batch size 4.0
2023-08-07 03:25:48,593 - 0:04:47 - 98.0s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.353 , qa loss 0.353 , lm loss 0.000 , avg batch size 4.0
2023-08-07 03:26:50,801 - 0:05:49 - 62.2s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-08-07 03:28:28,355 - 0:07:27 - 97.6s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.281 , qa loss 0.281 , lm loss 0.000 , avg batch size 4.0
2023-08-07 03:29:30,920 - 0:08:29 - 62.6s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-07 03:31:04,503 - 0:10:03 - 93.6s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.253 , qa loss 0.253 , lm loss 0.000 , avg batch size 4.0
2023-08-07 03:32:06,626 - 0:11:05 - 62.1s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-07 03:33:43,881 - 0:12:42 - 97.3s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.243 , qa loss 0.243 , lm loss 0.000 , avg batch size 4.0
2023-08-07 03:34:49,007 - 0:13:47 - 65.1s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-07 03:36:37,152 - 0:15:35 - 108.1s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.228 , qa loss 0.228 , lm loss 0.000 , avg batch size 4.0
2023-08-07 03:37:46,453 - 0:16:45 - 69.3s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-07 03:39:35,071 - 0:18:33 - 108.6s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.219 , qa loss 0.219 , lm loss 0.000 , avg batch size 4.0
2023-08-07 03:40:44,175 - 0:19:42 - 69.1s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-07 03:42:33,327 - 0:21:32 - 109.2s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.211 , qa loss 0.211 , lm loss 0.000 , avg batch size 4.0
2023-08-07 03:43:42,858 - 0:22:41 - 69.5s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-07 03:45:31,225 - 0:24:30 - 108.4s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.204 , qa loss 0.204 , lm loss 0.000 , avg batch size 4.0
2023-08-07 03:46:41,216 - 0:25:40 - 70.0s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-07 03:48:29,979 - 0:27:28 - 108.8s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.195 , qa loss 0.195 , lm loss 0.000 , avg batch size 4.0
2023-08-07 03:49:41,191 - 0:28:39 - 71.2s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-07 03:51:31,467 - 0:30:30 - 110.3s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.186 , qa loss 0.186 , lm loss 0.000 , avg batch size 4.0
2023-08-07 03:52:41,398 - 0:31:40 - 69.9s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-07 03:54:24,070 - 0:33:22 - 102.7s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.186 , qa loss 0.186 , lm loss 0.000 , avg batch size 4.0
2023-08-07 03:55:35,486 - 0:34:34 - 71.4s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-07 03:57:24,500 - 0:36:23 - 109.0s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.182 , qa loss 0.182 , lm loss 0.000 , avg batch size 4.0
2023-08-07 03:58:34,708 - 0:37:33 - 70.2s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-07 04:00:22,689 - 0:39:21 - 108.0s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.175 , qa loss 0.175 , lm loss 0.000 , avg batch size 4.0
2023-08-07 04:01:32,925 - 0:40:31 - 70.2s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-07 04:03:22,021 - 0:42:20 - 109.1s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.179 , qa loss 0.179 , lm loss 0.000 , avg batch size 4.0
2023-08-07 04:04:32,270 - 0:43:31 - 70.2s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-07 04:06:21,960 - 0:45:20 - 109.7s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.168 , qa loss 0.168 , lm loss 0.000 , avg batch size 4.0
2023-08-07 04:07:33,644 - 0:46:32 - 71.7s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-07 04:09:22,598 - 0:48:21 - 109.0s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.173 , qa loss 0.173 , lm loss 0.000 , avg batch size 4.0
2023-08-07 04:10:33,592 - 0:49:32 - 71.0s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-07 04:12:19,486 - 0:51:18 - 105.9s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.166 , qa loss 0.166 , lm loss 0.000 , avg batch size 4.0
2023-08-07 04:13:29,322 - 0:52:28 - 69.8s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-07 04:15:18,088 - 0:54:16 - 108.8s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.163 , qa loss 0.163 , lm loss 0.000 , avg batch size 4.0
2023-08-07 04:16:25,113 - 0:55:23 - 67.0s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-07 04:18:13,317 - 0:57:12 - 108.2s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.160 , qa loss 0.160 , lm loss 0.000 , avg batch size 4.0
2023-08-07 04:19:25,879 - 0:58:24 - 72.6s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-07 04:19:27,365 - 0:58:26 - 1.5s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-07 04:19:27,366 - 0:58:26 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-07 04:19:27,548 - 0:58:26 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-08-07 04:19:38,017 - 0:58:36 - 10.5s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-07 04:22:13,558 - 1:01:12 - 155.5s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 6.830 , qa loss 6.830 , lm loss 0.000 , avg batch size 4.0
2023-08-07 04:23:36,835 - 1:02:35 - 83.3s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 5.31 , qa loss 5.31 , lm loss 0.00 , avg batch size 4.0
2023-08-07 04:26:10,276 - 1:05:09 - 153.4s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 1.845 , qa loss 1.845 , lm loss 0.000 , avg batch size 4.0
2023-08-07 04:27:34,034 - 1:06:32 - 83.8s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 1.70 , qa loss 1.70 , lm loss 0.00 , avg batch size 4.0
2023-08-07 04:30:06,258 - 1:09:05 - 152.2s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 1.249 , qa loss 1.249 , lm loss 0.000 , avg batch size 4.0
2023-08-07 04:31:26,220 - 1:10:25 - 80.0s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 1.18 , qa loss 1.18 , lm loss 0.00 , avg batch size 4.0
2023-08-07 04:33:57,617 - 1:12:56 - 151.4s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 1.011 , qa loss 1.011 , lm loss 0.000 , avg batch size 4.0
2023-08-07 04:35:21,675 - 1:14:20 - 84.1s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.98 , qa loss 0.98 , lm loss 0.00 , avg batch size 4.0
2023-08-07 04:37:52,771 - 1:16:51 - 151.1s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.888 , qa loss 0.888 , lm loss 0.000 , avg batch size 4.0
2023-08-07 04:39:18,518 - 1:18:17 - 85.7s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.87 , qa loss 0.87 , lm loss 0.00 , avg batch size 4.0
2023-08-07 04:41:47,213 - 1:20:46 - 148.7s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.807 , qa loss 0.807 , lm loss 0.000 , avg batch size 4.0
2023-08-07 04:43:08,417 - 1:22:07 - 81.2s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.81 , qa loss 0.81 , lm loss 0.00 , avg batch size 4.0
2023-08-07 04:45:39,383 - 1:24:38 - 151.0s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.727 , qa loss 0.727 , lm loss 0.000 , avg batch size 4.0
2023-08-07 04:47:05,076 - 1:26:03 - 85.7s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.73 , qa loss 0.73 , lm loss 0.00 , avg batch size 4.0
2023-08-07 04:49:37,305 - 1:28:36 - 152.2s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.693 , qa loss 0.693 , lm loss 0.000 , avg batch size 4.0
2023-08-07 04:50:59,250 - 1:29:58 - 81.9s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.70 , qa loss 0.70 , lm loss 0.00 , avg batch size 4.0
2023-08-07 04:53:28,488 - 1:32:27 - 149.2s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.667 , qa loss 0.667 , lm loss 0.000 , avg batch size 4.0
2023-08-07 04:54:53,797 - 1:33:52 - 85.3s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.67 , qa loss 0.67 , lm loss 0.00 , avg batch size 4.0
2023-08-07 04:57:25,323 - 1:36:24 - 151.5s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.633 , qa loss 0.633 , lm loss 0.000 , avg batch size 4.0
2023-08-07 04:58:50,748 - 1:37:49 - 85.4s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:01:25,148 - 1:40:23 - 154.4s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.582 , qa loss 0.582 , lm loss 0.000 , avg batch size 4.0
2023-08-07 05:02:49,492 - 1:41:48 - 84.3s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:05:18,382 - 1:44:17 - 148.9s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.575 , qa loss 0.575 , lm loss 0.000 , avg batch size 4.0
2023-08-07 05:06:39,233 - 1:45:38 - 80.9s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:09:12,813 - 1:48:11 - 153.6s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.534 , qa loss 0.534 , lm loss 0.000 , avg batch size 4.0
2023-08-07 05:10:36,275 - 1:49:35 - 83.5s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:13:06,039 - 1:52:04 - 149.8s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.519 , qa loss 0.519 , lm loss 0.000 , avg batch size 4.0
2023-08-07 05:14:32,733 - 1:53:31 - 86.7s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:17:04,059 - 1:56:02 - 151.3s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.489 , qa loss 0.489 , lm loss 0.000 , avg batch size 4.0
2023-08-07 05:18:28,902 - 1:57:27 - 84.8s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:21:00,819 - 1:59:59 - 151.9s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.487 , qa loss 0.487 , lm loss 0.000 , avg batch size 4.0
2023-08-07 05:22:27,032 - 2:01:25 - 86.2s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:24:56,176 - 2:03:54 - 149.1s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.464 , qa loss 0.464 , lm loss 0.000 , avg batch size 4.0
2023-08-07 05:26:22,316 - 2:05:21 - 86.1s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:28:53,882 - 2:07:52 - 151.6s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.453 , qa loss 0.453 , lm loss 0.000 , avg batch size 4.0
2023-08-07 05:30:14,225 - 2:09:13 - 80.3s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:32:40,826 - 2:11:39 - 146.6s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.445 , qa loss 0.445 , lm loss 0.000 , avg batch size 4.0
2023-08-07 05:34:07,489 - 2:13:06 - 86.7s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:36:38,382 - 2:15:37 - 150.9s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.446 , qa loss 0.446 , lm loss 0.000 , avg batch size 4.0
2023-08-07 05:38:05,434 - 2:17:04 - 87.1s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:38:06,890 - 2:17:05 - 1.5s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-07 05:38:06,891 - 2:17:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-07 05:38:07,085 - 2:17:05 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-08-07 05:38:16,310 - 2:17:15 - 9.2s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-07 05:39:38,917 - 2:18:37 - 82.6s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 5.72 , qa loss 5.72 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:41:02,206 - 2:20:00 - 83.3s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.85 , qa loss 0.85 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:42:23,889 - 2:21:22 - 81.7s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.62 , qa loss 0.62 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:43:46,785 - 2:22:45 - 82.9s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:45:09,339 - 2:24:08 - 82.6s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:46:30,727 - 2:25:29 - 81.4s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:47:51,857 - 2:26:50 - 81.1s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:49:10,748 - 2:28:09 - 78.9s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:50:31,404 - 2:29:30 - 80.7s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:51:50,881 - 2:30:49 - 79.5s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:53:09,577 - 2:32:08 - 78.7s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:54:30,594 - 2:33:29 - 81.0s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:55:51,273 - 2:34:50 - 80.7s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:57:12,362 - 2:36:11 - 81.1s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:58:32,520 - 2:37:31 - 80.2s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-07 05:59:52,930 - 2:38:51 - 80.4s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-07 06:01:13,223 - 2:40:12 - 80.3s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-07 06:02:32,534 - 2:41:31 - 79.3s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-07 06:03:50,488 - 2:42:49 - 78.0s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-07 06:05:10,108 - 2:44:08 - 79.6s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 02:44:01
CPU Execution time: 02:30:47
................................................................................................................................
Training Adapter + Prefix at prefix length 100
................................................................................................................................
2023-08-07 06:05:22,335 - 0:00:08 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[2], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout=[], lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-07 06:05:40,124 - 0:00:26 - 17.8s - INFO - __main__ - task: sst, epoch: 20
2023-08-07 06:05:40,125 - 0:00:26 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-07 06:05:45,483 - 0:00:31 - 5.4s - INFO - __main__ - len of test dataset: 1821
2023-08-07 06:05:57,637 - 0:00:43 - 12.2s - INFO - __main__ - score: {'sst': OrderedDict([('em', 83.08621636463481), ('nf1', 83.08621636463481), ('nem', 83.08621636463481)]), 'srl': None, 'woz.en': None}
2023-08-07 06:06:12,120 - 0:00:58 - 14.5s - INFO - __main__ - task: srl, epoch: 20
2023-08-07 06:06:12,120 - 0:00:58 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-07 06:06:18,372 - 0:01:04 - 6.3s - INFO - __main__ - len of test dataset: 2201
2023-08-07 06:44:30,490 - 0:39:16 - 2292.1s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 43.88914129940936), ('nf1', 63.84367565367926), ('nem', 48.65970013630168)]), 'woz.en': None}
2023-08-07 06:44:44,464 - 0:39:30 - 14.0s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-07 06:44:44,464 - 0:39:30 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-07 06:44:50,479 - 0:39:36 - 6.0s - INFO - __main__ - len of test dataset: 1646
2023-08-07 07:02:00,475 - 0:56:46 - 1030.0s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 15.066828675577156), ('nf1', 90.96329888553466), ('nem', 81.591737545565), ('joint_goal_em', 78.1895504252734), ('turn_request_em', 89.24665856622114), ('turn_goal_em', 88.21385176184691), ('avg_dialogue', 83.71810449574727)])}
