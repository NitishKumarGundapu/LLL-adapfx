Available number of GPU = 2 < n_gpus = 12
Continue training with 2 GPUs
2023-07-28 22:51:06,452 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[7, 15], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[31457.28, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=2, n_train_epochs={'sst': 10, 'srl': 10, 'woz.en': 10}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[11010, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[11010, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-07-28 22:51:06,452 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-07-28 22:51:06,452 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-28 22:51:09,309 - 0:00:08 - 2.9s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
[0]
2023-07-28 22:51:42,320 - 0:00:41 - 33.0s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 69200
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-07-28 22:53:10,757 - 0:02:09 - 88.4s - INFO - __main__ - progress 0.578 , lr 5.9E-05 , loss 1.438 , qa loss 1.438 , lm loss 0.000 , avg batch size 4.0
2023-07-28 22:54:02,032 - 0:03:01 - 51.3s - INFO - __main__ - epoch 1/10 done , tot steps 1730 , lr 5.6E-05 , loss 0.95 , qa loss 0.95 , lm loss 0.00 , avg batch size 4.0
2023-07-28 22:55:19,315 - 0:04:18 - 77.3s - INFO - __main__ - progress 1.578 , lr 5.3E-05 , loss 0.242 , qa loss 0.242 , lm loss 0.000 , avg batch size 4.0
2023-07-28 22:56:05,086 - 0:05:04 - 45.8s - INFO - __main__ - epoch 2/10 done , tot steps 3460 , lr 5.0E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-07-28 22:57:20,255 - 0:06:19 - 75.2s - INFO - __main__ - progress 2.578 , lr 4.6E-05 , loss 0.211 , qa loss 0.211 , lm loss 0.000 , avg batch size 4.0
2023-07-28 22:58:06,606 - 0:07:05 - 46.4s - INFO - __main__ - epoch 3/10 done , tot steps 5190 , lr 4.4E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-07-28 22:59:21,183 - 0:08:20 - 74.6s - INFO - __main__ - progress 3.578 , lr 4.0E-05 , loss 0.186 , qa loss 0.186 , lm loss 0.000 , avg batch size 4.0
2023-07-28 23:00:09,166 - 0:09:08 - 48.0s - INFO - __main__ - epoch 4/10 done , tot steps 6920 , lr 3.8E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:01:25,861 - 0:10:24 - 76.7s - INFO - __main__ - progress 4.578 , lr 3.4E-05 , loss 0.192 , qa loss 0.192 , lm loss 0.000 , avg batch size 4.0
2023-07-28 23:02:15,350 - 0:11:14 - 49.5s - INFO - __main__ - epoch 5/10 done , tot steps 8650 , lr 3.1E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:03:32,712 - 0:12:31 - 77.4s - INFO - __main__ - progress 5.578 , lr 2.8E-05 , loss 0.165 , qa loss 0.165 , lm loss 0.000 , avg batch size 4.0
2023-07-28 23:04:20,498 - 0:13:19 - 47.8s - INFO - __main__ - epoch 6/10 done , tot steps 10380 , lr 2.5E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:05:29,765 - 0:14:28 - 69.3s - INFO - __main__ - progress 6.578 , lr 2.1E-05 , loss 0.151 , qa loss 0.151 , lm loss 0.000 , avg batch size 4.0
2023-07-28 23:06:12,035 - 0:15:11 - 42.3s - INFO - __main__ - epoch 7/10 done , tot steps 12110 , lr 1.9E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:07:22,414 - 0:16:21 - 70.4s - INFO - __main__ - progress 7.578 , lr 1.5E-05 , loss 0.147 , qa loss 0.147 , lm loss 0.000 , avg batch size 4.0
2023-07-28 23:08:05,697 - 0:17:04 - 43.3s - INFO - __main__ - epoch 8/10 done , tot steps 13840 , lr 1.3E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:09:14,125 - 0:18:13 - 68.4s - INFO - __main__ - progress 8.578 , lr 8.9E-06 , loss 0.141 , qa loss 0.141 , lm loss 0.000 , avg batch size 4.0
2023-07-28 23:09:57,741 - 0:18:56 - 43.6s - INFO - __main__ - epoch 9/10 done , tot steps 15570 , lr 6.3E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:11:07,226 - 0:20:06 - 69.5s - INFO - __main__ - progress 9.578 , lr 2.7E-06 , loss 0.135 , qa loss 0.135 , lm loss 0.000 , avg batch size 4.0
2023-07-28 23:11:50,257 - 0:20:49 - 43.0s - INFO - __main__ - epoch 10/10 done , tot steps 17300 , lr 3.1E-08 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:11:51,363 - 0:20:50 - 1.1s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-07-28 23:11:51,364 - 0:20:50 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-28 23:11:51,507 - 0:20:50 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-07-28 23:12:06,097 - 0:21:05 - 14.6s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 64140
2023-07-28 23:13:46,545 - 0:22:45 - 100.4s - INFO - __main__ - progress 0.624 , lr 5.9E-05 , loss 2.931 , qa loss 2.931 , lm loss 0.000 , avg batch size 4.0
2023-07-28 23:14:39,857 - 0:23:38 - 53.3s - INFO - __main__ - epoch 1/10 done , tot steps 1604 , lr 5.6E-05 , loss 2.28 , qa loss 2.28 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:16:18,107 - 0:25:17 - 98.3s - INFO - __main__ - progress 1.624 , lr 5.2E-05 , loss 1.021 , qa loss 1.021 , lm loss 0.000 , avg batch size 4.0
2023-07-28 23:17:13,193 - 0:26:12 - 55.1s - INFO - __main__ - epoch 2/10 done , tot steps 3208 , lr 5.0E-05 , loss 0.99 , qa loss 0.99 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:18:50,893 - 0:27:50 - 97.7s - INFO - __main__ - progress 2.624 , lr 4.6E-05 , loss 0.852 , qa loss 0.852 , lm loss 0.000 , avg batch size 4.0
2023-07-28 23:19:46,009 - 0:28:45 - 55.1s - INFO - __main__ - epoch 3/10 done , tot steps 4812 , lr 4.4E-05 , loss 0.84 , qa loss 0.84 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:21:26,665 - 0:30:25 - 100.7s - INFO - __main__ - progress 3.624 , lr 4.0E-05 , loss 0.790 , qa loss 0.790 , lm loss 0.000 , avg batch size 4.0
2023-07-28 23:22:20,061 - 0:31:19 - 53.4s - INFO - __main__ - epoch 4/10 done , tot steps 6416 , lr 3.8E-05 , loss 0.78 , qa loss 0.78 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:24:00,362 - 0:32:59 - 100.3s - INFO - __main__ - progress 4.624 , lr 3.4E-05 , loss 0.754 , qa loss 0.754 , lm loss 0.000 , avg batch size 4.0
2023-07-28 23:24:53,793 - 0:33:52 - 53.4s - INFO - __main__ - epoch 5/10 done , tot steps 8020 , lr 3.1E-05 , loss 0.73 , qa loss 0.73 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:26:33,805 - 0:35:32 - 100.0s - INFO - __main__ - progress 5.624 , lr 2.7E-05 , loss 0.688 , qa loss 0.688 , lm loss 0.000 , avg batch size 4.0
2023-07-28 23:27:28,475 - 0:36:27 - 54.7s - INFO - __main__ - epoch 6/10 done , tot steps 9624 , lr 2.5E-05 , loss 0.69 , qa loss 0.69 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:29:09,207 - 0:38:08 - 100.7s - INFO - __main__ - progress 6.624 , lr 2.1E-05 , loss 0.657 , qa loss 0.657 , lm loss 0.000 , avg batch size 4.0
2023-07-28 23:30:04,931 - 0:39:04 - 55.7s - INFO - __main__ - epoch 7/10 done , tot steps 11228 , lr 1.9E-05 , loss 0.66 , qa loss 0.66 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:31:45,129 - 0:40:44 - 100.2s - INFO - __main__ - progress 7.624 , lr 1.5E-05 , loss 0.643 , qa loss 0.643 , lm loss 0.000 , avg batch size 4.0
2023-07-28 23:32:39,802 - 0:41:38 - 54.7s - INFO - __main__ - epoch 8/10 done , tot steps 12832 , lr 1.3E-05 , loss 0.64 , qa loss 0.64 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:34:18,482 - 0:43:17 - 98.7s - INFO - __main__ - progress 8.624 , lr 8.6E-06 , loss 0.605 , qa loss 0.605 , lm loss 0.000 , avg batch size 4.0
2023-07-28 23:35:13,300 - 0:44:12 - 54.8s - INFO - __main__ - epoch 9/10 done , tot steps 14436 , lr 6.3E-06 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:36:52,348 - 0:45:51 - 99.0s - INFO - __main__ - progress 9.624 , lr 2.4E-06 , loss 0.612 , qa loss 0.612 , lm loss 0.000 , avg batch size 4.0
2023-07-28 23:37:45,760 - 0:46:44 - 53.4s - INFO - __main__ - epoch 10/10 done , tot steps 16040 , lr 3.1E-08 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:37:46,995 - 0:46:46 - 1.2s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-07-28 23:37:46,996 - 0:46:46 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-28 23:37:47,144 - 0:46:46 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-07-28 23:38:00,316 - 0:46:59 - 13.2s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 25360
2023-07-28 23:38:57,208 - 0:47:56 - 56.9s - INFO - __main__ - epoch 1/10 done , tot steps 634 , lr 5.6E-05 , loss 2.28 , qa loss 2.28 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:39:52,631 - 0:48:51 - 55.4s - INFO - __main__ - epoch 2/10 done , tot steps 1268 , lr 5.0E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:40:48,334 - 0:49:47 - 55.7s - INFO - __main__ - epoch 3/10 done , tot steps 1902 , lr 4.4E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:41:44,204 - 0:50:43 - 55.9s - INFO - __main__ - epoch 4/10 done , tot steps 2536 , lr 3.8E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:42:40,686 - 0:51:39 - 56.5s - INFO - __main__ - epoch 5/10 done , tot steps 3170 , lr 3.1E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:43:35,464 - 0:52:34 - 54.8s - INFO - __main__ - epoch 6/10 done , tot steps 3804 , lr 2.5E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:44:30,796 - 0:53:29 - 55.3s - INFO - __main__ - epoch 7/10 done , tot steps 4438 , lr 1.9E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:45:25,700 - 0:54:24 - 54.9s - INFO - __main__ - epoch 8/10 done , tot steps 5072 , lr 1.3E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:46:19,612 - 0:55:18 - 53.9s - INFO - __main__ - epoch 9/10 done , tot steps 5706 , lr 6.3E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-07-28 23:47:15,456 - 0:56:14 - 55.8s - INFO - __main__ - epoch 10/10 done , tot steps 6340 , lr 3.0E-08 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 00:56:10
CPU Execution time: 00:49:04
