................................................................................................................................
 Training Adapter + Prefix at 4 starting layers 
................................................................................................................................
Available number of GPU = 7 < n_gpus = 12
Continue training with 7 GPUs
2023-08-03 21:06:38,167 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[0, 2, 3, 8, 9, 13, 15], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[24903.68, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=7, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-03 21:06:38,168 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-03 21:06:38,168 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-03 21:06:40,745 - 0:00:07 - 2.6s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
[0]
2023-08-03 21:06:53,827 - 0:00:20 - 13.1s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-03 21:08:03,236 - 0:01:29 - 69.4s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.034 , qa loss 2.034 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:08:43,011 - 0:02:09 - 39.8s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.32 , qa loss 1.32 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:09:51,871 - 0:03:18 - 68.9s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.304 , qa loss 0.304 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:10:30,973 - 0:03:57 - 39.1s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:11:39,314 - 0:05:05 - 68.3s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.255 , qa loss 0.255 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:12:17,878 - 0:05:44 - 38.6s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:13:25,224 - 0:06:51 - 67.3s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.238 , qa loss 0.238 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:14:03,542 - 0:07:29 - 38.3s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:15:12,856 - 0:08:39 - 69.3s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.213 , qa loss 0.213 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:15:52,629 - 0:09:18 - 39.8s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:17:02,634 - 0:10:28 - 70.0s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.185 , qa loss 0.185 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:17:41,937 - 0:11:08 - 39.3s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:18:51,034 - 0:12:17 - 69.1s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.172 , qa loss 0.172 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:19:30,256 - 0:12:56 - 39.2s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:20:39,259 - 0:14:05 - 69.0s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.163 , qa loss 0.163 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:21:18,869 - 0:14:45 - 39.6s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:22:28,000 - 0:15:54 - 69.1s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.161 , qa loss 0.161 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:23:07,440 - 0:16:33 - 39.4s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:24:16,537 - 0:17:42 - 69.1s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.149 , qa loss 0.149 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:24:56,430 - 0:18:22 - 39.9s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:26:05,463 - 0:19:31 - 69.0s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.141 , qa loss 0.141 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:26:44,357 - 0:20:10 - 38.9s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:27:53,492 - 0:21:19 - 69.1s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.136 , qa loss 0.136 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:28:32,349 - 0:21:58 - 38.9s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:29:40,408 - 0:23:06 - 68.1s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.141 , qa loss 0.141 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:30:19,716 - 0:23:46 - 39.3s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:31:28,716 - 0:24:55 - 69.0s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.132 , qa loss 0.132 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:32:07,379 - 0:25:33 - 38.7s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:33:15,477 - 0:26:41 - 68.1s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.113 , qa loss 0.113 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:33:55,341 - 0:27:21 - 39.9s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:35:04,282 - 0:28:30 - 68.9s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.114 , qa loss 0.114 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:35:43,973 - 0:29:10 - 39.7s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:36:52,942 - 0:30:19 - 69.0s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.096 , qa loss 0.096 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:37:32,839 - 0:30:59 - 39.9s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:38:42,958 - 0:32:09 - 70.1s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.102 , qa loss 0.102 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:39:22,729 - 0:32:49 - 39.8s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:40:32,051 - 0:33:58 - 69.3s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.105 , qa loss 0.105 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:41:10,971 - 0:34:37 - 38.9s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:42:19,248 - 0:35:45 - 68.3s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.099 , qa loss 0.099 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:42:59,898 - 0:36:26 - 40.7s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:43:01,161 - 0:36:27 - 1.3s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-03 21:43:01,161 - 0:36:27 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-03 21:43:01,283 - 0:36:27 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-08-03 21:43:11,584 - 0:36:37 - 10.3s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-03 21:44:49,955 - 0:38:16 - 98.4s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 4.052 , qa loss 4.052 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:45:39,728 - 0:39:06 - 49.8s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 3.02 , qa loss 3.02 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:47:18,964 - 0:40:45 - 99.2s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 1.113 , qa loss 1.113 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:48:08,813 - 0:41:35 - 49.8s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 1.04 , qa loss 1.04 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:49:46,671 - 0:43:13 - 97.9s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.916 , qa loss 0.916 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:50:36,800 - 0:44:03 - 50.1s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.89 , qa loss 0.89 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:52:14,948 - 0:45:41 - 98.1s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.810 , qa loss 0.810 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:53:05,522 - 0:46:31 - 50.6s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.81 , qa loss 0.81 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:54:42,684 - 0:48:09 - 97.2s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.727 , qa loss 0.727 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:55:34,978 - 0:49:01 - 52.3s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.72 , qa loss 0.72 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:57:14,576 - 0:50:40 - 99.6s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.707 , qa loss 0.707 , lm loss 0.000 , avg batch size 4.0
2023-08-03 21:58:04,970 - 0:51:31 - 50.4s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.70 , qa loss 0.70 , lm loss 0.00 , avg batch size 4.0
2023-08-03 21:59:43,757 - 0:53:10 - 98.8s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.662 , qa loss 0.662 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:00:34,067 - 0:54:00 - 50.3s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:02:12,013 - 0:55:38 - 97.9s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.650 , qa loss 0.650 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:03:03,189 - 0:56:29 - 51.2s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:04:42,345 - 0:58:08 - 99.2s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.603 , qa loss 0.603 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:05:32,642 - 0:58:59 - 50.3s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:07:10,670 - 1:00:37 - 98.0s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.585 , qa loss 0.585 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:08:01,200 - 1:01:27 - 50.5s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:09:38,397 - 1:03:04 - 97.2s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.573 , qa loss 0.573 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:10:29,262 - 1:03:55 - 50.9s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.57 , qa loss 0.57 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:12:07,943 - 1:05:34 - 98.7s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.555 , qa loss 0.555 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:13:00,688 - 1:06:27 - 52.7s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.55 , qa loss 0.55 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:14:40,674 - 1:08:07 - 100.0s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.553 , qa loss 0.553 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:15:33,088 - 1:08:59 - 52.4s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:17:15,436 - 1:10:41 - 102.3s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.539 , qa loss 0.539 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:18:06,416 - 1:11:32 - 51.0s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:19:47,210 - 1:13:13 - 100.8s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.511 , qa loss 0.511 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:20:38,409 - 1:14:04 - 51.2s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:22:35,283 - 1:16:01 - 116.9s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.522 , qa loss 0.522 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:23:27,754 - 1:16:54 - 52.5s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:25:07,617 - 1:18:33 - 99.9s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.503 , qa loss 0.503 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:25:58,193 - 1:19:24 - 50.6s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:27:37,471 - 1:21:03 - 99.3s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.496 , qa loss 0.496 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:28:27,288 - 1:21:53 - 49.8s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:30:05,973 - 1:23:32 - 98.7s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.487 , qa loss 0.487 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:30:58,257 - 1:24:24 - 52.3s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:32:43,507 - 1:26:09 - 105.3s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.493 , qa loss 0.493 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:33:42,837 - 1:27:09 - 59.3s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:33:44,155 - 1:27:10 - 1.3s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-03 22:33:44,156 - 1:27:10 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-03 22:33:44,284 - 1:27:10 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-08-03 22:33:54,657 - 1:27:21 - 10.4s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-03 22:34:48,562 - 1:28:14 - 53.9s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 4.19 , qa loss 4.19 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:35:47,665 - 1:29:14 - 59.1s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.90 , qa loss 0.90 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:36:42,783 - 1:30:09 - 55.1s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:37:53,798 - 1:31:20 - 71.0s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:38:47,688 - 1:32:14 - 53.9s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:39:58,787 - 1:33:25 - 71.1s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:40:51,980 - 1:34:18 - 53.2s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:41:46,571 - 1:35:12 - 54.6s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:42:55,847 - 1:36:22 - 69.3s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:43:47,680 - 1:37:14 - 51.8s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:44:44,916 - 1:38:11 - 57.2s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:45:40,515 - 1:39:06 - 55.6s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:46:36,792 - 1:40:03 - 56.3s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:47:31,698 - 1:40:58 - 54.9s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:48:37,457 - 1:42:03 - 65.8s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:49:32,856 - 1:42:59 - 55.4s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:50:43,925 - 1:44:10 - 71.1s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:51:39,901 - 1:45:06 - 56.0s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:52:49,209 - 1:46:15 - 69.3s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:53:46,517 - 1:47:12 - 57.3s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:47:09
CPU Execution time: 01:32:51
................................................................................................................................
 Testing Adapter + Prefix at 4 starting layers 
................................................................................................................................
2023-08-03 22:53:54,539 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[0], dynamic_epochs=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-03 22:54:05,752 - 0:00:16 - 11.2s - INFO - __main__ - task: sst, epoch: 20
2023-08-03 22:54:05,753 - 0:00:16 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-03 22:54:10,223 - 0:00:20 - 4.5s - INFO - __main__ - len of test dataset: 1821
2023-08-03 22:54:21,055 - 0:00:31 - 10.8s - INFO - __main__ - score: {'sst': OrderedDict([('em', 89.23668314113125), ('nf1', 89.23668314113125), ('nem', 89.23668314113125)]), 'srl': None, 'woz.en': None}
2023-08-03 22:54:32,738 - 0:00:43 - 11.7s - INFO - __main__ - task: srl, epoch: 20
2023-08-03 22:54:32,738 - 0:00:43 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-03 22:54:37,662 - 0:00:48 - 4.9s - INFO - __main__ - len of test dataset: 2201
2023-08-03 23:20:54,877 - 0:27:05 - 1577.2s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 47.88732394366197), ('nf1', 66.35298066491177), ('nem', 52.476147205815536)]), 'woz.en': None}
2023-08-03 23:21:06,492 - 0:27:16 - 11.6s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-03 23:21:06,492 - 0:27:16 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-03 23:21:11,425 - 0:27:21 - 4.9s - INFO - __main__ - len of test dataset: 1646
2023-08-03 23:34:30,504 - 0:40:40 - 799.1s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 15.552855407047387), ('nf1', 92.13222749978523), ('nem', 82.98906439854193), ('joint_goal_em', 76.48845686512759), ('turn_request_em', 91.00850546780073), ('turn_goal_em', 88.09234507897933), ('avg_dialogue', 83.74848116646416)])}
................................................................................................................................
 Training Adapter + Prefix at 6 starting layers 
................................................................................................................................
Available number of GPU = 6 < n_gpus = 12
Continue training with 6 GPUs
2023-08-03 23:34:37,167 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[0, 3, 8, 9, 13, 15], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[26214.4, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=6, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[9175, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[9175, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-03 23:34:37,167 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-03 23:34:37,167 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-03 23:34:40,199 - 0:00:08 - 3.0s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
[0]
2023-08-03 23:34:53,303 - 0:00:21 - 13.1s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-03 23:36:08,718 - 0:01:36 - 75.4s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.101 , qa loss 2.101 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:36:50,010 - 0:02:17 - 41.3s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.35 , qa loss 1.35 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:38:00,380 - 0:03:28 - 70.4s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.309 , qa loss 0.309 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:38:43,114 - 0:04:10 - 42.7s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:39:54,856 - 0:05:22 - 71.7s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.245 , qa loss 0.245 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:40:35,675 - 0:06:03 - 40.8s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:41:49,212 - 0:07:17 - 73.5s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.225 , qa loss 0.225 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:42:29,917 - 0:07:57 - 40.7s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:43:41,251 - 0:09:09 - 71.3s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.211 , qa loss 0.211 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:44:22,782 - 0:09:50 - 41.5s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:45:33,419 - 0:11:01 - 70.6s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.192 , qa loss 0.192 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:46:13,190 - 0:11:40 - 39.8s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:47:27,644 - 0:12:55 - 74.5s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.164 , qa loss 0.164 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:48:09,603 - 0:13:37 - 42.0s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:49:21,978 - 0:14:49 - 72.4s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.160 , qa loss 0.160 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:50:05,094 - 0:15:32 - 43.1s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:51:17,085 - 0:16:44 - 72.0s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.146 , qa loss 0.146 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:51:59,033 - 0:17:26 - 41.9s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:53:12,919 - 0:18:40 - 73.9s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.142 , qa loss 0.142 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:53:56,319 - 0:19:24 - 43.4s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:55:08,615 - 0:20:36 - 72.3s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.125 , qa loss 0.125 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:56:00,403 - 0:21:28 - 51.8s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:57:13,509 - 0:22:41 - 73.1s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.135 , qa loss 0.135 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:58:03,082 - 0:23:30 - 49.6s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:59:15,325 - 0:24:43 - 72.2s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.106 , qa loss 0.106 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:00:03,722 - 0:25:31 - 48.4s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:01:21,982 - 0:26:49 - 78.3s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.101 , qa loss 0.101 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:02:10,502 - 0:27:38 - 48.5s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:03:27,295 - 0:28:55 - 76.8s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.099 , qa loss 0.099 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:04:16,077 - 0:29:43 - 48.8s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:05:31,774 - 0:30:59 - 75.7s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.100 , qa loss 0.100 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:06:22,384 - 0:31:50 - 50.6s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:07:40,368 - 0:33:08 - 78.0s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.092 , qa loss 0.092 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:08:30,992 - 0:33:58 - 50.6s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:09:51,149 - 0:35:18 - 80.2s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.086 , qa loss 0.086 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:10:37,768 - 0:36:05 - 46.6s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:11:56,240 - 0:37:24 - 78.5s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.080 , qa loss 0.080 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:12:43,729 - 0:38:11 - 47.5s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:14:02,168 - 0:39:29 - 78.4s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.072 , qa loss 0.072 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:14:49,467 - 0:40:17 - 47.3s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:14:50,800 - 0:40:18 - 1.3s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-04 00:14:50,801 - 0:40:18 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-04 00:14:50,945 - 0:40:18 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-08-04 00:15:01,936 - 0:40:29 - 11.0s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-04 00:16:51,177 - 0:42:18 - 109.2s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 4.185 , qa loss 4.185 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:17:47,130 - 0:43:14 - 56.0s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 3.05 , qa loss 3.05 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:19:36,095 - 0:45:03 - 109.0s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.981 , qa loss 0.981 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:20:32,055 - 0:45:59 - 56.0s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.94 , qa loss 0.94 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:22:19,684 - 0:47:47 - 107.6s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.808 , qa loss 0.808 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:23:16,195 - 0:48:44 - 56.5s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.79 , qa loss 0.79 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:25:02,360 - 0:50:30 - 106.2s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.709 , qa loss 0.709 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:25:59,440 - 0:51:27 - 57.1s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.70 , qa loss 0.70 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:27:47,696 - 0:53:15 - 108.3s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.637 , qa loss 0.637 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:28:45,321 - 0:54:13 - 57.6s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:30:32,774 - 0:56:00 - 107.5s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.619 , qa loss 0.619 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:31:30,846 - 0:56:58 - 58.1s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:33:18,086 - 0:58:45 - 107.2s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.585 , qa loss 0.585 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:34:14,640 - 0:59:42 - 56.6s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:36:01,507 - 1:01:29 - 106.9s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.541 , qa loss 0.541 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:36:58,876 - 1:02:26 - 57.4s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:38:46,089 - 1:04:13 - 107.2s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.526 , qa loss 0.526 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:39:42,735 - 1:05:10 - 56.6s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:41:26,171 - 1:06:53 - 103.4s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.501 , qa loss 0.501 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:42:24,247 - 1:07:52 - 58.1s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:44:10,681 - 1:09:38 - 106.4s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.485 , qa loss 0.485 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:45:09,410 - 1:10:37 - 58.7s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:46:58,837 - 1:12:26 - 109.4s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.471 , qa loss 0.471 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:47:55,902 - 1:13:23 - 57.1s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:49:43,247 - 1:15:11 - 107.3s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.454 , qa loss 0.454 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:50:40,652 - 1:16:08 - 57.4s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:52:55,695 - 1:18:23 - 135.0s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.436 , qa loss 0.436 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:54:07,204 - 1:19:35 - 71.5s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:56:28,105 - 1:21:55 - 140.9s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.420 , qa loss 0.420 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:57:43,613 - 1:23:11 - 75.5s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:00:00,944 - 1:25:28 - 137.3s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.410 , qa loss 0.410 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:01:20,550 - 1:26:48 - 79.6s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:03:37,553 - 1:29:05 - 137.0s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.418 , qa loss 0.418 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:04:49,784 - 1:30:17 - 72.2s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:07:07,534 - 1:32:35 - 137.8s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.399 , qa loss 0.399 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:08:26,612 - 1:33:54 - 79.1s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:10:43,627 - 1:36:11 - 137.0s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.395 , qa loss 0.395 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:12:04,253 - 1:37:32 - 80.6s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:14:20,148 - 1:39:47 - 135.9s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.377 , qa loss 0.377 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:15:35,137 - 1:41:02 - 75.0s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:15:36,550 - 1:41:04 - 1.4s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-04 01:15:36,551 - 1:41:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-04 01:15:36,711 - 1:41:04 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-08-04 01:15:47,216 - 1:41:15 - 10.5s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-04 01:16:57,966 - 1:42:25 - 70.7s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 4.23 , qa loss 4.23 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:18:03,234 - 1:43:31 - 65.3s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 1.40 , qa loss 1.40 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:19:12,952 - 1:44:40 - 69.7s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.80 , qa loss 0.80 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:20:17,510 - 1:45:45 - 64.6s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.62 , qa loss 0.62 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:21:26,011 - 1:46:53 - 68.5s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:22:32,032 - 1:47:59 - 66.0s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:23:41,516 - 1:49:09 - 69.5s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:24:48,788 - 1:50:16 - 67.3s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:25:57,766 - 1:51:25 - 69.0s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:27:05,014 - 1:52:32 - 67.2s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:28:14,167 - 1:53:41 - 69.2s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:29:20,570 - 1:54:48 - 66.4s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:30:30,438 - 1:55:58 - 69.9s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:31:35,018 - 1:57:02 - 64.6s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:32:41,959 - 1:58:09 - 66.9s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:33:45,094 - 1:59:12 - 63.1s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:34:53,042 - 2:00:20 - 67.9s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:35:55,686 - 2:01:23 - 62.6s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:37:02,287 - 2:02:30 - 66.6s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:38:04,725 - 2:03:32 - 62.4s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 02:03:29
CPU Execution time: 01:50:29
................................................................................................................................
 Testing Adapter + Prefix at 6 starting layers 
................................................................................................................................
2023-08-04 01:38:12,814 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[2], dynamic_epochs=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-04 01:38:27,040 - 0:00:19 - 14.2s - INFO - __main__ - task: sst, epoch: 20
2023-08-04 01:38:27,041 - 0:00:19 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-04 01:38:32,006 - 0:00:24 - 5.0s - INFO - __main__ - len of test dataset: 1821
2023-08-04 01:38:43,065 - 0:00:35 - 11.1s - INFO - __main__ - score: {'sst': OrderedDict([('em', 89.45634266886327), ('nf1', 89.45634266886327), ('nem', 89.45634266886327)]), 'srl': None, 'woz.en': None}
2023-08-04 01:38:54,705 - 0:00:46 - 11.6s - INFO - __main__ - task: srl, epoch: 20
2023-08-04 01:38:54,705 - 0:00:46 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-04 01:38:59,910 - 0:00:52 - 5.2s - INFO - __main__ - len of test dataset: 2201
2023-08-04 02:06:22,297 - 0:28:14 - 1642.4s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 49.341208541572016), ('nf1', 67.79685739121038), ('nem', 54.29350295320309)]), 'woz.en': None}
2023-08-04 02:06:33,834 - 0:28:25 - 11.5s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-04 02:06:33,835 - 0:28:25 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-04 02:06:39,508 - 0:28:31 - 5.7s - INFO - __main__ - len of test dataset: 1646
2023-08-04 02:19:40,365 - 0:41:32 - 780.9s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 15.370595382746052), ('nf1', 92.26033248511986), ('nem', 82.38153098420413), ('joint_goal_em', 76.60996354799514), ('turn_request_em', 90.34021871202917), ('turn_goal_em', 88.03159173754557), ('avg_dialogue', 83.47509113001215)])}
................................................................................................................................
 Training Adapter + Prefix at 8 starting layers 
................................................................................................................................
Available number of GPU = 6 < n_gpus = 12
Continue training with 6 GPUs
2023-08-04 02:19:47,455 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[2, 3, 7, 8, 13, 15], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[26214.4, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=6, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[9175, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[9175, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-04 02:19:47,455 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-04 02:19:47,455 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-04 02:19:50,234 - 0:00:08 - 2.8s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
[0]
2023-08-04 02:20:02,937 - 0:00:20 - 12.7s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-04 02:21:18,254 - 0:01:36 - 75.3s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.794 , qa loss 1.794 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:22:02,341 - 0:02:20 - 44.1s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.17 , qa loss 1.17 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:23:16,998 - 0:03:34 - 74.7s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.300 , qa loss 0.300 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:24:01,799 - 0:04:19 - 44.8s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:25:16,635 - 0:05:34 - 74.8s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.245 , qa loss 0.245 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:26:00,695 - 0:06:18 - 44.1s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:27:30,844 - 0:07:48 - 90.1s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.228 , qa loss 0.228 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:28:14,046 - 0:08:31 - 43.2s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:29:28,363 - 0:09:46 - 74.3s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.195 , qa loss 0.195 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:30:11,250 - 0:10:29 - 42.9s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:31:24,750 - 0:11:42 - 73.5s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.190 , qa loss 0.190 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:32:10,182 - 0:12:28 - 45.4s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:33:27,471 - 0:13:45 - 77.3s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.169 , qa loss 0.169 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:34:12,570 - 0:14:30 - 45.1s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:35:29,844 - 0:15:47 - 77.3s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.153 , qa loss 0.153 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:36:13,605 - 0:16:31 - 43.8s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:37:30,515 - 0:17:48 - 76.9s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.144 , qa loss 0.144 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:38:13,495 - 0:18:31 - 43.0s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:39:31,008 - 0:19:48 - 77.5s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.126 , qa loss 0.126 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:40:14,478 - 0:20:32 - 43.5s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:41:29,362 - 0:21:47 - 74.9s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.113 , qa loss 0.113 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:42:13,133 - 0:22:31 - 43.8s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:43:30,483 - 0:23:48 - 77.3s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.117 , qa loss 0.117 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:44:14,535 - 0:24:32 - 44.1s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:45:31,515 - 0:25:49 - 77.0s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.114 , qa loss 0.114 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:46:15,081 - 0:26:32 - 43.6s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:47:32,930 - 0:27:50 - 77.8s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.104 , qa loss 0.104 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:48:17,321 - 0:28:35 - 44.4s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:49:33,996 - 0:29:51 - 76.7s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.092 , qa loss 0.092 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:50:17,254 - 0:30:35 - 43.3s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:51:33,122 - 0:31:51 - 75.9s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.087 , qa loss 0.087 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:52:16,292 - 0:32:34 - 43.2s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:53:30,690 - 0:33:48 - 74.4s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.082 , qa loss 0.082 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:54:13,395 - 0:34:31 - 42.7s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:55:27,075 - 0:35:44 - 73.7s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.080 , qa loss 0.080 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:56:08,687 - 0:36:26 - 41.6s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:57:21,769 - 0:37:39 - 73.1s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.075 , qa loss 0.075 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:58:05,438 - 0:38:23 - 43.7s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:59:20,661 - 0:39:38 - 75.2s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.078 , qa loss 0.078 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:00:05,870 - 0:40:23 - 45.2s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:00:07,224 - 0:40:25 - 1.4s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-04 03:00:07,225 - 0:40:25 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-04 03:00:07,352 - 0:40:25 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-08-04 03:00:17,719 - 0:40:35 - 10.4s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-04 03:02:04,888 - 0:42:22 - 107.2s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 3.642 , qa loss 3.642 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:02:59,889 - 0:43:17 - 55.0s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.69 , qa loss 2.69 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:04:45,571 - 0:45:03 - 105.7s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.887 , qa loss 0.887 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:05:39,354 - 0:45:57 - 53.8s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.86 , qa loss 0.86 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:07:25,161 - 0:47:43 - 105.8s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.751 , qa loss 0.751 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:08:20,317 - 0:48:38 - 55.2s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.73 , qa loss 0.73 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:10:05,557 - 0:50:23 - 105.2s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.663 , qa loss 0.663 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:10:59,039 - 0:51:16 - 53.5s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.64 , qa loss 0.64 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:12:42,342 - 0:53:00 - 103.3s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.595 , qa loss 0.595 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:13:36,464 - 0:53:54 - 54.1s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:15:18,613 - 0:55:36 - 102.1s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.555 , qa loss 0.555 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:16:13,518 - 0:56:31 - 54.9s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.55 , qa loss 0.55 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:17:57,973 - 0:58:15 - 104.5s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.511 , qa loss 0.511 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:19:01,951 - 0:59:19 - 64.0s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:20:47,835 - 1:01:05 - 105.9s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.484 , qa loss 0.484 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:21:43,251 - 1:02:01 - 55.4s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:23:29,200 - 1:03:47 - 105.9s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.454 , qa loss 0.454 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:24:24,847 - 1:04:42 - 55.6s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:26:08,297 - 1:06:26 - 103.5s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.433 , qa loss 0.433 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:27:03,484 - 1:07:21 - 55.2s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:28:46,208 - 1:09:04 - 102.7s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.426 , qa loss 0.426 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:29:42,447 - 1:10:00 - 56.2s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:31:28,535 - 1:11:46 - 106.1s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.395 , qa loss 0.395 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:32:27,369 - 1:12:45 - 58.8s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:34:12,544 - 1:14:30 - 105.2s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.377 , qa loss 0.377 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:35:06,498 - 1:15:24 - 54.0s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:36:49,366 - 1:17:07 - 102.9s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.373 , qa loss 0.373 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:37:46,256 - 1:18:04 - 56.9s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:39:46,726 - 1:20:04 - 120.5s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.359 , qa loss 0.359 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:40:42,185 - 1:21:00 - 55.5s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:42:28,355 - 1:22:46 - 106.2s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.345 , qa loss 0.345 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:43:21,642 - 1:23:39 - 53.3s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:45:05,479 - 1:25:23 - 103.8s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.338 , qa loss 0.338 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:45:59,465 - 1:26:17 - 54.0s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:47:43,635 - 1:28:01 - 104.2s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.331 , qa loss 0.331 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:48:37,490 - 1:28:55 - 53.9s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:50:22,279 - 1:30:40 - 104.8s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.327 , qa loss 0.327 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:51:22,554 - 1:31:40 - 60.3s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:53:06,014 - 1:33:23 - 103.5s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.324 , qa loss 0.324 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:54:00,175 - 1:34:18 - 54.2s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:54:01,609 - 1:34:19 - 1.4s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-04 03:54:01,609 - 1:34:19 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-04 03:54:01,747 - 1:34:19 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-08-04 03:54:10,878 - 1:34:28 - 9.1s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-04 03:55:10,656 - 1:35:28 - 59.8s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 3.42 , qa loss 3.42 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:56:10,032 - 1:36:27 - 59.4s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.71 , qa loss 0.71 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:57:09,803 - 1:37:27 - 59.8s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:58:08,934 - 1:38:26 - 59.1s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:59:11,176 - 1:39:29 - 62.2s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:00:08,982 - 1:40:26 - 57.8s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:01:09,881 - 1:41:27 - 60.9s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:02:08,140 - 1:42:26 - 58.3s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:03:06,265 - 1:43:24 - 58.1s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:04:04,428 - 1:44:22 - 58.2s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:05:02,800 - 1:45:20 - 58.4s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:05:59,501 - 1:46:17 - 56.7s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:06:57,438 - 1:47:15 - 57.9s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:07:53,666 - 1:48:11 - 56.2s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:08:55,342 - 1:49:13 - 61.7s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:09:54,796 - 1:50:12 - 59.5s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:10:54,246 - 1:51:12 - 59.5s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:11:50,580 - 1:52:08 - 56.3s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:12:46,979 - 1:53:04 - 56.4s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:13:48,046 - 1:54:05 - 61.1s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:54:02
CPU Execution time: 01:40:20
................................................................................................................................
 Training Adapter + Prefix at 8 starting layers 
................................................................................................................................
2023-08-04 04:13:56,348 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[2], dynamic_epochs=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-04 04:14:10,164 - 0:00:18 - 13.8s - INFO - __main__ - task: sst, epoch: 20
2023-08-04 04:14:10,165 - 0:00:18 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-04 04:14:14,598 - 0:00:23 - 4.4s - INFO - __main__ - len of test dataset: 1821
2023-08-04 04:14:24,978 - 0:00:33 - 10.4s - INFO - __main__ - score: {'sst': OrderedDict([('em', 88.6326194398682), ('nf1', 88.6326194398682), ('nem', 88.6326194398682)]), 'srl': None, 'woz.en': None}
2023-08-04 04:14:36,682 - 0:00:45 - 11.7s - INFO - __main__ - task: srl, epoch: 20
2023-08-04 04:14:36,683 - 0:00:45 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-04 04:14:41,386 - 0:00:50 - 4.7s - INFO - __main__ - len of test dataset: 2201
2023-08-04 04:42:10,053 - 0:28:18 - 1648.7s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 49.568378009995456), ('nf1', 68.20188497850003), ('nem', 54.384370740572464)]), 'woz.en': None}
2023-08-04 04:42:21,600 - 0:28:30 - 11.5s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-04 04:42:21,600 - 0:28:30 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-04 04:42:26,254 - 0:28:35 - 4.7s - INFO - __main__ - len of test dataset: 1646
2023-08-04 04:55:42,840 - 0:41:51 - 796.6s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 16.52490886998785), ('nf1', 92.97655159745078), ('nem', 84.87241798298906), ('joint_goal_em', 80.4374240583232), ('turn_request_em', 91.49453219927096), ('turn_goal_em', 89.73268529769138), ('avg_dialogue', 85.96597812879708)])}
................................................................................................................................
 The End Man! 
................................................................................................................................
