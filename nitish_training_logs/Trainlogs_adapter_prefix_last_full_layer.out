Available number of GPU = 5 < n_gpus = 12
Continue training with 5 GPUs
2023-07-29 09:27:29,028 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[4, 7, 13, 14, 15], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[27525.12, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=5, n_train_epochs={'sst': 10, 'srl': 10, 'woz.en': 10}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[9633, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[9633, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-07-29 09:27:29,028 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-07-29 09:27:29,028 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-29 09:27:31,706 - 0:00:07 - 2.7s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
[0]
2023-07-29 09:27:44,394 - 0:00:20 - 12.7s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 69200
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-07-29 09:29:15,959 - 0:01:52 - 91.6s - INFO - __main__ - progress 0.578 , lr 5.9E-05 , loss 1.339 , qa loss 1.339 , lm loss 0.000 , avg batch size 4.0
2023-07-29 09:30:21,630 - 0:02:57 - 65.7s - INFO - __main__ - epoch 1/10 done , tot steps 1730 , lr 5.6E-05 , loss 0.91 , qa loss 0.91 , lm loss 0.00 , avg batch size 4.0
2023-07-29 09:31:58,025 - 0:04:34 - 96.4s - INFO - __main__ - progress 1.578 , lr 5.3E-05 , loss 0.278 , qa loss 0.278 , lm loss 0.000 , avg batch size 4.0
2023-07-29 09:33:03,184 - 0:05:39 - 65.2s - INFO - __main__ - epoch 2/10 done , tot steps 3460 , lr 5.0E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-07-29 09:34:41,373 - 0:07:17 - 98.2s - INFO - __main__ - progress 2.578 , lr 4.6E-05 , loss 0.242 , qa loss 0.242 , lm loss 0.000 , avg batch size 4.0
2023-07-29 09:35:46,156 - 0:08:22 - 64.8s - INFO - __main__ - epoch 3/10 done , tot steps 5190 , lr 4.4E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-07-29 09:37:23,678 - 0:09:59 - 97.5s - INFO - __main__ - progress 3.578 , lr 4.0E-05 , loss 0.220 , qa loss 0.220 , lm loss 0.000 , avg batch size 4.0
2023-07-29 09:38:28,006 - 0:11:04 - 64.3s - INFO - __main__ - epoch 4/10 done , tot steps 6920 , lr 3.8E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-07-29 09:40:06,009 - 0:12:42 - 98.0s - INFO - __main__ - progress 4.578 , lr 3.4E-05 , loss 0.205 , qa loss 0.205 , lm loss 0.000 , avg batch size 4.0
2023-07-29 09:41:10,675 - 0:13:46 - 64.7s - INFO - __main__ - epoch 5/10 done , tot steps 8650 , lr 3.1E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-07-29 09:42:48,516 - 0:15:24 - 97.8s - INFO - __main__ - progress 5.578 , lr 2.8E-05 , loss 0.190 , qa loss 0.190 , lm loss 0.000 , avg batch size 4.0
2023-07-29 09:43:52,386 - 0:16:28 - 63.9s - INFO - __main__ - epoch 6/10 done , tot steps 10380 , lr 2.5E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-07-29 09:45:32,353 - 0:18:08 - 100.0s - INFO - __main__ - progress 6.578 , lr 2.1E-05 , loss 0.164 , qa loss 0.164 , lm loss 0.000 , avg batch size 4.0
2023-07-29 09:46:39,955 - 0:19:16 - 67.6s - INFO - __main__ - epoch 7/10 done , tot steps 12110 , lr 1.9E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-29 09:48:18,049 - 0:20:54 - 98.1s - INFO - __main__ - progress 7.578 , lr 1.5E-05 , loss 0.161 , qa loss 0.161 , lm loss 0.000 , avg batch size 4.0
2023-07-29 09:49:25,601 - 0:22:01 - 67.6s - INFO - __main__ - epoch 8/10 done , tot steps 13840 , lr 1.3E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-29 09:51:04,444 - 0:23:40 - 98.8s - INFO - __main__ - progress 8.578 , lr 8.9E-06 , loss 0.143 , qa loss 0.143 , lm loss 0.000 , avg batch size 4.0
2023-07-29 09:52:11,130 - 0:24:47 - 66.7s - INFO - __main__ - epoch 9/10 done , tot steps 15570 , lr 6.3E-06 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-07-29 09:53:50,470 - 0:26:26 - 99.3s - INFO - __main__ - progress 9.578 , lr 2.7E-06 , loss 0.133 , qa loss 0.133 , lm loss 0.000 , avg batch size 4.0
2023-07-29 09:54:59,498 - 0:27:35 - 69.0s - INFO - __main__ - epoch 10/10 done , tot steps 17300 , lr 3.1E-08 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-07-29 09:55:00,943 - 0:27:37 - 1.4s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-07-29 09:55:00,944 - 0:27:37 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-29 09:55:01,085 - 0:27:37 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-07-29 09:55:12,843 - 0:27:48 - 11.8s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 64140
2023-07-29 09:58:19,071 - 0:30:55 - 186.2s - INFO - __main__ - progress 0.624 , lr 5.9E-05 , loss 3.047 , qa loss 3.047 , lm loss 0.000 , avg batch size 4.0
2023-07-29 09:59:53,933 - 0:32:30 - 94.9s - INFO - __main__ - epoch 1/10 done , tot steps 1604 , lr 5.6E-05 , loss 2.27 , qa loss 2.27 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:02:38,975 - 0:35:15 - 165.0s - INFO - __main__ - progress 1.624 , lr 5.2E-05 , loss 0.819 , qa loss 0.819 , lm loss 0.000 , avg batch size 4.0
2023-07-29 10:04:01,077 - 0:36:37 - 82.1s - INFO - __main__ - epoch 2/10 done , tot steps 3208 , lr 5.0E-05 , loss 0.80 , qa loss 0.80 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:06:44,720 - 0:39:20 - 163.6s - INFO - __main__ - progress 2.624 , lr 4.6E-05 , loss 0.695 , qa loss 0.695 , lm loss 0.000 , avg batch size 4.0
2023-07-29 10:08:02,325 - 0:40:38 - 77.6s - INFO - __main__ - epoch 3/10 done , tot steps 4812 , lr 4.4E-05 , loss 0.68 , qa loss 0.68 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:10:45,589 - 0:43:21 - 163.3s - INFO - __main__ - progress 3.624 , lr 4.0E-05 , loss 0.604 , qa loss 0.604 , lm loss 0.000 , avg batch size 4.0
2023-07-29 10:12:10,348 - 0:44:46 - 84.8s - INFO - __main__ - epoch 4/10 done , tot steps 6416 , lr 3.8E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:14:55,729 - 0:47:31 - 165.4s - INFO - __main__ - progress 4.624 , lr 3.4E-05 , loss 0.511 , qa loss 0.511 , lm loss 0.000 , avg batch size 4.0
2023-07-29 10:16:16,045 - 0:48:52 - 80.3s - INFO - __main__ - epoch 5/10 done , tot steps 8020 , lr 3.1E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:18:54,094 - 0:51:30 - 158.0s - INFO - __main__ - progress 5.624 , lr 2.7E-05 , loss 0.463 , qa loss 0.463 , lm loss 0.000 , avg batch size 4.0
2023-07-29 10:20:22,978 - 0:52:59 - 88.9s - INFO - __main__ - epoch 6/10 done , tot steps 9624 , lr 2.5E-05 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:22:56,846 - 0:55:32 - 153.9s - INFO - __main__ - progress 6.624 , lr 2.1E-05 , loss 0.415 , qa loss 0.415 , lm loss 0.000 , avg batch size 4.0
2023-07-29 10:24:27,364 - 0:57:03 - 90.5s - INFO - __main__ - epoch 7/10 done , tot steps 11228 , lr 1.9E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:27:00,688 - 0:59:36 - 153.3s - INFO - __main__ - progress 7.624 , lr 1.5E-05 , loss 0.401 , qa loss 0.401 , lm loss 0.000 , avg batch size 4.0
2023-07-29 10:28:33,023 - 1:01:09 - 92.3s - INFO - __main__ - epoch 8/10 done , tot steps 12832 , lr 1.3E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:31:02,606 - 1:03:38 - 149.6s - INFO - __main__ - progress 8.624 , lr 8.6E-06 , loss 0.364 , qa loss 0.364 , lm loss 0.000 , avg batch size 4.0
2023-07-29 10:32:38,564 - 1:05:14 - 96.0s - INFO - __main__ - epoch 9/10 done , tot steps 14436 , lr 6.3E-06 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:34:59,980 - 1:07:36 - 141.4s - INFO - __main__ - progress 9.624 , lr 2.4E-06 , loss 0.371 , qa loss 0.371 , lm loss 0.000 , avg batch size 4.0
2023-07-29 10:36:16,455 - 1:08:52 - 76.5s - INFO - __main__ - epoch 10/10 done , tot steps 16040 , lr 3.1E-08 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:36:18,042 - 1:08:54 - 1.6s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-07-29 10:36:18,043 - 1:08:54 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-29 10:36:18,176 - 1:08:54 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-07-29 10:36:30,889 - 1:09:06 - 12.7s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 25360
2023-07-29 10:37:44,989 - 1:10:21 - 74.1s - INFO - __main__ - epoch 1/10 done , tot steps 634 , lr 5.6E-05 , loss 2.78 , qa loss 2.78 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:38:55,291 - 1:11:31 - 70.3s - INFO - __main__ - epoch 2/10 done , tot steps 1268 , lr 5.0E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:40:07,245 - 1:12:43 - 72.0s - INFO - __main__ - epoch 3/10 done , tot steps 1902 , lr 4.4E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:41:18,910 - 1:13:55 - 71.7s - INFO - __main__ - epoch 4/10 done , tot steps 2536 , lr 3.8E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:42:27,330 - 1:15:03 - 68.4s - INFO - __main__ - epoch 5/10 done , tot steps 3170 , lr 3.1E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:43:43,112 - 1:16:19 - 75.8s - INFO - __main__ - epoch 6/10 done , tot steps 3804 , lr 2.5E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:44:50,056 - 1:17:26 - 66.9s - INFO - __main__ - epoch 7/10 done , tot steps 4438 , lr 1.9E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:46:06,942 - 1:18:43 - 76.9s - INFO - __main__ - epoch 8/10 done , tot steps 5072 , lr 1.3E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:47:08,678 - 1:19:44 - 61.7s - INFO - __main__ - epoch 9/10 done , tot steps 5706 , lr 6.3E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-07-29 10:48:15,104 - 1:20:51 - 66.4s - INFO - __main__ - epoch 10/10 done , tot steps 6340 , lr 3.0E-08 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:20:47
CPU Execution time: 01:13:50
