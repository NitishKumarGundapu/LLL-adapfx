................................................................................................................................
 Training Adapter + Prefix with flat at all layers
................................................................................................................................
Available number of GPU = 6 < n_gpus = 12
Continue training with 6 GPUs
2023-08-03 22:12:08,672 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[2, 3, 8, 9, 13, 15], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[26214.4, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=6, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[9175, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[9175, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-03 22:12:08,673 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-03 22:12:08,673 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-03 22:12:11,701 - 0:00:08 - 3.0s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
[0]
2023-08-03 22:12:24,067 - 0:00:20 - 12.4s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-03 22:13:44,628 - 0:01:40 - 80.6s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.943 , qa loss 1.943 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:14:30,071 - 0:02:26 - 45.4s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.28 , qa loss 1.28 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:15:47,118 - 0:03:43 - 77.0s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.349 , qa loss 0.349 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:16:32,621 - 0:04:28 - 45.5s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:17:49,093 - 0:05:45 - 76.5s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.303 , qa loss 0.303 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:18:34,425 - 0:06:30 - 45.3s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:19:51,597 - 0:07:47 - 77.2s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.274 , qa loss 0.274 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:20:38,315 - 0:08:34 - 46.7s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:22:11,259 - 0:10:07 - 92.9s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.271 , qa loss 0.271 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:22:55,401 - 0:10:51 - 44.1s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:24:17,449 - 0:12:13 - 82.0s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.252 , qa loss 0.252 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:25:00,845 - 0:12:57 - 43.4s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:26:18,299 - 0:14:14 - 77.5s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.243 , qa loss 0.243 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:27:02,486 - 0:14:58 - 44.2s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:28:16,595 - 0:16:12 - 74.1s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.230 , qa loss 0.230 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:29:01,079 - 0:16:57 - 44.5s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:30:14,954 - 0:18:11 - 73.9s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.222 , qa loss 0.222 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:31:18,282 - 0:19:14 - 63.3s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:32:35,783 - 0:20:32 - 77.5s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.215 , qa loss 0.215 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:33:21,175 - 0:21:17 - 45.4s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:34:41,889 - 0:22:38 - 80.7s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.215 , qa loss 0.215 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:35:27,975 - 0:23:24 - 46.1s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:36:47,826 - 0:24:44 - 79.9s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.215 , qa loss 0.215 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:37:33,323 - 0:25:29 - 45.5s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:38:52,874 - 0:26:49 - 79.6s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.209 , qa loss 0.209 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:39:38,483 - 0:27:34 - 45.6s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:40:58,249 - 0:28:54 - 79.8s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.203 , qa loss 0.203 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:41:46,444 - 0:29:42 - 48.2s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:43:20,981 - 0:31:17 - 94.5s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.195 , qa loss 0.195 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:44:07,536 - 0:32:03 - 46.6s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:45:25,899 - 0:33:22 - 78.4s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.190 , qa loss 0.190 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:46:11,842 - 0:34:08 - 45.9s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:47:31,371 - 0:35:27 - 79.5s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.198 , qa loss 0.198 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:48:16,603 - 0:36:12 - 45.2s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:49:37,781 - 0:37:34 - 81.2s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.191 , qa loss 0.191 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:50:23,642 - 0:38:19 - 45.9s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:51:42,973 - 0:39:39 - 79.3s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.195 , qa loss 0.195 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:52:28,887 - 0:40:25 - 45.9s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:53:47,940 - 0:41:44 - 79.1s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.193 , qa loss 0.193 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:54:35,784 - 0:42:32 - 47.8s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:54:36,779 - 0:42:33 - 1.0s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-03 22:54:36,779 - 0:42:33 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-03 22:54:36,915 - 0:42:33 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-08-03 22:54:49,612 - 0:42:45 - 12.7s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-03 22:56:38,395 - 0:44:34 - 108.8s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 7.166 , qa loss 7.166 , lm loss 0.000 , avg batch size 4.0
2023-08-03 22:57:33,399 - 0:45:29 - 55.0s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 6.11 , qa loss 6.11 , lm loss 0.00 , avg batch size 4.0
2023-08-03 22:59:19,484 - 0:47:15 - 106.1s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 2.959 , qa loss 2.959 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:00:15,848 - 0:48:12 - 56.4s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 2.64 , qa loss 2.64 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:02:00,716 - 0:49:57 - 104.9s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 1.660 , qa loss 1.660 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:02:56,352 - 0:50:52 - 55.6s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 1.57 , qa loss 1.57 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:04:41,384 - 0:52:37 - 105.0s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 1.247 , qa loss 1.247 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:05:36,975 - 0:53:33 - 55.6s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 1.20 , qa loss 1.20 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:07:24,821 - 0:55:21 - 107.8s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 1.064 , qa loss 1.064 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:08:18,745 - 0:56:15 - 53.9s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 1.04 , qa loss 1.04 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:10:05,363 - 0:58:01 - 106.6s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.956 , qa loss 0.956 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:11:02,275 - 0:58:58 - 56.9s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.93 , qa loss 0.93 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:12:51,376 - 1:00:47 - 109.1s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.852 , qa loss 0.852 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:13:46,284 - 1:01:42 - 54.9s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.85 , qa loss 0.85 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:15:34,671 - 1:03:30 - 108.4s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.788 , qa loss 0.788 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:16:29,752 - 1:04:26 - 55.1s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.80 , qa loss 0.80 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:18:17,753 - 1:06:14 - 108.0s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.770 , qa loss 0.770 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:19:13,555 - 1:07:09 - 55.8s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.76 , qa loss 0.76 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:21:01,237 - 1:08:57 - 107.7s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.718 , qa loss 0.718 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:21:59,066 - 1:09:55 - 57.8s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.72 , qa loss 0.72 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:23:44,168 - 1:11:40 - 105.1s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.681 , qa loss 0.681 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:24:40,186 - 1:12:36 - 56.0s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.69 , qa loss 0.69 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:26:23,860 - 1:14:20 - 103.7s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.665 , qa loss 0.665 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:27:20,297 - 1:15:16 - 56.4s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.66 , qa loss 0.66 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:29:07,863 - 1:17:04 - 107.6s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.628 , qa loss 0.628 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:30:02,153 - 1:17:58 - 54.3s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.64 , qa loss 0.64 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:31:48,284 - 1:19:44 - 106.1s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.622 , qa loss 0.622 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:32:45,721 - 1:20:42 - 57.4s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.62 , qa loss 0.62 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:34:32,587 - 1:22:28 - 106.9s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.611 , qa loss 0.611 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:35:28,678 - 1:23:24 - 56.1s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.60 , qa loss 0.60 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:37:14,945 - 1:25:11 - 106.3s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.576 , qa loss 0.576 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:38:11,515 - 1:26:07 - 56.6s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:39:58,005 - 1:27:54 - 106.5s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.567 , qa loss 0.567 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:40:55,916 - 1:28:52 - 57.9s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:42:41,369 - 1:30:37 - 105.5s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.555 , qa loss 0.555 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:43:39,337 - 1:31:35 - 58.0s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:45:27,754 - 1:33:24 - 108.4s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.532 , qa loss 0.532 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:46:33,180 - 1:34:29 - 65.4s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:48:23,103 - 1:36:19 - 109.9s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.533 , qa loss 0.533 , lm loss 0.000 , avg batch size 4.0
2023-08-03 23:49:20,363 - 1:37:16 - 57.3s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:49:21,401 - 1:37:17 - 1.0s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-03 23:49:21,401 - 1:37:17 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-03 23:49:21,531 - 1:37:17 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-08-03 23:49:30,643 - 1:37:26 - 9.1s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-03 23:50:34,843 - 1:38:31 - 64.2s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 5.74 , qa loss 5.74 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:51:35,338 - 1:39:31 - 60.5s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 1.31 , qa loss 1.31 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:52:37,077 - 1:40:33 - 61.7s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.85 , qa loss 0.85 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:53:37,466 - 1:41:33 - 60.4s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.66 , qa loss 0.66 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:54:40,152 - 1:42:36 - 62.7s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.57 , qa loss 0.57 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:55:41,821 - 1:43:38 - 61.7s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:56:44,585 - 1:44:40 - 62.8s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:57:44,785 - 1:45:41 - 60.2s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:58:43,393 - 1:46:39 - 58.6s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-08-03 23:59:45,449 - 1:47:41 - 62.1s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:00:50,260 - 1:48:46 - 64.8s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:01:53,491 - 1:49:49 - 63.2s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:02:57,509 - 1:50:53 - 64.0s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:03:59,564 - 1:51:55 - 62.1s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:05:02,131 - 1:52:58 - 62.6s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:06:05,527 - 1:54:01 - 63.4s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:07:09,503 - 1:55:05 - 64.0s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:08:13,014 - 1:56:09 - 63.5s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:09:20,228 - 1:57:16 - 67.2s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:10:23,673 - 1:58:19 - 63.4s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:58:16
CPU Execution time: 01:44:29
................................................................................................................................
 Testing Adapter + Prefix with flat at all layers 
................................................................................................................................
2023-08-04 00:10:31,932 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[2], dynamic_epochs=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-04 00:10:45,368 - 0:00:18 - 13.4s - INFO - __main__ - task: sst, epoch: 20
2023-08-04 00:10:45,368 - 0:00:18 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-04 00:10:57,193 - 0:00:30 - 11.8s - INFO - __main__ - len of test dataset: 1821
2023-08-04 00:11:07,720 - 0:00:40 - 10.5s - INFO - __main__ - score: {'sst': OrderedDict([('em', 83.30587589236683), ('nf1', 83.30587589236683), ('nem', 83.30587589236683)]), 'srl': None, 'woz.en': None}
2023-08-04 00:11:19,339 - 0:00:52 - 11.6s - INFO - __main__ - task: srl, epoch: 20
2023-08-04 00:11:19,340 - 0:00:52 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-04 00:11:24,922 - 0:00:58 - 5.6s - INFO - __main__ - len of test dataset: 2201
2023-08-04 00:37:27,248 - 0:27:00 - 1562.3s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 39.93639254884144), ('nf1', 59.86001027859646), ('nem', 44.570649704679695)]), 'woz.en': None}
2023-08-04 00:37:38,902 - 0:27:12 - 11.7s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-04 00:37:38,903 - 0:27:12 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-04 00:37:44,362 - 0:27:17 - 5.5s - INFO - __main__ - len of test dataset: 1646
2023-08-04 00:50:38,992 - 0:40:12 - 774.6s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 14.88456865127582), ('nf1', 90.35154598338079), ('nem', 79.82989064398542), ('joint_goal_em', 73.0255164034022), ('turn_request_em', 88.82138517618469), ('turn_goal_em', 86.45200486026732), ('avg_dialogue', 80.92345078979343)])}
................................................................................................................................
 Training Adapter + Prefix with flat at alt even layers 
................................................................................................................................
Available number of GPU = 7 < n_gpus = 12
Continue training with 7 GPUs
2023-08-04 00:50:45,682 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[0, 2, 3, 7, 8, 13, 15], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[24903.68, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=7, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-04 00:50:45,682 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-04 00:50:45,683 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-04 00:50:48,599 - 0:00:07 - 2.9s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
[0]
2023-08-04 00:51:03,510 - 0:00:22 - 14.9s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-04 00:52:45,029 - 0:02:04 - 101.5s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 3.454 , qa loss 3.454 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:53:46,289 - 0:03:05 - 61.3s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 2.16 , qa loss 2.16 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:55:18,816 - 0:04:38 - 92.5s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.399 , qa loss 0.399 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:56:19,852 - 0:05:39 - 61.0s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-08-04 00:57:54,085 - 0:07:13 - 94.2s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.316 , qa loss 0.316 , lm loss 0.000 , avg batch size 4.0
2023-08-04 00:58:52,341 - 0:08:11 - 58.3s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:00:31,359 - 0:09:50 - 99.0s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.281 , qa loss 0.281 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:01:38,989 - 0:10:58 - 67.6s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:03:19,084 - 0:12:38 - 100.1s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.254 , qa loss 0.254 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:04:20,829 - 0:13:40 - 61.7s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:05:50,928 - 0:15:10 - 90.1s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.246 , qa loss 0.246 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:06:52,910 - 0:16:12 - 62.0s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:08:30,619 - 0:17:49 - 97.7s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.238 , qa loss 0.238 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:09:25,621 - 0:18:44 - 55.0s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:11:07,359 - 0:20:26 - 101.7s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.226 , qa loss 0.226 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:12:23,156 - 0:21:42 - 75.8s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:14:03,483 - 0:23:22 - 100.3s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.212 , qa loss 0.212 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:15:05,070 - 0:24:24 - 61.6s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:16:26,958 - 0:25:46 - 81.9s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.222 , qa loss 0.222 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:17:17,007 - 0:26:36 - 50.0s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:18:41,146 - 0:28:00 - 84.1s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.217 , qa loss 0.217 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:19:30,702 - 0:28:49 - 49.6s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:20:51,680 - 0:30:10 - 81.0s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.206 , qa loss 0.206 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:21:43,951 - 0:31:03 - 52.3s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:23:07,505 - 0:32:26 - 83.6s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.204 , qa loss 0.204 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:23:59,772 - 0:33:19 - 52.3s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:25:23,162 - 0:34:42 - 83.4s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.196 , qa loss 0.196 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:26:15,977 - 0:35:35 - 52.8s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:27:39,764 - 0:36:59 - 83.8s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.194 , qa loss 0.194 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:28:32,487 - 0:37:51 - 52.7s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:29:57,539 - 0:39:16 - 85.1s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.189 , qa loss 0.189 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:30:48,290 - 0:40:07 - 50.8s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:32:13,748 - 0:41:32 - 85.5s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.187 , qa loss 0.187 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:33:03,061 - 0:42:22 - 49.3s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:34:27,525 - 0:43:46 - 84.5s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.181 , qa loss 0.181 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:35:15,449 - 0:44:34 - 47.9s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:36:41,835 - 0:46:01 - 86.4s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.175 , qa loss 0.175 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:37:31,231 - 0:46:50 - 49.4s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:38:45,996 - 0:48:05 - 74.8s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.186 , qa loss 0.186 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:39:27,797 - 0:48:47 - 41.8s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:39:29,013 - 0:48:48 - 1.2s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-04 01:39:29,013 - 0:48:48 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-04 01:39:29,148 - 0:48:48 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-08-04 01:39:39,003 - 0:48:58 - 9.9s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-04 01:41:20,344 - 0:50:39 - 101.3s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 6.048 , qa loss 6.048 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:42:14,641 - 0:51:33 - 54.3s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 4.86 , qa loss 4.86 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:43:56,160 - 0:53:15 - 101.5s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 2.186 , qa loss 2.186 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:44:51,351 - 0:54:10 - 55.2s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 2.05 , qa loss 2.05 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:46:33,682 - 0:55:52 - 102.3s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 1.580 , qa loss 1.580 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:47:26,207 - 0:56:45 - 52.5s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 1.52 , qa loss 1.52 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:49:09,048 - 0:58:28 - 102.8s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 1.334 , qa loss 1.334 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:50:01,775 - 0:59:21 - 52.7s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 1.30 , qa loss 1.30 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:51:43,869 - 1:01:03 - 102.1s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 1.165 , qa loss 1.165 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:52:36,702 - 1:01:55 - 52.8s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 1.15 , qa loss 1.15 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:54:16,816 - 1:03:36 - 100.1s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 1.075 , qa loss 1.075 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:55:09,903 - 1:04:29 - 53.1s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 1.06 , qa loss 1.06 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:56:50,666 - 1:06:09 - 100.8s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 1.002 , qa loss 1.002 , lm loss 0.000 , avg batch size 4.0
2023-08-04 01:57:44,429 - 1:07:03 - 53.8s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.98 , qa loss 0.98 , lm loss 0.00 , avg batch size 4.0
2023-08-04 01:59:23,305 - 1:08:42 - 98.9s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.942 , qa loss 0.942 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:00:17,267 - 1:09:36 - 54.0s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.93 , qa loss 0.93 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:01:57,759 - 1:11:17 - 100.5s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.881 , qa loss 0.881 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:02:51,199 - 1:12:10 - 53.4s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.89 , qa loss 0.89 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:04:32,363 - 1:13:51 - 101.2s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.853 , qa loss 0.853 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:05:25,978 - 1:14:45 - 53.6s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.85 , qa loss 0.85 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:07:08,387 - 1:16:27 - 102.4s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.811 , qa loss 0.811 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:08:01,747 - 1:17:20 - 53.4s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.81 , qa loss 0.81 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:09:44,665 - 1:19:03 - 102.9s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.788 , qa loss 0.788 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:10:37,136 - 1:19:56 - 52.5s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.80 , qa loss 0.80 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:12:19,120 - 1:21:38 - 102.0s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.778 , qa loss 0.778 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:13:11,161 - 1:22:30 - 52.0s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.78 , qa loss 0.78 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:14:53,493 - 1:24:12 - 102.3s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.751 , qa loss 0.751 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:15:45,236 - 1:25:04 - 51.7s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.75 , qa loss 0.75 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:17:26,240 - 1:26:45 - 101.0s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.751 , qa loss 0.751 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:18:18,954 - 1:27:38 - 52.7s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.74 , qa loss 0.74 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:20:00,948 - 1:29:20 - 102.0s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.732 , qa loss 0.732 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:20:52,055 - 1:30:11 - 51.1s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.73 , qa loss 0.73 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:22:31,300 - 1:31:50 - 99.2s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.722 , qa loss 0.722 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:23:24,670 - 1:32:43 - 53.4s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.72 , qa loss 0.72 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:25:06,189 - 1:34:25 - 101.5s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.713 , qa loss 0.713 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:26:00,571 - 1:35:19 - 54.4s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.72 , qa loss 0.72 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:27:57,589 - 1:37:16 - 117.0s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.720 , qa loss 0.720 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:28:51,063 - 1:38:10 - 53.5s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.71 , qa loss 0.71 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:30:32,852 - 1:39:52 - 101.8s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.702 , qa loss 0.702 , lm loss 0.000 , avg batch size 4.0
2023-08-04 02:31:25,504 - 1:40:44 - 52.7s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.69 , qa loss 0.69 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:31:26,765 - 1:40:46 - 1.3s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-04 02:31:26,766 - 1:40:46 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-04 02:31:26,907 - 1:40:46 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-08-04 02:31:35,908 - 1:40:55 - 9.0s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-04 02:32:31,252 - 1:41:50 - 55.3s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 6.16 , qa loss 6.16 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:33:25,111 - 1:42:44 - 53.9s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 1.12 , qa loss 1.12 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:34:33,712 - 1:43:52 - 68.6s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.80 , qa loss 0.80 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:35:27,025 - 1:44:46 - 53.3s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:36:33,783 - 1:45:53 - 66.8s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:37:25,475 - 1:46:44 - 51.7s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:38:33,684 - 1:47:52 - 68.2s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:39:27,130 - 1:48:46 - 53.4s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:40:33,661 - 1:49:52 - 66.5s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:41:27,059 - 1:50:46 - 53.4s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:42:32,828 - 1:51:52 - 65.8s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:43:27,276 - 1:52:46 - 54.4s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:44:34,538 - 1:53:53 - 67.3s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:45:26,727 - 1:54:45 - 52.2s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:46:35,002 - 1:55:54 - 68.3s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:47:27,530 - 1:56:46 - 52.5s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:48:36,855 - 1:57:56 - 69.3s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:49:27,935 - 1:58:47 - 51.1s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:50:37,198 - 1:59:56 - 69.3s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-04 02:51:29,345 - 2:00:48 - 52.1s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 02:00:44
CPU Execution time: 01:44:41
................................................................................................................................
 Testing Adapter + Prefix with flat at alt even layers 
................................................................................................................................
2023-08-04 02:51:37,790 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[0], dynamic_epochs=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-04 02:51:49,096 - 0:00:16 - 11.3s - INFO - __main__ - task: sst, epoch: 20
2023-08-04 02:51:49,098 - 0:00:16 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-04 02:51:54,198 - 0:00:21 - 5.1s - INFO - __main__ - len of test dataset: 1821
2023-08-04 02:52:05,249 - 0:00:32 - 11.1s - INFO - __main__ - score: {'sst': OrderedDict([('em', 85.777045579352), ('nf1', 85.777045579352), ('nem', 85.777045579352)]), 'srl': None, 'woz.en': None}
2023-08-04 02:52:16,885 - 0:00:44 - 11.6s - INFO - __main__ - task: srl, epoch: 20
2023-08-04 02:52:16,885 - 0:00:44 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-04 02:52:35,438 - 0:01:03 - 18.6s - INFO - __main__ - len of test dataset: 2201
2023-08-04 03:18:33,850 - 0:27:01 - 1558.4s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 37.30122671512949), ('nf1', 57.75461918000522), ('nem', 42.48069059518401)]), 'woz.en': None}
2023-08-04 03:18:46,008 - 0:27:13 - 12.2s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-04 03:18:46,009 - 0:27:13 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-04 03:18:50,929 - 0:27:18 - 4.9s - INFO - __main__ - len of test dataset: 1646
2023-08-04 03:31:48,967 - 0:40:16 - 778.0s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 14.88456865127582), ('nf1', 91.20295291315954), ('nem', 80.86269744835965), ('joint_goal_em', 73.99756986634264), ('turn_request_em', 89.30741190765492), ('turn_goal_em', 87.12029161603888), ('avg_dialogue', 81.65249088699878)])}
................................................................................................................................
 Training Adapter + Prefix with flat at alt odd layers 
................................................................................................................................
Available number of GPU = 7 < n_gpus = 12
Continue training with 7 GPUs
2023-08-04 03:31:55,182 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[0, 3, 7, 8, 11, 13, 15], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[24903.68, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=7, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[8716, 11927, 11927, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-04 03:31:55,182 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-04 03:31:55,182 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-04 03:31:58,299 - 0:00:07 - 3.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
[0]
2023-08-04 03:32:08,843 - 0:00:18 - 10.5s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-08-04 03:33:21,486 - 0:01:30 - 72.6s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 4.316 , qa loss 4.316 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:34:02,146 - 0:02:11 - 40.7s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 2.62 , qa loss 2.62 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:35:13,465 - 0:03:22 - 71.3s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.277 , qa loss 0.277 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:35:54,526 - 0:04:03 - 41.1s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:37:04,114 - 0:05:13 - 69.6s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.246 , qa loss 0.246 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:37:46,132 - 0:05:55 - 42.0s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:39:11,741 - 0:07:21 - 85.6s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.228 , qa loss 0.228 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:39:51,922 - 0:08:01 - 40.2s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:41:03,348 - 0:09:12 - 71.4s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.204 , qa loss 0.204 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:41:42,388 - 0:09:51 - 39.0s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:42:52,238 - 0:11:01 - 69.8s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.199 , qa loss 0.199 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:43:40,204 - 0:11:49 - 48.0s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:44:51,142 - 0:13:00 - 70.9s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.188 , qa loss 0.188 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:45:30,219 - 0:13:39 - 39.1s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:46:42,254 - 0:14:51 - 72.0s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.174 , qa loss 0.174 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:47:21,940 - 0:15:31 - 39.7s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:48:32,307 - 0:16:41 - 70.4s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.172 , qa loss 0.172 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:49:14,179 - 0:17:23 - 41.9s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:50:23,360 - 0:18:32 - 69.2s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.169 , qa loss 0.169 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:51:02,187 - 0:19:11 - 38.8s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:52:12,909 - 0:20:22 - 70.7s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.165 , qa loss 0.165 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:52:51,805 - 0:21:01 - 38.9s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:54:00,683 - 0:22:10 - 68.9s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.166 , qa loss 0.166 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:54:41,956 - 0:22:51 - 41.3s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:55:52,033 - 0:24:01 - 70.1s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.160 , qa loss 0.160 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:56:32,361 - 0:24:41 - 40.3s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:57:43,078 - 0:25:52 - 70.7s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.151 , qa loss 0.151 , lm loss 0.000 , avg batch size 4.0
2023-08-04 03:58:28,189 - 0:26:37 - 45.1s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-04 03:59:38,446 - 0:27:47 - 70.3s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.153 , qa loss 0.153 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:00:27,685 - 0:28:37 - 49.2s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:01:37,067 - 0:29:46 - 69.4s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.143 , qa loss 0.143 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:02:26,071 - 0:30:35 - 49.0s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:03:35,917 - 0:31:45 - 69.8s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.143 , qa loss 0.143 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:04:22,093 - 0:32:31 - 46.2s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:05:30,863 - 0:33:40 - 68.8s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.129 , qa loss 0.129 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:06:17,939 - 0:34:27 - 47.1s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:07:27,246 - 0:35:36 - 69.3s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.141 , qa loss 0.141 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:08:13,106 - 0:36:22 - 45.9s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:09:23,044 - 0:37:32 - 69.9s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.141 , qa loss 0.141 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:10:14,398 - 0:38:23 - 51.4s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:10:15,585 - 0:38:25 - 1.2s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-04 04:10:15,586 - 0:38:25 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-04 04:10:15,715 - 0:38:25 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-08-04 04:10:24,116 - 0:38:33 - 8.4s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-04 04:12:06,101 - 0:40:15 - 102.0s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 5.289 , qa loss 5.289 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:13:05,243 - 0:41:14 - 59.1s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 3.97 , qa loss 3.97 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:14:46,756 - 0:42:56 - 101.5s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 1.501 , qa loss 1.501 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:15:37,277 - 0:43:46 - 50.5s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 1.44 , qa loss 1.44 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:17:16,357 - 0:45:25 - 99.1s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 1.207 , qa loss 1.207 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:18:10,398 - 0:46:19 - 54.0s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 1.16 , qa loss 1.16 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:19:50,671 - 0:48:00 - 100.3s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 1.011 , qa loss 1.011 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:20:43,128 - 0:48:52 - 52.5s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.98 , qa loss 0.98 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:22:24,693 - 0:50:34 - 101.6s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.923 , qa loss 0.923 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:23:17,826 - 0:51:27 - 53.1s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.91 , qa loss 0.91 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:25:00,083 - 0:53:09 - 102.3s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.849 , qa loss 0.849 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:25:50,507 - 0:53:59 - 50.4s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.84 , qa loss 0.84 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:27:31,566 - 0:55:41 - 101.1s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.806 , qa loss 0.806 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:28:22,639 - 0:56:32 - 51.1s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.79 , qa loss 0.79 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:30:00,322 - 0:58:09 - 97.7s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.784 , qa loss 0.784 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:30:52,922 - 0:59:02 - 52.6s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.77 , qa loss 0.77 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:32:33,628 - 1:00:43 - 100.7s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.738 , qa loss 0.738 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:33:25,774 - 1:01:35 - 52.1s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.74 , qa loss 0.74 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:35:07,021 - 1:03:16 - 101.2s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.700 , qa loss 0.700 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:35:58,489 - 1:04:07 - 51.5s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.71 , qa loss 0.71 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:37:39,766 - 1:05:49 - 101.3s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.696 , qa loss 0.696 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:38:32,399 - 1:06:41 - 52.6s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.70 , qa loss 0.70 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:40:14,096 - 1:08:23 - 101.7s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.678 , qa loss 0.678 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:41:06,275 - 1:09:15 - 52.2s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.68 , qa loss 0.68 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:42:47,449 - 1:10:56 - 101.2s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.668 , qa loss 0.668 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:43:39,143 - 1:11:48 - 51.7s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.66 , qa loss 0.66 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:45:19,720 - 1:13:29 - 100.6s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.642 , qa loss 0.642 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:46:12,037 - 1:14:21 - 52.3s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:47:52,990 - 1:16:02 - 101.0s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.644 , qa loss 0.644 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:48:44,800 - 1:16:54 - 51.8s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.64 , qa loss 0.64 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:50:25,162 - 1:18:34 - 100.4s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.636 , qa loss 0.636 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:51:16,118 - 1:19:25 - 51.0s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:52:56,786 - 1:21:06 - 100.7s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.631 , qa loss 0.631 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:53:49,485 - 1:21:58 - 52.7s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.62 , qa loss 0.62 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:55:30,751 - 1:23:40 - 101.3s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.610 , qa loss 0.610 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:56:22,168 - 1:24:31 - 51.4s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-08-04 04:58:02,070 - 1:26:11 - 99.9s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.600 , qa loss 0.600 , lm loss 0.000 , avg batch size 4.0
2023-08-04 04:58:52,343 - 1:27:01 - 50.3s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.60 , qa loss 0.60 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:00:28,973 - 1:28:38 - 96.6s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.591 , qa loss 0.591 , lm loss 0.000 , avg batch size 4.0
2023-08-04 05:01:23,126 - 1:29:32 - 54.2s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.60 , qa loss 0.60 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:01:24,350 - 1:29:33 - 1.2s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-04 05:01:24,350 - 1:29:33 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-04 05:01:24,475 - 1:29:33 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-08-04 05:01:38,547 - 1:29:48 - 14.1s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-04 05:02:31,680 - 1:30:41 - 53.1s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 6.00 , qa loss 6.00 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:03:22,867 - 1:31:32 - 51.2s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.70 , qa loss 0.70 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:04:14,496 - 1:32:23 - 51.6s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:05:06,094 - 1:33:15 - 51.6s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:05:58,086 - 1:34:07 - 52.0s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:06:50,968 - 1:35:00 - 52.9s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:07:43,018 - 1:35:52 - 52.1s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:08:34,692 - 1:36:44 - 51.7s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:09:25,792 - 1:37:35 - 51.1s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:10:17,502 - 1:38:26 - 51.7s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:11:09,563 - 1:39:19 - 52.1s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:12:02,369 - 1:40:11 - 52.8s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:12:54,825 - 1:41:04 - 52.5s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:13:47,127 - 1:41:56 - 52.3s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:14:38,880 - 1:42:48 - 51.8s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:15:30,868 - 1:43:40 - 52.0s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:16:23,024 - 1:44:32 - 52.2s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:17:14,841 - 1:45:24 - 51.8s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:18:07,075 - 1:46:16 - 52.2s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-04 05:18:59,875 - 1:47:09 - 52.8s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:47:05
CPU Execution time: 01:31:39
................................................................................................................................
 Training Adapter + Prefix with flat at alt odd layers 
................................................................................................................................
2023-08-04 05:19:07,186 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[0], dynamic_epochs=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-04 05:19:18,685 - 0:00:16 - 11.5s - INFO - __main__ - task: sst, epoch: 20
2023-08-04 05:19:18,686 - 0:00:16 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-04 05:19:23,713 - 0:00:21 - 5.0s - INFO - __main__ - len of test dataset: 1821
2023-08-04 05:19:34,775 - 0:00:32 - 11.1s - INFO - __main__ - score: {'sst': OrderedDict([('em', 88.08347062053817), ('nf1', 88.08347062053817), ('nem', 88.08347062053817)]), 'srl': None, 'woz.en': None}
2023-08-04 05:19:45,986 - 0:00:43 - 11.2s - INFO - __main__ - task: srl, epoch: 20
2023-08-04 05:19:45,986 - 0:00:43 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-04 05:19:51,162 - 0:00:48 - 5.2s - INFO - __main__ - len of test dataset: 2201
2023-08-04 05:49:04,160 - 0:30:01 - 1753.0s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 36.0290776919582), ('nf1', 57.798166650923356), ('nem', 41.526578827805544)]), 'woz.en': None}
2023-08-04 05:49:16,108 - 0:30:13 - 11.9s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-04 05:49:16,108 - 0:30:13 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-04 05:49:20,993 - 0:30:18 - 4.9s - INFO - __main__ - len of test dataset: 1646
2023-08-04 06:02:32,455 - 0:43:29 - 791.5s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 15.91737545565006), ('nf1', 92.10884295938978), ('nem', 82.86755771567437), ('joint_goal_em', 78.06804374240583), ('turn_request_em', 90.5224787363305), ('turn_goal_em', 88.21385176184691), ('avg_dialogue', 84.29526123936816)])}
................................................................................................................................
 The End Man! 
................................................................................................................................
