Available number of GPU = 1 < n_gpus = 12
Continue training with 1 GPUs
2023-07-30 21:24:33,377 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[13], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='model_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 10, 'srl': 10, 'woz.en': 10}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[11468], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[11468], unbound=0, use_sep=False, weight_decay=0.01)
2023-07-30 21:24:33,378 - 0:00:06 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-07-30 21:24:33,378 - 0:00:06 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-30 21:24:37,255 - 0:00:10 - 3.9s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
[0]
2023-07-30 21:24:50,157 - 0:00:23 - 12.9s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 69200
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2023-07-30 21:26:03,444 - 0:01:36 - 73.3s - INFO - __main__ - progress 0.578 , lr 5.9E-05 , loss 1.940 , qa loss 1.940 , lm loss 0.000 , avg batch size 4.0
2023-07-30 21:26:45,507 - 0:02:18 - 42.1s - INFO - __main__ - epoch 1/10 done , tot steps 1730 , lr 5.6E-05 , loss 1.26 , qa loss 1.26 , lm loss 0.00 , avg batch size 4.0
2023-07-30 21:27:56,652 - 0:03:29 - 71.1s - INFO - __main__ - progress 1.578 , lr 5.3E-05 , loss 0.300 , qa loss 0.300 , lm loss 0.000 , avg batch size 4.0
2023-07-30 21:28:38,142 - 0:04:11 - 41.5s - INFO - __main__ - epoch 2/10 done , tot steps 3460 , lr 5.0E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-07-30 21:29:48,796 - 0:05:21 - 70.7s - INFO - __main__ - progress 2.578 , lr 4.6E-05 , loss 0.272 , qa loss 0.272 , lm loss 0.000 , avg batch size 4.0
2023-07-30 21:30:31,110 - 0:06:04 - 42.3s - INFO - __main__ - epoch 3/10 done , tot steps 5190 , lr 4.4E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-07-30 21:31:41,042 - 0:07:14 - 69.9s - INFO - __main__ - progress 3.578 , lr 4.0E-05 , loss 0.229 , qa loss 0.229 , lm loss 0.000 , avg batch size 4.0
2023-07-30 21:32:22,303 - 0:07:55 - 41.3s - INFO - __main__ - epoch 4/10 done , tot steps 6920 , lr 3.8E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-07-30 21:33:32,524 - 0:09:05 - 70.2s - INFO - __main__ - progress 4.578 , lr 3.4E-05 , loss 0.195 , qa loss 0.195 , lm loss 0.000 , avg batch size 4.0
2023-07-30 21:34:12,420 - 0:09:45 - 39.9s - INFO - __main__ - epoch 5/10 done , tot steps 8650 , lr 3.1E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-07-30 21:35:21,139 - 0:10:54 - 68.7s - INFO - __main__ - progress 5.578 , lr 2.8E-05 , loss 0.181 , qa loss 0.181 , lm loss 0.000 , avg batch size 4.0
2023-07-30 21:35:56,953 - 0:11:30 - 35.8s - INFO - __main__ - epoch 6/10 done , tot steps 10380 , lr 2.5E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-07-30 21:36:56,848 - 0:12:29 - 59.9s - INFO - __main__ - progress 6.578 , lr 2.1E-05 , loss 0.168 , qa loss 0.168 , lm loss 0.000 , avg batch size 4.0
2023-07-30 21:37:33,601 - 0:13:06 - 36.8s - INFO - __main__ - epoch 7/10 done , tot steps 12110 , lr 1.9E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-07-30 21:38:44,867 - 0:14:17 - 71.3s - INFO - __main__ - progress 7.578 , lr 1.5E-05 , loss 0.161 , qa loss 0.161 , lm loss 0.000 , avg batch size 4.0
2023-07-30 21:39:25,971 - 0:14:59 - 41.1s - INFO - __main__ - epoch 8/10 done , tot steps 13840 , lr 1.3E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-30 21:40:36,707 - 0:16:09 - 70.7s - INFO - __main__ - progress 8.578 , lr 8.9E-06 , loss 0.156 , qa loss 0.156 , lm loss 0.000 , avg batch size 4.0
2023-07-30 21:41:18,239 - 0:16:51 - 41.5s - INFO - __main__ - epoch 9/10 done , tot steps 15570 , lr 6.3E-06 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-30 21:42:29,486 - 0:18:02 - 71.2s - INFO - __main__ - progress 9.578 , lr 2.7E-06 , loss 0.149 , qa loss 0.149 , lm loss 0.000 , avg batch size 4.0
2023-07-30 21:43:12,023 - 0:18:45 - 42.5s - INFO - __main__ - epoch 10/10 done , tot steps 17300 , lr 3.1E-08 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-07-30 21:43:13,631 - 0:18:46 - 1.6s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-07-30 21:43:13,632 - 0:18:46 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-30 21:43:13,789 - 0:18:46 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-07-30 21:43:24,350 - 0:18:57 - 10.6s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 64140
2023-07-30 21:45:01,944 - 0:20:35 - 97.6s - INFO - __main__ - progress 0.624 , lr 5.9E-05 , loss 3.049 , qa loss 3.049 , lm loss 0.000 , avg batch size 4.0
2023-07-30 21:45:52,221 - 0:21:25 - 50.3s - INFO - __main__ - epoch 1/10 done , tot steps 1604 , lr 5.6E-05 , loss 2.32 , qa loss 2.32 , lm loss 0.00 , avg batch size 4.0
2023-07-30 21:47:28,547 - 0:23:01 - 96.3s - INFO - __main__ - progress 1.624 , lr 5.2E-05 , loss 0.927 , qa loss 0.927 , lm loss 0.000 , avg batch size 4.0
2023-07-30 21:48:20,526 - 0:23:53 - 52.0s - INFO - __main__ - epoch 2/10 done , tot steps 3208 , lr 5.0E-05 , loss 0.88 , qa loss 0.88 , lm loss 0.00 , avg batch size 4.0
2023-07-30 21:49:58,211 - 0:25:31 - 97.7s - INFO - __main__ - progress 2.624 , lr 4.6E-05 , loss 0.716 , qa loss 0.716 , lm loss 0.000 , avg batch size 4.0
2023-07-30 21:50:48,817 - 0:26:21 - 50.6s - INFO - __main__ - epoch 3/10 done , tot steps 4812 , lr 4.4E-05 , loss 0.71 , qa loss 0.71 , lm loss 0.00 , avg batch size 4.0
2023-07-30 21:52:25,205 - 0:27:58 - 96.4s - INFO - __main__ - progress 3.624 , lr 4.0E-05 , loss 0.632 , qa loss 0.632 , lm loss 0.000 , avg batch size 4.0
2023-07-30 21:53:15,310 - 0:28:48 - 50.1s - INFO - __main__ - epoch 4/10 done , tot steps 6416 , lr 3.8E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-07-30 21:54:49,244 - 0:30:22 - 93.9s - INFO - __main__ - progress 4.624 , lr 3.4E-05 , loss 0.591 , qa loss 0.591 , lm loss 0.000 , avg batch size 4.0
2023-07-30 21:55:39,499 - 0:31:12 - 50.3s - INFO - __main__ - epoch 5/10 done , tot steps 8020 , lr 3.1E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-07-30 21:57:06,859 - 0:32:39 - 87.4s - INFO - __main__ - progress 5.624 , lr 2.7E-05 , loss 0.543 , qa loss 0.543 , lm loss 0.000 , avg batch size 4.0
2023-07-30 21:57:55,637 - 0:33:28 - 48.8s - INFO - __main__ - epoch 6/10 done , tot steps 9624 , lr 2.5E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-07-30 21:59:34,756 - 0:35:07 - 99.1s - INFO - __main__ - progress 6.624 , lr 2.1E-05 , loss 0.521 , qa loss 0.521 , lm loss 0.000 , avg batch size 4.0
2023-07-30 22:00:26,310 - 0:35:59 - 51.6s - INFO - __main__ - epoch 7/10 done , tot steps 11228 , lr 1.9E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-07-30 22:02:05,725 - 0:37:38 - 99.4s - INFO - __main__ - progress 7.624 , lr 1.5E-05 , loss 0.502 , qa loss 0.502 , lm loss 0.000 , avg batch size 4.0
2023-07-30 22:02:57,471 - 0:38:30 - 51.7s - INFO - __main__ - epoch 8/10 done , tot steps 12832 , lr 1.3E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-07-30 22:04:35,951 - 0:40:09 - 98.5s - INFO - __main__ - progress 8.624 , lr 8.6E-06 , loss 0.460 , qa loss 0.460 , lm loss 0.000 , avg batch size 4.0
2023-07-30 22:05:28,040 - 0:41:01 - 52.1s - INFO - __main__ - epoch 9/10 done , tot steps 14436 , lr 6.3E-06 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-07-30 22:07:08,072 - 0:42:41 - 100.0s - INFO - __main__ - progress 9.624 , lr 2.4E-06 , loss 0.452 , qa loss 0.452 , lm loss 0.000 , avg batch size 4.0
2023-07-30 22:07:59,167 - 0:43:32 - 51.1s - INFO - __main__ - epoch 10/10 done , tot steps 16040 , lr 3.1E-08 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-07-30 22:08:00,836 - 0:43:33 - 1.7s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-07-30 22:08:00,837 - 0:43:33 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-30 22:08:00,997 - 0:43:34 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-07-30 22:08:12,645 - 0:43:45 - 11.6s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 25360
2023-07-30 22:09:07,503 - 0:44:40 - 54.9s - INFO - __main__ - epoch 1/10 done , tot steps 634 , lr 5.6E-05 , loss 2.73 , qa loss 2.73 , lm loss 0.00 , avg batch size 4.0
2023-07-30 22:10:03,644 - 0:45:36 - 56.1s - INFO - __main__ - epoch 2/10 done , tot steps 1268 , lr 5.0E-05 , loss 0.72 , qa loss 0.72 , lm loss 0.00 , avg batch size 4.0
2023-07-30 22:11:00,604 - 0:46:33 - 57.0s - INFO - __main__ - epoch 3/10 done , tot steps 1902 , lr 4.4E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-07-30 22:11:52,245 - 0:47:25 - 51.6s - INFO - __main__ - epoch 4/10 done , tot steps 2536 , lr 3.8E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-07-30 22:12:46,680 - 0:48:19 - 54.4s - INFO - __main__ - epoch 5/10 done , tot steps 3170 , lr 3.1E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-07-30 22:13:40,564 - 0:49:13 - 53.9s - INFO - __main__ - epoch 6/10 done , tot steps 3804 , lr 2.5E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-07-30 22:14:32,627 - 0:50:05 - 52.1s - INFO - __main__ - epoch 7/10 done , tot steps 4438 , lr 1.9E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-07-30 22:15:24,537 - 0:50:57 - 51.9s - INFO - __main__ - epoch 8/10 done , tot steps 5072 , lr 1.3E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-07-30 22:16:13,475 - 0:51:46 - 48.9s - INFO - __main__ - epoch 9/10 done , tot steps 5706 , lr 6.3E-06 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-07-30 22:16:58,633 - 0:52:31 - 45.2s - INFO - __main__ - epoch 10/10 done , tot steps 6340 , lr 3.0E-08 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 00:52:26
CPU Execution time: 00:45:59
