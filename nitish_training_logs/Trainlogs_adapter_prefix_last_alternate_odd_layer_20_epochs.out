Available number of GPU = 1 < n_gpus = 12
Continue training with 1 GPUs
2023-08-02 18:53:38,279 - 0:00:11 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[13], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[11468], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[11468], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-02 18:53:38,280 - 0:00:11 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-02 18:53:38,280 - 0:00:11 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-02 18:53:45,897 - 0:00:18 - 7.6s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
[0]
2023-08-02 18:53:59,129 - 0:00:32 - 13.2s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2023-08-02 18:55:37,004 - 0:02:10 - 97.9s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.909 , qa loss 1.909 , lm loss 0.000 , avg batch size 4.0
2023-08-02 18:56:35,385 - 0:03:08 - 58.4s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.22 , qa loss 1.22 , lm loss 0.00 , avg batch size 4.0
2023-08-02 18:58:15,538 - 0:04:48 - 100.2s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.282 , qa loss 0.282 , lm loss 0.000 , avg batch size 4.0
2023-08-02 18:59:05,992 - 0:05:39 - 50.5s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:00:22,597 - 0:06:55 - 76.6s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.202 , qa loss 0.202 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:01:09,778 - 0:07:42 - 47.2s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:02:24,857 - 0:08:57 - 75.1s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.179 , qa loss 0.179 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:03:17,504 - 0:09:50 - 52.6s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:04:40,031 - 0:11:13 - 82.5s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.157 , qa loss 0.157 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:05:29,205 - 0:12:02 - 49.2s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:06:48,869 - 0:13:21 - 79.7s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.151 , qa loss 0.151 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:07:35,834 - 0:14:08 - 47.0s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:08:59,064 - 0:15:32 - 83.2s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.147 , qa loss 0.147 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:09:48,058 - 0:16:21 - 49.0s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:11:04,936 - 0:17:37 - 76.9s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.130 , qa loss 0.130 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:11:52,825 - 0:18:25 - 47.9s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:13:10,398 - 0:19:43 - 77.6s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.133 , qa loss 0.133 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:13:58,915 - 0:20:31 - 48.5s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:15:35,462 - 0:22:08 - 96.5s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.117 , qa loss 0.117 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:16:31,832 - 0:23:04 - 56.4s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:18:01,001 - 0:24:34 - 89.2s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.112 , qa loss 0.112 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:19:01,870 - 0:25:34 - 60.9s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:20:35,890 - 0:27:08 - 94.0s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.106 , qa loss 0.106 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:21:35,495 - 0:28:08 - 59.6s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:22:59,268 - 0:29:32 - 83.8s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.099 , qa loss 0.099 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:23:44,897 - 0:30:17 - 45.6s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:24:59,436 - 0:31:32 - 74.5s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.098 , qa loss 0.098 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:25:42,132 - 0:32:15 - 42.7s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:26:49,057 - 0:33:22 - 66.9s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.094 , qa loss 0.094 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:27:31,497 - 0:34:04 - 42.4s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:28:48,996 - 0:35:22 - 77.5s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.096 , qa loss 0.096 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:29:37,150 - 0:36:10 - 48.2s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:31:08,147 - 0:37:41 - 91.0s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.081 , qa loss 0.081 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:32:15,763 - 0:38:48 - 67.6s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:33:55,196 - 0:40:28 - 99.4s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.088 , qa loss 0.088 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:35:01,978 - 0:41:35 - 66.8s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:36:23,282 - 0:42:56 - 81.3s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.080 , qa loss 0.080 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:37:12,377 - 0:43:45 - 49.1s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:38:28,027 - 0:45:01 - 75.6s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.094 , qa loss 0.094 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:39:12,753 - 0:45:45 - 44.7s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:39:13,946 - 0:45:46 - 1.2s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-02 19:39:13,947 - 0:45:46 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-02 19:39:14,127 - 0:45:47 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-08-02 19:39:25,454 - 0:45:58 - 11.3s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-02 19:41:19,136 - 0:47:52 - 113.7s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 3.099 , qa loss 3.099 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:42:21,880 - 0:48:54 - 62.7s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.31 , qa loss 2.31 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:44:17,604 - 0:50:50 - 115.7s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.845 , qa loss 0.845 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:45:18,826 - 0:51:51 - 61.2s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.81 , qa loss 0.81 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:47:11,130 - 0:53:44 - 112.3s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.710 , qa loss 0.710 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:48:05,989 - 0:54:39 - 54.9s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.71 , qa loss 0.71 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:49:42,878 - 0:56:15 - 96.9s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.636 , qa loss 0.636 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:50:34,551 - 0:57:07 - 51.7s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:52:14,527 - 0:58:47 - 100.0s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.596 , qa loss 0.596 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:53:20,498 - 0:59:53 - 66.0s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:55:25,521 - 1:01:58 - 125.0s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.550 , qa loss 0.550 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:56:35,071 - 1:03:08 - 69.6s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-08-02 19:58:35,344 - 1:05:08 - 120.3s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.528 , qa loss 0.528 , lm loss 0.000 , avg batch size 4.0
2023-08-02 19:59:46,511 - 1:06:19 - 71.2s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:01:38,173 - 1:08:11 - 111.7s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.509 , qa loss 0.509 , lm loss 0.000 , avg batch size 4.0
2023-08-02 20:02:31,145 - 1:09:04 - 53.0s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:04:05,798 - 1:10:38 - 94.7s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.490 , qa loss 0.490 , lm loss 0.000 , avg batch size 4.0
2023-08-02 20:04:56,927 - 1:11:29 - 51.1s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:06:44,832 - 1:13:17 - 107.9s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.458 , qa loss 0.458 , lm loss 0.000 , avg batch size 4.0
2023-08-02 20:07:43,007 - 1:14:16 - 58.2s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:09:29,578 - 1:16:02 - 106.6s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.451 , qa loss 0.451 , lm loss 0.000 , avg batch size 4.0
2023-08-02 20:10:31,785 - 1:17:04 - 62.2s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:12:38,056 - 1:19:11 - 126.3s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.447 , qa loss 0.447 , lm loss 0.000 , avg batch size 4.0
2023-08-02 20:13:41,038 - 1:20:14 - 63.0s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:15:22,152 - 1:21:55 - 101.1s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.423 , qa loss 0.423 , lm loss 0.000 , avg batch size 4.0
2023-08-02 20:16:14,283 - 1:22:47 - 52.1s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:17:55,728 - 1:24:28 - 101.4s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.398 , qa loss 0.398 , lm loss 0.000 , avg batch size 4.0
2023-08-02 20:19:01,723 - 1:25:34 - 66.0s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:21:00,011 - 1:27:33 - 118.3s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.403 , qa loss 0.403 , lm loss 0.000 , avg batch size 4.0
2023-08-02 20:22:04,052 - 1:28:37 - 64.0s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:23:56,623 - 1:30:29 - 112.6s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.390 , qa loss 0.390 , lm loss 0.000 , avg batch size 4.0
2023-08-02 20:24:57,279 - 1:31:30 - 60.7s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:26:41,188 - 1:33:14 - 103.9s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.374 , qa loss 0.374 , lm loss 0.000 , avg batch size 4.0
2023-08-02 20:27:31,191 - 1:34:04 - 50.0s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:29:02,621 - 1:35:35 - 91.4s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.355 , qa loss 0.355 , lm loss 0.000 , avg batch size 4.0
2023-08-02 20:29:54,807 - 1:36:27 - 52.2s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:31:58,548 - 1:38:31 - 123.7s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.356 , qa loss 0.356 , lm loss 0.000 , avg batch size 4.0
2023-08-02 20:33:08,910 - 1:39:41 - 70.4s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:35:17,003 - 1:41:50 - 128.1s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.347 , qa loss 0.347 , lm loss 0.000 , avg batch size 4.0
2023-08-02 20:36:25,257 - 1:42:58 - 68.3s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:36:26,882 - 1:42:59 - 1.6s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-02 20:36:26,883 - 1:42:59 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-02 20:36:27,208 - 1:43:00 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-08-02 20:36:40,440 - 1:43:13 - 13.2s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-02 20:37:49,637 - 1:44:22 - 69.2s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 3.35 , qa loss 3.35 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:38:54,794 - 1:45:27 - 65.2s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:39:48,743 - 1:46:21 - 53.9s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:40:41,209 - 1:47:14 - 52.5s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:41:33,407 - 1:48:06 - 52.2s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:42:26,871 - 1:48:59 - 53.5s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:43:21,448 - 1:49:54 - 54.6s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:44:22,101 - 1:50:55 - 60.7s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:45:21,784 - 1:51:54 - 59.7s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:46:19,542 - 1:52:52 - 57.8s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:47:17,928 - 1:53:50 - 58.4s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:48:14,064 - 1:54:47 - 56.1s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:49:12,630 - 1:55:45 - 58.6s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:50:19,276 - 1:56:52 - 66.6s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:51:13,666 - 1:57:46 - 54.4s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:52:07,737 - 1:58:40 - 54.1s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:53:01,410 - 1:59:34 - 53.7s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:53:55,533 - 2:00:28 - 54.1s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:54:51,069 - 2:01:24 - 55.5s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-02 20:55:47,791 - 2:02:20 - 56.7s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 02:02:10
CPU Execution time: 01:31:40
