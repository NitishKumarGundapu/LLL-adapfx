Available number of GPU = 1 < n_gpus = 12
Continue training with 1 GPUs
2023-07-30 23:56:40,570 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[13], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='model_ag/gpt2/lll/wikisql_sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'wikisql': 10, 'sst': 10, 'srl': 10, 'woz.en': 10}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['wikisql', 'sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[11468], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[11468], unbound=0, use_sep=False, weight_decay=0.01)
2023-07-30 23:56:40,571 - 0:00:06 - 0.0s - INFO - __main__ - start to train { task: ['wikisql'], seq train type: lll }
2023-07-30 23:56:40,571 - 0:00:06 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-30 23:56:43,913 - 0:00:10 - 3.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
[0]
2023-07-31 00:02:26,753 - 0:05:53 - 342.8s - INFO - __main__ - len of train dataset: 56355 , max train batch size 37 , num of opt steps: 563550
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2023-07-31 00:09:24,884 - 0:12:51 - 418.1s - INFO - __main__ - progress 0.540 , lr 5.9E-05 , loss 2.514 , qa loss 2.514 , lm loss 0.000 , avg batch size 30.4
2023-07-31 00:15:06,463 - 0:18:32 - 341.6s - INFO - __main__ - epoch 1/10 done , tot steps 1868 , lr 5.6E-05 , loss 1.51 , qa loss 1.51 , lm loss 0.00 , avg batch size 30.2
2023-07-31 00:21:50,196 - 0:25:16 - 403.7s - INFO - __main__ - progress 1.533 , lr 5.3E-05 , loss 0.255 , qa loss 0.255 , lm loss 0.000 , avg batch size 30.1
2023-07-31 00:27:30,315 - 0:30:56 - 340.1s - INFO - __main__ - epoch 2/10 done , tot steps 3740 , lr 5.0E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 30.1
2023-07-31 00:34:12,663 - 0:37:39 - 402.3s - INFO - __main__ - progress 2.536 , lr 4.7E-05 , loss 0.192 , qa loss 0.192 , lm loss 0.000 , avg batch size 30.2
2023-07-31 00:39:53,439 - 0:43:19 - 340.8s - INFO - __main__ - epoch 3/10 done , tot steps 5609 , lr 4.4E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 30.2
2023-07-31 00:46:37,044 - 0:50:03 - 403.6s - INFO - __main__ - progress 3.532 , lr 4.0E-05 , loss 0.164 , qa loss 0.164 , lm loss 0.000 , avg batch size 30.0
2023-07-31 00:52:18,790 - 0:55:45 - 341.7s - INFO - __main__ - epoch 4/10 done , tot steps 7483 , lr 3.8E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 30.1
2023-07-31 00:59:03,307 - 1:02:29 - 404.5s - INFO - __main__ - progress 4.534 , lr 3.4E-05 , loss 0.145 , qa loss 0.145 , lm loss 0.000 , avg batch size 30.1
2023-07-31 01:04:45,667 - 1:08:12 - 342.4s - INFO - __main__ - epoch 5/10 done , tot steps 9358 , lr 3.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 30.1
2023-07-31 01:11:28,292 - 1:14:54 - 402.6s - INFO - __main__ - progress 5.534 , lr 2.8E-05 , loss 0.135 , qa loss 0.135 , lm loss 0.000 , avg batch size 30.1
2023-07-31 01:17:07,027 - 1:20:33 - 338.7s - INFO - __main__ - epoch 6/10 done , tot steps 11226 , lr 2.5E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 30.2
2023-07-31 01:23:49,733 - 1:27:16 - 402.7s - INFO - __main__ - progress 6.531 , lr 2.2E-05 , loss 0.129 , qa loss 0.129 , lm loss 0.000 , avg batch size 29.9
2023-07-31 01:29:28,249 - 1:32:54 - 338.5s - INFO - __main__ - epoch 7/10 done , tot steps 13090 , lr 1.9E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 30.2
2023-07-31 01:36:10,786 - 1:39:37 - 402.5s - INFO - __main__ - progress 7.534 , lr 1.5E-05 , loss 0.123 , qa loss 0.123 , lm loss 0.000 , avg batch size 30.1
2023-07-31 01:41:51,113 - 1:45:17 - 340.3s - INFO - __main__ - epoch 8/10 done , tot steps 14960 , lr 1.3E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 30.1
2023-07-31 01:48:33,655 - 1:52:00 - 402.5s - INFO - __main__ - progress 8.534 , lr 9.2E-06 , loss 0.119 , qa loss 0.119 , lm loss 0.000 , avg batch size 30.1
2023-07-31 01:54:13,281 - 1:57:39 - 339.6s - INFO - __main__ - epoch 9/10 done , tot steps 16828 , lr 6.3E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 30.2
2023-07-31 02:00:56,999 - 2:04:23 - 403.7s - INFO - __main__ - progress 9.533 , lr 3.0E-06 , loss 0.116 , qa loss 0.116 , lm loss 0.000 , avg batch size 30.0
2023-07-31 02:06:37,889 - 2:10:04 - 340.9s - INFO - __main__ - epoch 10/10 done , tot steps 18697 , lr 3.1E-08 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 30.2
2023-07-31 02:06:40,299 - 2:10:06 - 2.4s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-07-31 02:06:40,300 - 2:10:06 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-31 02:06:40,480 - 2:10:06 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[wikisql]
The task with which model is saved wikisql
[1]
2023-07-31 02:06:56,478 - 2:10:22 - 16.0s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 69200
2023-07-31 02:08:24,984 - 2:11:51 - 88.5s - INFO - __main__ - progress 0.578 , lr 5.9E-05 , loss 3.227 , qa loss 3.227 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:09:20,125 - 2:12:46 - 55.1s - INFO - __main__ - epoch 1/10 done , tot steps 1730 , lr 5.6E-05 , loss 1.97 , qa loss 1.97 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:10:37,335 - 2:14:03 - 77.2s - INFO - __main__ - progress 1.578 , lr 5.3E-05 , loss 0.220 , qa loss 0.220 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:11:24,756 - 2:14:51 - 47.4s - INFO - __main__ - epoch 2/10 done , tot steps 3460 , lr 5.0E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:12:42,518 - 2:16:08 - 77.8s - INFO - __main__ - progress 2.578 , lr 4.6E-05 , loss 0.191 , qa loss 0.191 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:13:33,253 - 2:16:59 - 50.7s - INFO - __main__ - epoch 3/10 done , tot steps 5190 , lr 4.4E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:15:07,773 - 2:18:34 - 94.5s - INFO - __main__ - progress 3.578 , lr 4.0E-05 , loss 0.177 , qa loss 0.177 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:16:05,745 - 2:19:32 - 58.0s - INFO - __main__ - epoch 4/10 done , tot steps 6920 , lr 3.8E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:17:37,674 - 2:21:04 - 91.9s - INFO - __main__ - progress 4.578 , lr 3.4E-05 , loss 0.162 , qa loss 0.162 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:18:35,889 - 2:22:02 - 58.2s - INFO - __main__ - epoch 5/10 done , tot steps 8650 , lr 3.1E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:20:07,591 - 2:23:34 - 91.7s - INFO - __main__ - progress 5.578 , lr 2.8E-05 , loss 0.143 , qa loss 0.143 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:21:06,374 - 2:24:32 - 58.8s - INFO - __main__ - epoch 6/10 done , tot steps 10380 , lr 2.5E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:22:37,539 - 2:26:03 - 91.2s - INFO - __main__ - progress 6.578 , lr 2.1E-05 , loss 0.128 , qa loss 0.128 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:23:33,327 - 2:26:59 - 55.8s - INFO - __main__ - epoch 7/10 done , tot steps 12110 , lr 1.9E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:25:03,693 - 2:28:30 - 90.4s - INFO - __main__ - progress 7.578 , lr 1.5E-05 , loss 0.119 , qa loss 0.119 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:26:01,056 - 2:29:27 - 57.4s - INFO - __main__ - epoch 8/10 done , tot steps 13840 , lr 1.3E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:27:32,475 - 2:30:58 - 91.4s - INFO - __main__ - progress 8.578 , lr 8.9E-06 , loss 0.119 , qa loss 0.119 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:28:30,430 - 2:31:56 - 58.0s - INFO - __main__ - epoch 9/10 done , tot steps 15570 , lr 6.3E-06 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:30:02,721 - 2:33:29 - 92.3s - INFO - __main__ - progress 9.578 , lr 2.7E-06 , loss 0.107 , qa loss 0.107 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:31:01,669 - 2:34:28 - 58.9s - INFO - __main__ - epoch 10/10 done , tot steps 17300 , lr 3.1E-08 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:31:02,766 - 2:34:29 - 1.1s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-07-31 02:31:02,767 - 2:34:29 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-31 02:31:02,931 - 2:34:29 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[2]
2023-07-31 02:31:16,032 - 2:34:42 - 13.1s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 64140
2023-07-31 02:33:12,014 - 2:36:38 - 116.0s - INFO - __main__ - progress 0.624 , lr 5.9E-05 , loss 3.745 , qa loss 3.745 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:34:11,949 - 2:37:38 - 59.9s - INFO - __main__ - epoch 1/10 done , tot steps 1604 , lr 5.6E-05 , loss 2.69 , qa loss 2.69 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:36:05,460 - 2:39:31 - 113.5s - INFO - __main__ - progress 1.624 , lr 5.2E-05 , loss 0.781 , qa loss 0.781 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:37:08,506 - 2:40:34 - 63.0s - INFO - __main__ - epoch 2/10 done , tot steps 3208 , lr 5.0E-05 , loss 0.76 , qa loss 0.76 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:39:02,891 - 2:42:29 - 114.4s - INFO - __main__ - progress 2.624 , lr 4.6E-05 , loss 0.654 , qa loss 0.654 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:40:02,974 - 2:43:29 - 60.1s - INFO - __main__ - epoch 3/10 done , tot steps 4812 , lr 4.4E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:41:58,504 - 2:45:24 - 115.5s - INFO - __main__ - progress 3.624 , lr 4.0E-05 , loss 0.571 , qa loss 0.571 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:42:58,199 - 2:46:24 - 59.7s - INFO - __main__ - epoch 4/10 done , tot steps 6416 , lr 3.8E-05 , loss 0.57 , qa loss 0.57 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:44:52,461 - 2:48:18 - 114.3s - INFO - __main__ - progress 4.624 , lr 3.4E-05 , loss 0.517 , qa loss 0.517 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:45:53,548 - 2:49:19 - 61.1s - INFO - __main__ - epoch 5/10 done , tot steps 8020 , lr 3.1E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:47:46,677 - 2:51:13 - 113.1s - INFO - __main__ - progress 5.624 , lr 2.7E-05 , loss 0.471 , qa loss 0.471 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:48:48,268 - 2:52:14 - 61.6s - INFO - __main__ - epoch 6/10 done , tot steps 9624 , lr 2.5E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:50:43,492 - 2:54:09 - 115.2s - INFO - __main__ - progress 6.624 , lr 2.1E-05 , loss 0.455 , qa loss 0.455 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:51:46,192 - 2:55:12 - 62.7s - INFO - __main__ - epoch 7/10 done , tot steps 11228 , lr 1.9E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:53:41,330 - 2:57:07 - 115.1s - INFO - __main__ - progress 7.624 , lr 1.5E-05 , loss 0.423 , qa loss 0.423 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:54:45,587 - 2:58:12 - 64.3s - INFO - __main__ - epoch 8/10 done , tot steps 12832 , lr 1.3E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:56:40,051 - 3:00:06 - 114.5s - INFO - __main__ - progress 8.624 , lr 8.6E-06 , loss 0.407 , qa loss 0.407 , lm loss 0.000 , avg batch size 4.0
2023-07-31 02:57:43,798 - 3:01:10 - 63.7s - INFO - __main__ - epoch 9/10 done , tot steps 14436 , lr 6.3E-06 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-07-31 02:59:38,904 - 3:03:05 - 115.1s - INFO - __main__ - progress 9.624 , lr 2.4E-06 , loss 0.401 , qa loss 0.401 , lm loss 0.000 , avg batch size 4.0
2023-07-31 03:00:44,028 - 3:04:10 - 65.1s - INFO - __main__ - epoch 10/10 done , tot steps 16040 , lr 3.1E-08 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-07-31 03:00:45,206 - 3:04:11 - 1.2s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-07-31 03:00:45,206 - 3:04:11 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-31 03:00:45,365 - 3:04:11 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[3]
2023-07-31 03:00:59,413 - 3:04:25 - 14.0s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 25360
2023-07-31 03:02:11,315 - 3:05:37 - 71.9s - INFO - __main__ - epoch 1/10 done , tot steps 634 , lr 5.6E-05 , loss 4.90 , qa loss 4.90 , lm loss 0.00 , avg batch size 4.0
2023-07-31 03:03:20,674 - 3:06:47 - 69.4s - INFO - __main__ - epoch 2/10 done , tot steps 1268 , lr 5.0E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-07-31 03:04:30,621 - 3:07:57 - 69.9s - INFO - __main__ - epoch 3/10 done , tot steps 1902 , lr 4.4E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-07-31 03:05:40,418 - 3:09:06 - 69.8s - INFO - __main__ - epoch 4/10 done , tot steps 2536 , lr 3.8E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-07-31 03:06:50,808 - 3:10:17 - 70.4s - INFO - __main__ - epoch 5/10 done , tot steps 3170 , lr 3.1E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-07-31 03:07:58,549 - 3:11:24 - 67.7s - INFO - __main__ - epoch 6/10 done , tot steps 3804 , lr 2.5E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-07-31 03:09:07,289 - 3:12:33 - 68.7s - INFO - __main__ - epoch 7/10 done , tot steps 4438 , lr 1.9E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-07-31 03:10:15,876 - 3:13:42 - 68.6s - INFO - __main__ - epoch 8/10 done , tot steps 5072 , lr 1.3E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-07-31 03:11:24,748 - 3:14:51 - 68.9s - INFO - __main__ - epoch 9/10 done , tot steps 5706 , lr 6.3E-06 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-07-31 03:12:33,183 - 3:15:59 - 68.4s - INFO - __main__ - epoch 10/10 done , tot steps 6340 , lr 3.0E-08 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 03:15:53
CPU Execution time: 03:06:01
