................................................................................................................................
 Training Adapters at rf 4
................................................................................................................................
Available number of GPU = 4 < n_gpus = 12
Continue training with 4 GPUs
2023-08-19 22:46:55,152 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[7, 10, 12, 14], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='0,1,2,3,4,5,6,7,8,9,10', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[28835.84, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=4, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[10092, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[10092, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-19 22:46:55,153 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-19 22:46:55,153 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-19 22:46:57,936 - 0:00:07 - 2.8s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
The Flat  : True

[0]
2023-08-19 22:47:06,532 - 0:00:16 - 8.6s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck          591,744       0.476       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
2023-08-19 22:48:03,744 - 0:01:13 - 57.2s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 5.677 , qa loss 5.677 , lm loss 0.000 , avg batch size 4.0
2023-08-19 22:48:33,626 - 0:01:43 - 29.9s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 3.43 , qa loss 3.43 , lm loss 0.00 , avg batch size 4.0
2023-08-19 22:49:30,737 - 0:02:40 - 57.1s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.351 , qa loss 0.351 , lm loss 0.000 , avg batch size 4.0
2023-08-19 22:50:01,647 - 0:03:11 - 30.9s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-08-19 22:50:58,236 - 0:04:07 - 56.6s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.333 , qa loss 0.333 , lm loss 0.000 , avg batch size 4.0
2023-08-19 22:51:30,119 - 0:04:39 - 31.9s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-19 22:52:25,864 - 0:05:35 - 55.7s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.318 , qa loss 0.318 , lm loss 0.000 , avg batch size 4.0
2023-08-19 22:52:57,619 - 0:06:07 - 31.8s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-19 22:53:53,808 - 0:07:03 - 56.2s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.298 , qa loss 0.298 , lm loss 0.000 , avg batch size 4.0
2023-08-19 22:54:23,291 - 0:07:32 - 29.5s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-19 22:55:21,096 - 0:08:30 - 57.8s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.294 , qa loss 0.294 , lm loss 0.000 , avg batch size 4.0
2023-08-19 22:55:50,543 - 0:09:00 - 29.4s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-19 22:56:47,778 - 0:09:57 - 57.2s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.279 , qa loss 0.279 , lm loss 0.000 , avg batch size 4.0
2023-08-19 22:57:18,730 - 0:10:28 - 31.0s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-19 22:58:15,041 - 0:11:24 - 56.3s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.293 , qa loss 0.293 , lm loss 0.000 , avg batch size 4.0
2023-08-19 22:58:45,102 - 0:11:54 - 30.1s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-19 22:59:41,147 - 0:12:50 - 56.0s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.286 , qa loss 0.286 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:00:10,926 - 0:13:20 - 29.8s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:01:08,449 - 0:14:18 - 57.5s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.279 , qa loss 0.279 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:01:38,509 - 0:14:48 - 30.1s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:02:35,635 - 0:15:45 - 57.1s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.287 , qa loss 0.287 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:03:07,331 - 0:16:16 - 31.7s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:04:04,430 - 0:17:14 - 57.1s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.270 , qa loss 0.270 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:04:34,068 - 0:17:43 - 29.6s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:05:30,979 - 0:18:40 - 56.9s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.286 , qa loss 0.286 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:06:01,052 - 0:19:10 - 30.1s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:06:56,613 - 0:20:06 - 55.6s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.275 , qa loss 0.275 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:07:27,560 - 0:20:37 - 30.9s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:08:23,644 - 0:21:33 - 56.1s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.272 , qa loss 0.272 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:08:54,067 - 0:22:03 - 30.4s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:09:49,828 - 0:22:59 - 55.8s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.274 , qa loss 0.274 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:10:19,729 - 0:23:29 - 29.9s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:11:15,436 - 0:24:25 - 55.7s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.256 , qa loss 0.256 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:11:45,682 - 0:24:55 - 30.2s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:12:42,762 - 0:25:52 - 57.1s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.270 , qa loss 0.270 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:13:12,809 - 0:26:22 - 30.0s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:14:09,743 - 0:27:19 - 56.9s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.261 , qa loss 0.261 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:14:40,356 - 0:27:49 - 30.6s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:15:36,207 - 0:28:45 - 55.9s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.267 , qa loss 0.267 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:16:08,747 - 0:29:18 - 32.5s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:16:10,172 - 0:29:19 - 1.4s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-19 23:16:10,173 - 0:29:19 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-19 23:16:10,301 - 0:29:19 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck          591,744       0.476       1       1
srl                      bottleneck          591,744       0.476       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-19 23:16:16,918 - 0:29:26 - 6.6s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-19 23:17:39,674 - 0:30:49 - 82.8s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 7.790 , qa loss 7.790 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:18:23,049 - 0:31:32 - 43.4s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 5.64 , qa loss 5.64 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:19:45,287 - 0:32:54 - 82.2s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 1.989 , qa loss 1.989 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:20:27,718 - 0:33:37 - 42.4s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 1.98 , qa loss 1.98 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:21:50,107 - 0:34:59 - 82.4s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 1.888 , qa loss 1.888 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:22:34,617 - 0:35:44 - 44.5s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 1.88 , qa loss 1.88 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:23:59,873 - 0:37:09 - 85.3s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 1.863 , qa loss 1.863 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:24:41,905 - 0:37:51 - 42.0s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 1.84 , qa loss 1.84 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:26:06,509 - 0:39:16 - 84.6s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 1.792 , qa loss 1.792 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:26:47,290 - 0:39:56 - 40.8s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 1.79 , qa loss 1.79 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:28:09,530 - 0:41:19 - 82.2s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 1.777 , qa loss 1.777 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:28:53,298 - 0:42:02 - 43.8s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 1.77 , qa loss 1.77 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:30:17,131 - 0:43:26 - 83.8s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 1.747 , qa loss 1.747 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:30:58,930 - 0:44:08 - 41.8s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 1.75 , qa loss 1.75 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:32:21,032 - 0:45:30 - 82.1s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 1.725 , qa loss 1.725 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:33:03,599 - 0:46:13 - 42.6s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 1.74 , qa loss 1.74 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:34:26,917 - 0:47:36 - 83.3s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 1.732 , qa loss 1.732 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:35:09,738 - 0:48:19 - 42.8s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 1.72 , qa loss 1.72 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:36:34,188 - 0:49:43 - 84.5s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 1.695 , qa loss 1.695 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:37:15,907 - 0:50:25 - 41.7s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 1.69 , qa loss 1.69 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:38:39,463 - 0:51:49 - 83.6s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 1.677 , qa loss 1.677 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:39:20,319 - 0:52:29 - 40.9s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 1.68 , qa loss 1.68 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:40:43,806 - 0:53:53 - 83.5s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 1.671 , qa loss 1.671 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:41:24,685 - 0:54:34 - 40.9s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 1.68 , qa loss 1.68 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:42:47,354 - 0:55:56 - 82.7s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 1.678 , qa loss 1.678 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:43:30,475 - 0:56:40 - 43.1s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 1.67 , qa loss 1.67 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:44:53,909 - 0:58:03 - 83.4s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 1.645 , qa loss 1.645 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:45:35,655 - 0:58:45 - 41.7s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 1.66 , qa loss 1.66 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:46:56,699 - 1:00:06 - 81.0s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 1.657 , qa loss 1.657 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:47:38,819 - 1:00:48 - 42.1s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 1.66 , qa loss 1.66 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:49:01,832 - 1:02:11 - 83.0s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 1.630 , qa loss 1.630 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:49:44,044 - 1:02:53 - 42.2s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 1.65 , qa loss 1.65 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:51:07,161 - 1:04:16 - 83.1s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 1.644 , qa loss 1.644 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:51:47,462 - 1:04:57 - 40.3s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 1.64 , qa loss 1.64 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:53:11,658 - 1:06:21 - 84.2s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 1.624 , qa loss 1.624 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:53:51,723 - 1:07:01 - 40.1s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 1.63 , qa loss 1.63 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:55:14,407 - 1:08:24 - 82.7s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 1.636 , qa loss 1.636 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:55:57,761 - 1:09:07 - 43.4s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 1.64 , qa loss 1.64 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:57:19,900 - 1:10:29 - 82.1s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 1.616 , qa loss 1.616 , lm loss 0.000 , avg batch size 4.0
2023-08-19 23:58:03,053 - 1:11:12 - 43.2s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 1.62 , qa loss 1.62 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:58:05,764 - 1:11:15 - 2.7s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-19 23:58:05,765 - 1:11:15 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-19 23:58:05,888 - 1:11:15 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck          591,744       0.476       0       0
srl                      bottleneck          591,744       0.476       1       1
woz_en                   bottleneck          591,744       0.476       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-19 23:58:11,717 - 1:11:21 - 5.8s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-19 23:58:55,085 - 1:12:04 - 43.4s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 10.03 , qa loss 10.03 , lm loss 0.00 , avg batch size 4.0
2023-08-19 23:59:37,541 - 1:12:47 - 42.5s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 1.70 , qa loss 1.70 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:00:19,618 - 1:13:29 - 42.1s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 1.10 , qa loss 1.10 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:01:02,051 - 1:14:11 - 42.4s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.89 , qa loss 0.89 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:01:43,881 - 1:14:53 - 41.8s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.79 , qa loss 0.79 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:02:26,684 - 1:15:36 - 42.8s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.72 , qa loss 0.72 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:03:08,891 - 1:16:18 - 42.2s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.67 , qa loss 0.67 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:03:50,826 - 1:17:00 - 41.9s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:04:33,155 - 1:17:42 - 42.3s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:05:15,357 - 1:18:24 - 42.2s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:05:59,490 - 1:19:09 - 44.1s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.60 , qa loss 0.60 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:06:41,121 - 1:19:50 - 41.6s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:07:22,291 - 1:20:31 - 41.2s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:08:04,699 - 1:21:14 - 42.4s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:08:46,402 - 1:21:56 - 41.7s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:09:28,077 - 1:22:37 - 41.7s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:10:11,447 - 1:23:21 - 43.4s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:10:54,313 - 1:24:03 - 42.9s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:11:38,085 - 1:24:47 - 43.8s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:12:20,864 - 1:25:30 - 42.8s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:25:27
CPU Execution time: 01:11:03
................................................................................................................................
Testing Adapters at rf 4
................................................................................................................................
2023-08-20 00:12:29,540 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[7], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 00:12:43,151 - 0:00:19 - 13.6s - INFO - __main__ - task: sst, epoch: 20
2023-08-20 00:12:43,152 - 0:00:19 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-20 00:12:46,878 - 0:00:23 - 3.7s - INFO - __main__ - len of test dataset: 1821
2023-08-20 00:12:57,825 - 0:00:34 - 10.9s - INFO - __main__ - score: {'sst': OrderedDict([('em', 77.48489840746842), ('nf1', 77.48489840746842), ('nem', 77.48489840746842)]), 'srl': None, 'woz.en': None}
2023-08-20 00:13:08,872 - 0:00:45 - 11.0s - INFO - __main__ - task: srl, epoch: 20
2023-08-20 00:13:08,872 - 0:00:45 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-20 00:13:12,476 - 0:00:48 - 3.6s - INFO - __main__ - len of test dataset: 2201
2023-08-20 00:34:22,672 - 0:21:58 - 1270.2s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 9.67741935483871), ('nf1', 22.510260308780204), ('nem', 11.358473421172194)]), 'woz.en': None}
2023-08-20 00:34:33,836 - 0:22:10 - 11.2s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-20 00:34:33,836 - 0:22:10 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-20 00:34:38,078 - 0:22:14 - 4.2s - INFO - __main__ - len of test dataset: 1646
2023-08-20 00:46:26,165 - 0:34:02 - 708.1s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 12.393681652490887), ('nf1', 81.46982354758772), ('nem', 61.05710814094775), ('joint_goal_em', 29.52612393681652), ('turn_request_em', 81.10571081409478), ('turn_goal_em', 68.77278250303766), ('avg_dialogue', 55.31591737545565)])}
................................................................................................................................
 Training Adapters at rf 4
................................................................................................................................
Available number of GPU = 4 < n_gpus = 12
Continue training with 4 GPUs
2023-08-20 00:46:34,615 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[7, 10, 12, 14], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='0,1,2,3,4,5,6,7', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[28835.84, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=4, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[10092, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[10092, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 00:46:34,616 - 0:00:06 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-20 00:46:34,616 - 0:00:06 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 00:46:37,902 - 0:00:09 - 3.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [0, 1, 2, 3, 4, 5, 6, 7]
The Flat  : True

[0]
2023-08-20 00:46:48,373 - 0:00:20 - 10.5s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        2,366,976       1.902       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
2023-08-20 00:47:56,054 - 0:01:27 - 67.7s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 3.307 , qa loss 3.307 , lm loss 0.000 , avg batch size 4.0
2023-08-20 00:48:31,324 - 0:02:03 - 35.3s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 2.03 , qa loss 2.03 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:49:35,538 - 0:03:07 - 64.2s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.229 , qa loss 0.229 , lm loss 0.000 , avg batch size 4.0
2023-08-20 00:50:09,821 - 0:03:41 - 34.3s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:51:11,195 - 0:04:42 - 61.4s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.216 , qa loss 0.216 , lm loss 0.000 , avg batch size 4.0
2023-08-20 00:51:46,190 - 0:05:17 - 35.0s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:52:48,320 - 0:06:20 - 62.1s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.193 , qa loss 0.193 , lm loss 0.000 , avg batch size 4.0
2023-08-20 00:53:22,449 - 0:06:54 - 34.1s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:54:27,501 - 0:07:59 - 65.1s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.189 , qa loss 0.189 , lm loss 0.000 , avg batch size 4.0
2023-08-20 00:55:02,644 - 0:08:34 - 35.1s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:56:06,158 - 0:09:37 - 63.5s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.173 , qa loss 0.173 , lm loss 0.000 , avg batch size 4.0
2023-08-20 00:56:40,568 - 0:10:12 - 34.4s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:57:43,751 - 0:11:15 - 63.2s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.160 , qa loss 0.160 , lm loss 0.000 , avg batch size 4.0
2023-08-20 00:58:18,677 - 0:11:50 - 34.9s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 00:59:20,765 - 0:12:52 - 62.1s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.160 , qa loss 0.160 , lm loss 0.000 , avg batch size 4.0
2023-08-20 00:59:55,551 - 0:13:27 - 34.8s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:00:57,459 - 0:14:29 - 61.9s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.154 , qa loss 0.154 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:01:32,019 - 0:15:03 - 34.6s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:02:34,968 - 0:16:06 - 62.9s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.145 , qa loss 0.145 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:03:09,694 - 0:16:41 - 34.7s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:04:11,131 - 0:17:42 - 61.4s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.140 , qa loss 0.140 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:04:45,569 - 0:18:17 - 34.4s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:05:48,329 - 0:19:20 - 62.8s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.144 , qa loss 0.144 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:06:23,129 - 0:19:54 - 34.8s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:07:23,577 - 0:20:55 - 60.4s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.132 , qa loss 0.132 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:07:58,711 - 0:21:30 - 35.1s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:09:01,585 - 0:22:33 - 62.9s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.133 , qa loss 0.133 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:09:35,975 - 0:23:07 - 34.4s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:10:37,472 - 0:24:09 - 61.5s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.122 , qa loss 0.122 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:11:12,668 - 0:24:44 - 35.2s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:12:16,805 - 0:25:48 - 64.1s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.117 , qa loss 0.117 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:12:51,019 - 0:26:22 - 34.2s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:13:53,299 - 0:27:25 - 62.3s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.124 , qa loss 0.124 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:14:28,782 - 0:28:00 - 35.5s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:15:30,686 - 0:29:02 - 61.9s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.118 , qa loss 0.118 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:16:05,046 - 0:29:36 - 34.4s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:17:07,526 - 0:30:39 - 62.5s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.110 , qa loss 0.110 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:17:42,259 - 0:31:13 - 34.7s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:18:44,238 - 0:32:15 - 62.0s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.116 , qa loss 0.116 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:19:19,528 - 0:32:51 - 35.3s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:19:20,626 - 0:32:52 - 1.1s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-20 01:19:20,627 - 0:32:52 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 01:19:20,747 - 0:32:52 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        2,366,976       1.902       1       1
srl                      bottleneck        2,366,976       1.902       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 01:19:29,371 - 0:33:01 - 8.6s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-20 01:21:03,139 - 0:34:34 - 93.8s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 4.841 , qa loss 4.841 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:21:47,917 - 0:35:19 - 44.8s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 3.65 , qa loss 3.65 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:23:20,134 - 0:36:51 - 92.2s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 1.532 , qa loss 1.532 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:24:04,738 - 0:37:36 - 44.6s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 1.50 , qa loss 1.50 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:25:38,429 - 0:39:10 - 93.7s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 1.361 , qa loss 1.361 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:26:24,462 - 0:39:56 - 46.0s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 1.35 , qa loss 1.35 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:27:56,853 - 0:41:28 - 92.4s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 1.288 , qa loss 1.288 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:28:43,671 - 0:42:15 - 46.8s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 1.27 , qa loss 1.27 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:30:14,210 - 0:43:45 - 90.5s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 1.238 , qa loss 1.238 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:31:00,465 - 0:44:32 - 46.3s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 1.21 , qa loss 1.21 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:32:32,815 - 0:46:04 - 92.3s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 1.166 , qa loss 1.166 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:33:17,677 - 0:46:49 - 44.9s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 1.17 , qa loss 1.17 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:34:48,593 - 0:48:20 - 90.9s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 1.123 , qa loss 1.123 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:35:35,110 - 0:49:06 - 46.5s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 1.11 , qa loss 1.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:37:05,880 - 0:50:37 - 90.8s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 1.069 , qa loss 1.069 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:37:52,385 - 0:51:24 - 46.5s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 1.07 , qa loss 1.07 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:39:24,086 - 0:52:55 - 91.7s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 1.056 , qa loss 1.056 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:40:10,738 - 0:53:42 - 46.7s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 1.06 , qa loss 1.06 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:41:40,486 - 0:55:12 - 89.7s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 1.012 , qa loss 1.012 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:42:28,017 - 0:55:59 - 47.5s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 1.03 , qa loss 1.03 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:43:58,537 - 0:57:30 - 90.5s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 1.010 , qa loss 1.010 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:44:46,145 - 0:58:17 - 47.6s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 1.00 , qa loss 1.00 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:46:14,348 - 0:59:46 - 88.2s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.974 , qa loss 0.974 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:47:01,675 - 1:00:33 - 47.3s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.98 , qa loss 0.98 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:48:31,597 - 1:02:03 - 89.9s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.976 , qa loss 0.976 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:49:19,142 - 1:02:50 - 47.5s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.98 , qa loss 0.98 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:50:50,445 - 1:04:22 - 91.3s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.948 , qa loss 0.948 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:51:35,468 - 1:05:07 - 45.0s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.95 , qa loss 0.95 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:53:06,024 - 1:06:37 - 90.6s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.943 , qa loss 0.943 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:53:52,164 - 1:07:23 - 46.1s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.93 , qa loss 0.93 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:55:22,303 - 1:08:54 - 90.1s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.905 , qa loss 0.905 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:56:09,939 - 1:09:41 - 47.6s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.91 , qa loss 0.91 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:57:40,927 - 1:11:12 - 91.0s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.909 , qa loss 0.909 , lm loss 0.000 , avg batch size 4.0
2023-08-20 01:58:27,033 - 1:11:58 - 46.1s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.90 , qa loss 0.90 , lm loss 0.00 , avg batch size 4.0
2023-08-20 01:59:59,755 - 1:13:31 - 92.7s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.919 , qa loss 0.919 , lm loss 0.000 , avg batch size 4.0
2023-08-20 02:00:45,047 - 1:14:16 - 45.3s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.91 , qa loss 0.91 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:02:14,350 - 1:15:46 - 89.3s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.899 , qa loss 0.899 , lm loss 0.000 , avg batch size 4.0
2023-08-20 02:03:00,845 - 1:16:32 - 46.5s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.90 , qa loss 0.90 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:04:31,021 - 1:18:02 - 90.2s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.877 , qa loss 0.877 , lm loss 0.000 , avg batch size 4.0
2023-08-20 02:05:19,010 - 1:18:50 - 48.0s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.88 , qa loss 0.88 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:05:22,089 - 1:18:53 - 3.1s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-20 02:05:22,089 - 1:18:53 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 02:05:22,225 - 1:18:53 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        2,366,976       1.902       0       0
srl                      bottleneck        2,366,976       1.902       1       1
woz_en                   bottleneck        2,366,976       1.902       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 02:05:30,183 - 1:19:01 - 8.0s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-20 02:06:18,778 - 1:19:50 - 48.6s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 6.32 , qa loss 6.32 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:07:06,840 - 1:20:38 - 48.1s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:07:54,606 - 1:21:26 - 47.8s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:08:42,048 - 1:22:13 - 47.4s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:09:30,057 - 1:23:01 - 48.0s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:10:17,100 - 1:23:48 - 47.0s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:11:04,938 - 1:24:36 - 47.8s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:11:52,786 - 1:25:24 - 47.8s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:12:40,070 - 1:26:11 - 47.3s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:13:27,703 - 1:26:59 - 47.6s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:14:15,031 - 1:27:46 - 47.3s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:15:02,080 - 1:28:33 - 47.0s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:15:50,143 - 1:29:21 - 48.1s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:16:38,186 - 1:30:09 - 48.0s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:17:26,246 - 1:30:57 - 48.1s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:18:13,867 - 1:31:45 - 47.6s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:19:02,986 - 1:32:34 - 49.1s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:19:50,163 - 1:33:21 - 47.2s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:20:38,109 - 1:34:09 - 47.9s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:21:27,821 - 1:34:59 - 49.7s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:34:57
CPU Execution time: 01:21:55
................................................................................................................................
Testing Adapters at rf 4
................................................................................................................................
2023-08-20 02:21:39,416 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[7], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 02:21:53,636 - 0:00:20 - 14.2s - INFO - __main__ - task: sst, epoch: 20
2023-08-20 02:21:53,636 - 0:00:20 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-20 02:21:57,859 - 0:00:24 - 4.2s - INFO - __main__ - len of test dataset: 1821
2023-08-20 02:22:09,854 - 0:00:36 - 12.0s - INFO - __main__ - score: {'sst': OrderedDict([('em', 88.46787479406919), ('nf1', 88.46787479406919), ('nem', 88.46787479406919)]), 'srl': None, 'woz.en': None}
2023-08-20 02:22:21,675 - 0:00:48 - 11.8s - INFO - __main__ - task: srl, epoch: 20
2023-08-20 02:22:21,676 - 0:00:48 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-20 02:22:26,097 - 0:00:52 - 4.4s - INFO - __main__ - len of test dataset: 2201
2023-08-20 02:42:30,220 - 0:20:56 - 1204.1s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 27.214902317128576), ('nf1', 45.63870617010715), ('nem', 30.349840981372108)]), 'woz.en': None}
2023-08-20 02:42:42,754 - 0:21:09 - 12.5s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-20 02:42:42,755 - 0:21:09 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-20 02:42:47,103 - 0:21:13 - 4.3s - INFO - __main__ - len of test dataset: 1646
2023-08-20 02:54:04,818 - 0:32:31 - 677.7s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 16.281895504252734), ('nf1', 92.83026376890297), ('nem', 83.77885783718104), ('joint_goal_em', 78.79708383961118), ('turn_request_em', 91.31227217496962), ('turn_goal_em', 88.63912515188336), ('avg_dialogue', 85.05467800729039)])}
................................................................................................................................
 Training Adapters at rf 4
................................................................................................................................
Available number of GPU = 4 < n_gpus = 12
Continue training with 4 GPUs
2023-08-20 02:54:13,426 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[7, 10, 12, 14], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='0,1,2,3,4,5', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[28835.84, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=4, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[10092, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[10092, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 02:54:13,426 - 0:00:06 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-20 02:54:13,426 - 0:00:06 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 02:54:16,272 - 0:00:09 - 2.8s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [0, 1, 2, 3, 4, 5]
The Flat  : True

[0]
2023-08-20 02:54:27,240 - 0:00:20 - 11.0s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
2023-08-20 02:55:38,050 - 0:01:31 - 70.8s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.949 , qa loss 2.949 , lm loss 0.000 , avg batch size 4.0
2023-08-20 02:56:15,054 - 0:02:08 - 37.0s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.80 , qa loss 1.80 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:57:22,562 - 0:03:15 - 67.5s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.233 , qa loss 0.233 , lm loss 0.000 , avg batch size 4.0
2023-08-20 02:57:59,393 - 0:03:52 - 36.8s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-20 02:59:05,741 - 0:04:58 - 66.3s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.193 , qa loss 0.193 , lm loss 0.000 , avg batch size 4.0
2023-08-20 02:59:43,246 - 0:05:36 - 37.5s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:00:49,484 - 0:06:42 - 66.2s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.179 , qa loss 0.179 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:01:26,545 - 0:07:19 - 37.1s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:02:32,966 - 0:08:26 - 66.4s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.169 , qa loss 0.169 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:03:10,740 - 0:09:03 - 37.8s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:04:17,084 - 0:10:10 - 66.3s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.159 , qa loss 0.159 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:04:54,187 - 0:10:47 - 37.1s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:05:59,314 - 0:11:52 - 65.1s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.144 , qa loss 0.144 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:06:36,265 - 0:12:29 - 37.0s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:07:41,777 - 0:13:34 - 65.5s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.138 , qa loss 0.138 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:08:19,149 - 0:14:12 - 37.4s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:09:25,398 - 0:15:18 - 66.2s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.141 , qa loss 0.141 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:10:02,860 - 0:15:56 - 37.5s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:11:08,047 - 0:17:01 - 65.2s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.128 , qa loss 0.128 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:11:45,000 - 0:17:38 - 37.0s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:12:51,002 - 0:18:44 - 66.0s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.117 , qa loss 0.117 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:13:28,127 - 0:19:21 - 37.1s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:14:34,885 - 0:20:28 - 66.8s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.114 , qa loss 0.114 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:15:11,499 - 0:21:04 - 36.6s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:16:16,783 - 0:22:09 - 65.3s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.102 , qa loss 0.102 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:16:53,372 - 0:22:46 - 36.6s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:17:58,259 - 0:23:51 - 64.9s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.110 , qa loss 0.110 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:18:34,645 - 0:24:27 - 36.4s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:19:41,566 - 0:25:34 - 66.9s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.099 , qa loss 0.099 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:20:18,884 - 0:26:12 - 37.3s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:21:25,872 - 0:27:19 - 67.0s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.102 , qa loss 0.102 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:22:02,726 - 0:27:55 - 36.9s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:23:09,581 - 0:29:02 - 66.9s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.103 , qa loss 0.103 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:23:46,222 - 0:29:39 - 36.6s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:24:52,401 - 0:30:45 - 66.2s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.093 , qa loss 0.093 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:25:29,542 - 0:31:22 - 37.1s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:26:36,693 - 0:32:29 - 67.2s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.102 , qa loss 0.102 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:27:13,340 - 0:33:06 - 36.6s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:28:17,693 - 0:34:10 - 64.4s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.103 , qa loss 0.103 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:28:56,363 - 0:34:49 - 38.7s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:28:59,182 - 0:34:52 - 2.8s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-20 03:28:59,182 - 0:34:52 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 03:28:59,303 - 0:34:52 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        3,550,464       2.853       1       1
srl                      bottleneck        3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 03:29:07,927 - 0:35:01 - 8.6s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-20 03:30:45,427 - 0:36:38 - 97.5s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 4.349 , qa loss 4.349 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:31:34,894 - 0:37:28 - 49.5s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 3.23 , qa loss 3.23 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:33:09,738 - 0:39:02 - 94.8s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 1.195 , qa loss 1.195 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:33:58,041 - 0:39:51 - 48.3s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 1.16 , qa loss 1.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:35:34,265 - 0:41:27 - 96.2s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 1.057 , qa loss 1.057 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:36:24,182 - 0:42:17 - 49.9s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 1.04 , qa loss 1.04 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:37:59,417 - 0:43:52 - 95.2s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.968 , qa loss 0.968 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:38:49,438 - 0:44:42 - 50.0s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.96 , qa loss 0.96 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:40:24,067 - 0:46:17 - 94.6s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.891 , qa loss 0.891 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:41:13,805 - 0:47:06 - 49.7s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.88 , qa loss 0.88 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:42:50,678 - 0:48:43 - 96.9s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.841 , qa loss 0.841 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:43:39,342 - 0:49:32 - 48.7s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.84 , qa loss 0.84 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:45:14,035 - 0:51:07 - 94.7s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.817 , qa loss 0.817 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:46:02,550 - 0:51:55 - 48.5s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.82 , qa loss 0.82 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:47:38,446 - 0:53:31 - 95.9s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.784 , qa loss 0.784 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:48:27,920 - 0:54:21 - 49.5s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.78 , qa loss 0.78 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:50:01,814 - 0:55:54 - 93.9s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.762 , qa loss 0.762 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:50:51,116 - 0:56:44 - 49.3s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.75 , qa loss 0.75 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:52:26,863 - 0:58:20 - 95.7s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.720 , qa loss 0.720 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:53:15,713 - 0:59:08 - 48.9s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.73 , qa loss 0.73 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:54:51,754 - 1:00:44 - 96.0s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.688 , qa loss 0.688 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:55:40,393 - 1:01:33 - 48.6s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.70 , qa loss 0.70 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:57:16,762 - 1:03:09 - 96.4s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.679 , qa loss 0.679 , lm loss 0.000 , avg batch size 4.0
2023-08-20 03:58:05,959 - 1:03:59 - 49.2s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.68 , qa loss 0.68 , lm loss 0.00 , avg batch size 4.0
2023-08-20 03:59:41,684 - 1:05:34 - 95.7s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.664 , qa loss 0.664 , lm loss 0.000 , avg batch size 4.0
2023-08-20 04:00:30,523 - 1:06:23 - 48.8s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.66 , qa loss 0.66 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:02:06,359 - 1:07:59 - 95.8s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.650 , qa loss 0.650 , lm loss 0.000 , avg batch size 4.0
2023-08-20 04:02:55,747 - 1:08:48 - 49.4s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:04:34,052 - 1:10:27 - 98.3s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.637 , qa loss 0.637 , lm loss 0.000 , avg batch size 4.0
2023-08-20 04:05:20,129 - 1:11:13 - 46.1s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:06:55,312 - 1:12:48 - 95.2s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.613 , qa loss 0.613 , lm loss 0.000 , avg batch size 4.0
2023-08-20 04:07:44,512 - 1:13:37 - 49.2s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:09:19,777 - 1:15:12 - 95.3s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.610 , qa loss 0.610 , lm loss 0.000 , avg batch size 4.0
2023-08-20 04:10:07,833 - 1:16:01 - 48.1s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.62 , qa loss 0.62 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:11:43,315 - 1:17:36 - 95.5s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.594 , qa loss 0.594 , lm loss 0.000 , avg batch size 4.0
2023-08-20 04:12:32,139 - 1:18:25 - 48.8s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:14:07,338 - 1:20:00 - 95.2s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.582 , qa loss 0.582 , lm loss 0.000 , avg batch size 4.0
2023-08-20 04:14:56,271 - 1:20:49 - 48.9s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:16:31,190 - 1:22:24 - 94.9s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.585 , qa loss 0.585 , lm loss 0.000 , avg batch size 4.0
2023-08-20 04:17:23,414 - 1:23:16 - 52.2s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:17:25,380 - 1:23:18 - 2.0s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-20 04:17:25,381 - 1:23:18 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 04:17:25,514 - 1:23:18 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        3,550,464       2.853       0       0
srl                      bottleneck        3,550,464       2.853       1       1
woz_en                   bottleneck        3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 04:17:40,147 - 1:23:33 - 14.6s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-20 04:18:31,299 - 1:24:24 - 51.2s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 5.23 , qa loss 5.23 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:19:21,866 - 1:25:15 - 50.6s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:20:12,262 - 1:26:05 - 50.4s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:21:03,013 - 1:26:56 - 50.8s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:21:53,461 - 1:27:46 - 50.4s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:22:44,661 - 1:28:37 - 51.2s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:23:34,028 - 1:29:27 - 49.4s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:24:24,727 - 1:30:17 - 50.7s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:25:15,401 - 1:31:08 - 50.7s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:26:07,430 - 1:32:00 - 52.0s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:26:57,481 - 1:32:50 - 50.1s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:27:47,260 - 1:33:40 - 49.8s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:28:38,657 - 1:34:31 - 51.4s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:29:29,242 - 1:35:22 - 50.6s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:30:20,167 - 1:36:13 - 50.9s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:31:11,394 - 1:37:04 - 51.2s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:32:00,972 - 1:37:54 - 49.6s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:32:51,451 - 1:38:44 - 50.5s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:33:43,445 - 1:39:36 - 52.0s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 04:34:35,393 - 1:40:28 - 51.9s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:40:26
CPU Execution time: 01:27:48
................................................................................................................................
Testing Adapters at rf 4
................................................................................................................................
2023-08-20 04:34:47,786 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[7], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 04:35:02,078 - 0:00:20 - 14.3s - INFO - __main__ - task: sst, epoch: 20
2023-08-20 04:35:02,079 - 0:00:20 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-20 04:35:05,984 - 0:00:24 - 3.9s - INFO - __main__ - len of test dataset: 1821
2023-08-20 04:35:16,725 - 0:00:35 - 10.7s - INFO - __main__ - score: {'sst': OrderedDict([('em', 89.62108731466228), ('nf1', 89.62108731466228), ('nem', 89.62108731466228)]), 'srl': None, 'woz.en': None}
2023-08-20 04:35:28,493 - 0:00:47 - 11.8s - INFO - __main__ - task: srl, epoch: 20
2023-08-20 04:35:28,494 - 0:00:47 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-20 04:35:33,100 - 0:00:51 - 4.6s - INFO - __main__ - len of test dataset: 2201
2023-08-20 04:58:13,901 - 0:23:32 - 1360.8s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 36.39254884143571), ('nf1', 57.293809413734266), ('nem', 41.39027714675147)]), 'woz.en': None}
2023-08-20 04:58:25,466 - 0:23:44 - 11.6s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-20 04:58:25,466 - 0:23:44 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-20 04:58:29,888 - 0:23:48 - 4.4s - INFO - __main__ - len of test dataset: 1646
2023-08-20 05:09:34,008 - 0:34:52 - 664.1s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 16.646415552855405), ('nf1', 94.14242029065846), ('nem', 86.2089914945322), ('joint_goal_em', 85.47995139732684), ('turn_request_em', 91.79829890643985), ('turn_goal_em', 91.06925880923451), ('avg_dialogue', 88.63912515188335)])}
................................................................................................................................
 Training Adapters at rf 4
................................................................................................................................
Available number of GPU = 4 < n_gpus = 12
Continue training with 4 GPUs
2023-08-20 05:09:41,531 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[7, 10, 12, 14], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='0,1,2,3', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[28835.84, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=4, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[10092, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[10092, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 05:09:41,531 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-20 05:09:41,531 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 05:09:44,636 - 0:00:08 - 3.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [0, 1, 2, 3]
The Flat  : True

[0]
2023-08-20 05:09:55,765 - 0:00:20 - 11.1s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        4,733,952       3.804       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
2023-08-20 05:11:11,682 - 0:01:35 - 75.9s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.760 , qa loss 2.760 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:11:51,193 - 0:02:15 - 39.5s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.70 , qa loss 1.70 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:13:01,411 - 0:03:25 - 70.2s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.213 , qa loss 0.213 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:13:41,833 - 0:04:06 - 40.4s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:14:51,761 - 0:05:16 - 69.9s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.182 , qa loss 0.182 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:15:32,699 - 0:05:57 - 40.9s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:16:43,762 - 0:07:08 - 71.1s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.167 , qa loss 0.167 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:17:23,762 - 0:07:48 - 40.0s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:18:34,083 - 0:08:58 - 70.3s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.151 , qa loss 0.151 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:19:13,981 - 0:09:38 - 39.9s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:20:24,060 - 0:10:48 - 70.1s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.145 , qa loss 0.145 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:21:05,756 - 0:11:30 - 41.7s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:22:16,327 - 0:12:40 - 70.6s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.132 , qa loss 0.132 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:22:57,016 - 0:13:21 - 40.7s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:24:07,021 - 0:14:31 - 70.0s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.124 , qa loss 0.124 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:24:47,557 - 0:15:11 - 40.5s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:25:57,841 - 0:16:22 - 70.3s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.118 , qa loss 0.118 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:26:39,136 - 0:17:03 - 41.3s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:27:48,925 - 0:18:13 - 69.8s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.101 , qa loss 0.101 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:28:29,352 - 0:18:53 - 40.4s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:29:39,804 - 0:20:04 - 70.5s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.094 , qa loss 0.094 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:30:19,542 - 0:20:43 - 39.7s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:31:30,986 - 0:21:55 - 71.4s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.093 , qa loss 0.093 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:32:11,816 - 0:22:36 - 40.8s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:33:22,030 - 0:23:46 - 70.2s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.088 , qa loss 0.088 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:34:02,247 - 0:24:26 - 40.2s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:35:12,086 - 0:25:36 - 69.8s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.082 , qa loss 0.082 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:35:51,890 - 0:26:16 - 39.8s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:37:03,384 - 0:27:27 - 71.5s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.080 , qa loss 0.080 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:37:44,027 - 0:28:08 - 40.6s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:38:54,609 - 0:29:18 - 70.6s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.072 , qa loss 0.072 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:39:34,766 - 0:29:59 - 40.2s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:40:43,905 - 0:31:08 - 69.1s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.073 , qa loss 0.073 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:41:24,421 - 0:31:48 - 40.5s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:42:35,262 - 0:32:59 - 70.8s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.069 , qa loss 0.069 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:43:15,594 - 0:33:39 - 40.3s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:44:26,577 - 0:34:50 - 71.0s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.070 , qa loss 0.070 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:45:06,670 - 0:35:30 - 40.1s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:46:15,702 - 0:36:40 - 69.0s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.062 , qa loss 0.062 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:46:58,513 - 0:37:22 - 42.8s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:47:00,652 - 0:37:24 - 2.1s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-20 05:47:00,653 - 0:37:24 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 05:47:00,775 - 0:37:25 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        4,733,952       3.804       1       1
srl                      bottleneck        4,733,952       3.804       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 05:47:09,718 - 0:37:34 - 8.9s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-20 05:48:52,038 - 0:39:16 - 102.3s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 4.161 , qa loss 4.161 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:49:45,578 - 0:40:09 - 53.5s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 3.03 , qa loss 3.03 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:51:27,675 - 0:41:51 - 102.1s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 1.008 , qa loss 1.008 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:52:19,599 - 0:42:43 - 51.9s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.98 , qa loss 0.98 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:54:02,958 - 0:44:27 - 103.4s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.847 , qa loss 0.847 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:54:55,305 - 0:45:19 - 52.3s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.83 , qa loss 0.83 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:56:35,794 - 0:47:00 - 100.5s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.752 , qa loss 0.752 , lm loss 0.000 , avg batch size 4.0
2023-08-20 05:57:29,658 - 0:47:53 - 53.9s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.74 , qa loss 0.74 , lm loss 0.00 , avg batch size 4.0
2023-08-20 05:59:12,550 - 0:49:36 - 102.9s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.698 , qa loss 0.698 , lm loss 0.000 , avg batch size 4.0
2023-08-20 06:00:06,236 - 0:50:30 - 53.7s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.70 , qa loss 0.70 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:01:47,306 - 0:52:11 - 101.1s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.641 , qa loss 0.641 , lm loss 0.000 , avg batch size 4.0
2023-08-20 06:02:39,784 - 0:53:04 - 52.5s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:04:22,844 - 0:54:47 - 103.1s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.605 , qa loss 0.605 , lm loss 0.000 , avg batch size 4.0
2023-08-20 06:05:14,328 - 0:55:38 - 51.5s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:06:55,287 - 0:57:19 - 101.0s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.591 , qa loss 0.591 , lm loss 0.000 , avg batch size 4.0
2023-08-20 06:07:46,222 - 0:58:10 - 50.9s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:09:29,674 - 0:59:53 - 103.5s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.543 , qa loss 0.543 , lm loss 0.000 , avg batch size 4.0
2023-08-20 06:10:21,531 - 1:00:45 - 51.9s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:12:01,022 - 1:02:25 - 99.5s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.515 , qa loss 0.515 , lm loss 0.000 , avg batch size 4.0
2023-08-20 06:12:54,324 - 1:03:18 - 53.3s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:14:37,914 - 1:05:02 - 103.6s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.493 , qa loss 0.493 , lm loss 0.000 , avg batch size 4.0
2023-08-20 06:15:30,222 - 1:05:54 - 52.3s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:17:10,906 - 1:07:35 - 100.7s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.493 , qa loss 0.493 , lm loss 0.000 , avg batch size 4.0
2023-08-20 06:18:04,692 - 1:08:29 - 53.8s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:19:44,818 - 1:10:09 - 100.1s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.480 , qa loss 0.480 , lm loss 0.000 , avg batch size 4.0
2023-08-20 06:20:37,402 - 1:11:01 - 52.6s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:22:17,252 - 1:12:41 - 99.9s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.453 , qa loss 0.453 , lm loss 0.000 , avg batch size 4.0
2023-08-20 06:23:10,390 - 1:13:34 - 53.1s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:24:50,976 - 1:15:15 - 100.6s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.435 , qa loss 0.435 , lm loss 0.000 , avg batch size 4.0
2023-08-20 06:25:45,917 - 1:16:10 - 54.9s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:27:27,934 - 1:17:52 - 102.0s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.421 , qa loss 0.421 , lm loss 0.000 , avg batch size 4.0
2023-08-20 06:28:20,186 - 1:18:44 - 52.3s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:30:00,488 - 1:20:24 - 100.3s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.423 , qa loss 0.423 , lm loss 0.000 , avg batch size 4.0
2023-08-20 06:30:55,120 - 1:21:19 - 54.6s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:32:36,072 - 1:23:00 - 101.0s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.402 , qa loss 0.402 , lm loss 0.000 , avg batch size 4.0
2023-08-20 06:33:28,075 - 1:23:52 - 52.0s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:35:11,318 - 1:25:35 - 103.2s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.391 , qa loss 0.391 , lm loss 0.000 , avg batch size 4.0
2023-08-20 06:36:05,289 - 1:26:29 - 54.0s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:37:48,704 - 1:28:13 - 103.4s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.402 , qa loss 0.402 , lm loss 0.000 , avg batch size 4.0
2023-08-20 06:38:42,118 - 1:29:06 - 53.4s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:38:45,015 - 1:29:09 - 2.9s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-20 06:38:45,016 - 1:29:09 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 06:38:45,137 - 1:29:09 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        4,733,952       3.804       0       0
srl                      bottleneck        4,733,952       3.804       1       1
woz_en                   bottleneck        4,733,952       3.804       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 06:39:00,740 - 1:29:25 - 15.6s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-20 06:39:57,361 - 1:30:21 - 56.6s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 4.79 , qa loss 4.79 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:40:54,011 - 1:31:18 - 56.6s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:41:50,483 - 1:32:14 - 56.5s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:42:45,038 - 1:33:09 - 54.6s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:43:40,166 - 1:34:04 - 55.1s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:44:35,187 - 1:34:59 - 55.0s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:45:30,121 - 1:35:54 - 54.9s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:46:25,841 - 1:36:50 - 55.7s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:47:21,510 - 1:37:45 - 55.7s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:48:16,667 - 1:38:40 - 55.2s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:49:11,769 - 1:39:36 - 55.1s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:50:06,422 - 1:40:30 - 54.7s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:51:01,601 - 1:41:25 - 55.2s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:51:57,857 - 1:42:22 - 56.3s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:52:54,383 - 1:43:18 - 56.5s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:53:49,570 - 1:44:13 - 55.2s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:54:44,438 - 1:45:08 - 54.9s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:55:38,595 - 1:46:02 - 54.2s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:56:35,978 - 1:47:00 - 57.4s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-20 06:57:36,603 - 1:48:00 - 60.6s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:47:57
CPU Execution time: 01:34:57
................................................................................................................................
Testing Adapters at rf 4
................................................................................................................................
2023-08-20 06:57:46,509 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[7], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 06:58:00,783 - 0:00:20 - 14.3s - INFO - __main__ - task: sst, epoch: 20
2023-08-20 06:58:00,784 - 0:00:20 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-20 06:58:04,814 - 0:00:24 - 4.0s - INFO - __main__ - len of test dataset: 1821
2023-08-20 06:58:15,591 - 0:00:35 - 10.8s - INFO - __main__ - score: {'sst': OrderedDict([('em', 90.93904448105437), ('nf1', 90.93904448105437), ('nem', 90.93904448105437)]), 'srl': None, 'woz.en': None}
2023-08-20 06:58:27,395 - 0:00:47 - 11.8s - INFO - __main__ - task: srl, epoch: 20
2023-08-20 06:58:27,397 - 0:00:47 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-20 06:58:31,880 - 0:00:51 - 4.5s - INFO - __main__ - len of test dataset: 2201
2023-08-20 07:21:24,099 - 0:23:43 - 1372.2s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 42.935029532030896), ('nf1', 64.15411241330276), ('nem', 48.38709677419355)]), 'woz.en': None}
2023-08-20 07:21:36,225 - 0:23:55 - 12.1s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-20 07:21:36,225 - 0:23:55 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-20 07:21:40,578 - 0:24:00 - 4.4s - INFO - __main__ - len of test dataset: 1646
2023-08-20 07:33:15,717 - 0:35:35 - 695.1s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 16.767922235722963), ('nf1', 94.49684614022406), ('nem', 87.24179829890643), ('joint_goal_em', 86.39125151883353), ('turn_request_em', 92.52733900364521), ('turn_goal_em', 91.79829890643985), ('avg_dialogue', 89.45929526123936)])}
................................................................................................................................
 Training Adapters at rf 4
................................................................................................................................
Available number of GPU = 5 < n_gpus = 12
Continue training with 5 GPUs
2023-08-20 07:33:22,519 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[1, 7, 10, 12, 14], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='4,5,6,7,8,9,10,11', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[27525.12, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=5, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[9633, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[9633, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 07:33:22,520 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-20 07:33:22,520 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 07:33:25,551 - 0:00:08 - 3.0s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [4, 5, 6, 7, 8, 9, 10, 11]
The Flat  : True

[0]
2023-08-20 07:33:36,326 - 0:00:18 - 10.8s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        2,366,976       1.902       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
2023-08-20 07:34:46,613 - 0:01:29 - 70.3s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.847 , qa loss 2.847 , lm loss 0.000 , avg batch size 4.0
2023-08-20 07:35:23,989 - 0:02:06 - 37.4s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.77 , qa loss 1.77 , lm loss 0.00 , avg batch size 4.0
2023-08-20 07:36:30,746 - 0:03:13 - 66.8s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.273 , qa loss 0.273 , lm loss 0.000 , avg batch size 4.0
2023-08-20 07:37:08,390 - 0:03:51 - 37.6s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-20 07:38:15,566 - 0:04:58 - 67.2s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.219 , qa loss 0.219 , lm loss 0.000 , avg batch size 4.0
2023-08-20 07:38:52,883 - 0:05:35 - 37.3s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-20 07:39:59,040 - 0:06:41 - 66.2s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.183 , qa loss 0.183 , lm loss 0.000 , avg batch size 4.0
2023-08-20 07:40:36,694 - 0:07:19 - 37.7s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-20 07:41:43,018 - 0:08:25 - 66.3s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.169 , qa loss 0.169 , lm loss 0.000 , avg batch size 4.0
2023-08-20 07:42:20,336 - 0:09:02 - 37.3s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 07:43:27,664 - 0:10:10 - 67.3s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.167 , qa loss 0.167 , lm loss 0.000 , avg batch size 4.0
2023-08-20 07:44:05,002 - 0:10:47 - 37.3s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-20 07:45:11,052 - 0:11:53 - 66.0s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.150 , qa loss 0.150 , lm loss 0.000 , avg batch size 4.0
2023-08-20 07:45:48,208 - 0:12:30 - 37.2s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 07:46:54,366 - 0:13:37 - 66.2s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.146 , qa loss 0.146 , lm loss 0.000 , avg batch size 4.0
2023-08-20 07:47:30,931 - 0:14:13 - 36.6s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 07:48:38,209 - 0:15:20 - 67.3s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.130 , qa loss 0.130 , lm loss 0.000 , avg batch size 4.0
2023-08-20 07:49:15,667 - 0:15:58 - 37.5s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 07:50:22,396 - 0:17:05 - 66.7s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.126 , qa loss 0.126 , lm loss 0.000 , avg batch size 4.0
2023-08-20 07:50:59,430 - 0:17:42 - 37.0s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 07:52:05,924 - 0:18:48 - 66.5s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.119 , qa loss 0.119 , lm loss 0.000 , avg batch size 4.0
2023-08-20 07:52:43,242 - 0:19:25 - 37.3s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 07:53:50,233 - 0:20:32 - 67.0s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.107 , qa loss 0.107 , lm loss 0.000 , avg batch size 4.0
2023-08-20 07:54:28,462 - 0:21:11 - 38.2s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 07:55:34,226 - 0:22:16 - 65.8s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.097 , qa loss 0.097 , lm loss 0.000 , avg batch size 4.0
2023-08-20 07:56:11,662 - 0:22:54 - 37.4s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 07:57:17,770 - 0:24:00 - 66.1s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.099 , qa loss 0.099 , lm loss 0.000 , avg batch size 4.0
2023-08-20 07:57:54,906 - 0:24:37 - 37.1s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-20 07:59:01,957 - 0:25:44 - 67.1s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.098 , qa loss 0.098 , lm loss 0.000 , avg batch size 4.0
2023-08-20 07:59:40,026 - 0:26:22 - 38.1s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:00:45,422 - 0:27:28 - 65.4s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.077 , qa loss 0.077 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:01:22,814 - 0:28:05 - 37.4s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:02:30,088 - 0:29:12 - 67.3s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.093 , qa loss 0.093 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:03:07,666 - 0:29:50 - 37.6s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:04:13,554 - 0:30:56 - 65.9s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.082 , qa loss 0.082 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:04:51,913 - 0:31:34 - 38.4s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:05:57,768 - 0:32:40 - 65.9s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.079 , qa loss 0.079 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:06:36,061 - 0:33:18 - 38.3s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:07:42,042 - 0:34:24 - 66.0s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.080 , qa loss 0.080 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:08:21,691 - 0:35:04 - 39.6s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:08:25,328 - 0:35:07 - 3.6s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-20 08:08:25,329 - 0:35:07 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 08:08:25,454 - 0:35:08 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        2,366,976       1.902       1       1
srl                      bottleneck        2,366,976       1.902       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 08:08:32,846 - 0:35:15 - 7.4s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-20 08:10:10,394 - 0:36:53 - 97.5s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 5.298 , qa loss 5.298 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:11:00,018 - 0:37:42 - 49.6s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 3.79 , qa loss 3.79 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:12:35,489 - 0:39:18 - 95.5s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.986 , qa loss 0.986 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:13:25,244 - 0:40:07 - 49.8s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.95 , qa loss 0.95 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:15:00,617 - 0:41:43 - 95.4s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.834 , qa loss 0.834 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:15:50,515 - 0:42:33 - 49.9s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.84 , qa loss 0.84 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:17:26,112 - 0:44:08 - 95.6s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.774 , qa loss 0.774 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:18:15,419 - 0:44:58 - 49.3s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.76 , qa loss 0.76 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:19:53,200 - 0:46:35 - 97.8s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.712 , qa loss 0.712 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:20:41,397 - 0:47:24 - 48.2s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.71 , qa loss 0.71 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:22:18,443 - 0:49:01 - 97.0s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.682 , qa loss 0.682 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:23:08,195 - 0:49:50 - 49.8s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.66 , qa loss 0.66 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:24:43,852 - 0:51:26 - 95.7s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.653 , qa loss 0.653 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:25:35,002 - 0:52:17 - 51.1s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:27:12,055 - 0:53:54 - 97.1s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.596 , qa loss 0.596 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:28:00,631 - 0:54:43 - 48.6s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.60 , qa loss 0.60 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:29:36,089 - 0:56:18 - 95.5s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.578 , qa loss 0.578 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:30:26,904 - 0:57:09 - 50.8s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.57 , qa loss 0.57 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:32:03,243 - 0:58:45 - 96.3s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.567 , qa loss 0.567 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:32:52,464 - 0:59:35 - 49.2s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:34:29,252 - 1:01:11 - 96.8s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.545 , qa loss 0.545 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:35:18,534 - 1:02:01 - 49.3s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:36:55,160 - 1:03:37 - 96.6s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.522 , qa loss 0.522 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:37:44,451 - 1:04:27 - 49.3s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:39:19,722 - 1:06:02 - 95.3s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.507 , qa loss 0.507 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:40:08,482 - 1:06:51 - 48.8s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:41:43,818 - 1:08:26 - 95.3s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.489 , qa loss 0.489 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:42:33,455 - 1:09:16 - 49.6s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:44:07,539 - 1:10:50 - 94.1s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.494 , qa loss 0.494 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:44:57,136 - 1:11:39 - 49.6s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:46:35,342 - 1:13:17 - 98.2s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.491 , qa loss 0.491 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:47:23,756 - 1:14:06 - 48.4s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:48:58,169 - 1:15:40 - 94.4s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.482 , qa loss 0.482 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:49:48,432 - 1:16:31 - 50.3s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:51:23,802 - 1:18:06 - 95.4s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.447 , qa loss 0.447 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:52:13,405 - 1:18:56 - 49.6s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:53:48,143 - 1:20:30 - 94.7s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.442 , qa loss 0.442 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:54:38,241 - 1:21:20 - 50.1s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:56:15,835 - 1:22:58 - 97.6s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.463 , qa loss 0.463 , lm loss 0.000 , avg batch size 4.0
2023-08-20 08:57:07,722 - 1:23:50 - 51.9s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:57:08,939 - 1:23:51 - 1.2s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-20 08:57:08,940 - 1:23:51 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 08:57:09,065 - 1:23:51 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        2,366,976       1.902       0       0
srl                      bottleneck        2,366,976       1.902       1       1
woz_en                   bottleneck        2,366,976       1.902       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 08:57:15,973 - 1:23:58 - 6.9s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-20 08:58:07,779 - 1:24:50 - 51.8s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 5.68 , qa loss 5.68 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:58:58,535 - 1:25:41 - 50.8s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.70 , qa loss 0.70 , lm loss 0.00 , avg batch size 4.0
2023-08-20 08:59:48,807 - 1:26:31 - 50.3s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:00:39,513 - 1:27:22 - 50.7s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:01:30,201 - 1:28:12 - 50.7s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:02:21,592 - 1:29:04 - 51.4s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:03:11,758 - 1:29:54 - 50.2s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:04:01,406 - 1:30:44 - 49.6s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:04:51,818 - 1:31:34 - 50.4s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:05:41,175 - 1:32:23 - 49.4s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:06:30,432 - 1:33:13 - 49.3s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:07:21,583 - 1:34:04 - 51.2s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:08:12,094 - 1:34:54 - 50.5s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:09:01,834 - 1:35:44 - 49.7s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:09:51,281 - 1:36:33 - 49.4s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:10:40,956 - 1:37:23 - 49.7s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:11:31,367 - 1:38:14 - 50.4s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:12:21,171 - 1:39:03 - 49.8s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:13:11,802 - 1:39:54 - 50.6s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:14:03,876 - 1:40:46 - 52.1s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:40:44
CPU Execution time: 01:26:40
................................................................................................................................
Testing Adapters at rf 4
................................................................................................................................
2023-08-20 09:14:13,928 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[1], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 09:14:27,176 - 0:00:19 - 13.2s - INFO - __main__ - task: sst, epoch: 20
2023-08-20 09:14:27,177 - 0:00:19 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-20 09:14:30,899 - 0:00:22 - 3.7s - INFO - __main__ - len of test dataset: 1821
2023-08-20 09:14:41,019 - 0:00:32 - 10.1s - INFO - __main__ - score: {'sst': OrderedDict([('em', 89.73091707852828), ('nf1', 89.73091707852828), ('nem', 89.73091707852828)]), 'srl': None, 'woz.en': None}
2023-08-20 09:14:52,485 - 0:00:44 - 11.5s - INFO - __main__ - task: srl, epoch: 20
2023-08-20 09:14:52,486 - 0:00:44 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-20 09:14:56,836 - 0:00:48 - 4.3s - INFO - __main__ - len of test dataset: 2201
2023-08-20 09:38:03,761 - 0:23:55 - 1386.9s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 47.97819173103135), ('nf1', 66.73636428044829), ('nem', 52.56701499318491)]), 'woz.en': None}
2023-08-20 09:38:15,389 - 0:24:07 - 11.6s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-20 09:38:15,389 - 0:24:07 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-20 09:38:19,769 - 0:24:11 - 4.4s - INFO - __main__ - len of test dataset: 1646
2023-08-20 09:49:54,005 - 0:35:45 - 694.2s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 16.403402187120292), ('nf1', 92.78570120307664), ('nem', 84.02187120291616), ('joint_goal_em', 81.34872417982989), ('turn_request_em', 91.25151883353585), ('turn_goal_em', 88.94289185905225), ('avg_dialogue', 86.30012150668287)])}
................................................................................................................................
 Training Adapters at rf 4
................................................................................................................................
Available number of GPU = 5 < n_gpus = 12
Continue training with 5 GPUs
2023-08-20 09:50:00,983 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[1, 7, 10, 12, 14], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='6,7,8,9,10,11', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[27525.12, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=5, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[9633, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[9633, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 09:50:00,983 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-20 09:50:00,983 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 09:50:03,846 - 0:00:08 - 2.9s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [6, 7, 8, 9, 10, 11]
The Flat  : True

[0]
2023-08-20 09:50:15,242 - 0:00:19 - 11.4s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
2023-08-20 09:51:30,456 - 0:01:34 - 75.2s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.832 , qa loss 2.832 , lm loss 0.000 , avg batch size 4.0
2023-08-20 09:52:10,080 - 0:02:14 - 39.6s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.76 , qa loss 1.76 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:53:19,182 - 0:03:23 - 69.1s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.220 , qa loss 0.220 , lm loss 0.000 , avg batch size 4.0
2023-08-20 09:53:59,494 - 0:04:03 - 40.3s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:55:09,275 - 0:05:13 - 69.8s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.193 , qa loss 0.193 , lm loss 0.000 , avg batch size 4.0
2023-08-20 09:55:48,679 - 0:05:53 - 39.4s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:56:57,725 - 0:07:02 - 69.0s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.170 , qa loss 0.170 , lm loss 0.000 , avg batch size 4.0
2023-08-20 09:57:37,887 - 0:07:42 - 40.2s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-20 09:58:47,762 - 0:08:52 - 69.9s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.152 , qa loss 0.152 , lm loss 0.000 , avg batch size 4.0
2023-08-20 09:59:27,872 - 0:09:32 - 40.1s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:00:39,081 - 0:10:43 - 71.2s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.140 , qa loss 0.140 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:01:19,162 - 0:11:23 - 40.1s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:02:28,521 - 0:12:32 - 69.4s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.125 , qa loss 0.125 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:03:07,662 - 0:13:12 - 39.1s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:04:16,521 - 0:14:20 - 68.9s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.125 , qa loss 0.125 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:04:55,887 - 0:15:00 - 39.4s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:06:05,185 - 0:16:09 - 69.3s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.108 , qa loss 0.108 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:06:46,032 - 0:16:50 - 40.8s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:07:53,698 - 0:17:58 - 67.7s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.108 , qa loss 0.108 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:08:33,368 - 0:18:37 - 39.7s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:09:42,346 - 0:19:46 - 69.0s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.082 , qa loss 0.082 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:10:21,776 - 0:20:26 - 39.4s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:11:30,680 - 0:21:35 - 68.9s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.082 , qa loss 0.082 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:12:11,061 - 0:22:15 - 40.4s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:13:20,913 - 0:23:25 - 69.9s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.079 , qa loss 0.079 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:14:00,505 - 0:24:04 - 39.6s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:15:09,624 - 0:25:14 - 69.1s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.081 , qa loss 0.081 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:15:49,141 - 0:25:53 - 39.5s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:16:57,794 - 0:27:02 - 68.7s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.076 , qa loss 0.076 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:17:37,274 - 0:27:41 - 39.5s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:18:46,110 - 0:28:50 - 68.8s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.061 , qa loss 0.061 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:19:26,463 - 0:29:30 - 40.4s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:20:34,053 - 0:30:38 - 67.6s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.062 , qa loss 0.062 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:21:13,387 - 0:31:17 - 39.3s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:22:22,000 - 0:32:26 - 68.6s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.044 , qa loss 0.044 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:23:02,264 - 0:33:06 - 40.3s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:24:10,764 - 0:34:15 - 68.5s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.056 , qa loss 0.056 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:24:50,439 - 0:34:54 - 39.7s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:26:00,535 - 0:36:04 - 70.1s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.054 , qa loss 0.054 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:26:42,829 - 0:36:47 - 42.3s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:26:45,150 - 0:36:49 - 2.3s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-20 10:26:45,150 - 0:36:49 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 10:26:45,272 - 0:36:49 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        3,550,464       2.853       1       1
srl                      bottleneck        3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 10:26:54,961 - 0:36:59 - 9.7s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-20 10:28:35,488 - 0:38:39 - 100.5s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 4.722 , qa loss 4.722 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:29:28,444 - 0:39:32 - 53.0s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 3.33 , qa loss 3.33 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:31:05,393 - 0:41:09 - 96.9s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.898 , qa loss 0.898 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:31:58,527 - 0:42:02 - 53.1s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.86 , qa loss 0.86 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:33:36,010 - 0:43:40 - 97.5s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.734 , qa loss 0.734 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:34:29,989 - 0:44:34 - 54.0s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.72 , qa loss 0.72 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:36:08,420 - 0:46:12 - 98.4s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.664 , qa loss 0.664 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:37:00,310 - 0:47:04 - 51.9s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.66 , qa loss 0.66 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:38:41,729 - 0:48:46 - 101.4s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.623 , qa loss 0.623 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:39:46,571 - 0:49:50 - 64.8s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:41:39,112 - 0:51:43 - 112.5s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.583 , qa loss 0.583 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:42:41,850 - 0:52:46 - 62.7s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.57 , qa loss 0.57 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:44:39,330 - 0:54:43 - 117.5s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.548 , qa loss 0.548 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:45:45,156 - 0:55:49 - 65.8s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:47:46,515 - 0:57:50 - 121.4s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.520 , qa loss 0.520 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:48:49,170 - 0:58:53 - 62.7s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:50:38,427 - 1:00:42 - 109.3s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.479 , qa loss 0.479 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:51:30,544 - 1:01:34 - 52.1s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:53:32,727 - 1:03:37 - 122.2s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.478 , qa loss 0.478 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:54:59,070 - 1:05:03 - 86.3s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:56:39,247 - 1:06:43 - 100.2s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.450 , qa loss 0.450 , lm loss 0.000 , avg batch size 4.0
2023-08-20 10:57:31,012 - 1:07:35 - 51.8s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-08-20 10:59:10,585 - 1:09:14 - 99.6s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.431 , qa loss 0.431 , lm loss 0.000 , avg batch size 4.0
2023-08-20 11:00:02,376 - 1:10:06 - 51.8s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:01:42,054 - 1:11:46 - 99.7s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.425 , qa loss 0.425 , lm loss 0.000 , avg batch size 4.0
2023-08-20 11:02:33,281 - 1:12:37 - 51.2s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:04:12,606 - 1:14:16 - 99.3s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.421 , qa loss 0.421 , lm loss 0.000 , avg batch size 4.0
2023-08-20 11:05:04,765 - 1:15:09 - 52.2s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:06:44,899 - 1:16:49 - 100.1s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.393 , qa loss 0.393 , lm loss 0.000 , avg batch size 4.0
2023-08-20 11:07:36,038 - 1:17:40 - 51.1s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:09:17,365 - 1:19:21 - 101.3s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.382 , qa loss 0.382 , lm loss 0.000 , avg batch size 4.0
2023-08-20 11:10:07,788 - 1:20:12 - 50.4s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:11:46,127 - 1:21:50 - 98.3s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.378 , qa loss 0.378 , lm loss 0.000 , avg batch size 4.0
2023-08-20 11:12:39,124 - 1:22:43 - 53.0s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:14:18,932 - 1:24:23 - 99.8s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.372 , qa loss 0.372 , lm loss 0.000 , avg batch size 4.0
2023-08-20 11:15:09,319 - 1:25:13 - 50.4s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:16:48,816 - 1:26:53 - 99.5s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.359 , qa loss 0.359 , lm loss 0.000 , avg batch size 4.0
2023-08-20 11:17:40,312 - 1:27:44 - 51.5s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:19:18,930 - 1:29:23 - 98.6s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.364 , qa loss 0.364 , lm loss 0.000 , avg batch size 4.0
2023-08-20 11:20:11,626 - 1:30:16 - 52.7s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:20:12,908 - 1:30:17 - 1.3s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-20 11:20:12,909 - 1:30:17 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 11:20:13,035 - 1:30:17 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        3,550,464       2.853       0       0
srl                      bottleneck        3,550,464       2.853       1       1
woz_en                   bottleneck        3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 11:20:21,606 - 1:30:25 - 8.6s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-20 11:21:15,657 - 1:31:20 - 54.1s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 5.17 , qa loss 5.17 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:22:09,344 - 1:32:13 - 53.7s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:23:03,124 - 1:33:07 - 53.8s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:23:57,293 - 1:34:01 - 54.2s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:24:52,185 - 1:34:56 - 54.9s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:25:47,226 - 1:35:51 - 55.0s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:26:41,468 - 1:36:45 - 54.2s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:27:34,520 - 1:37:38 - 53.1s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:28:28,281 - 1:38:32 - 53.8s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:29:21,621 - 1:39:26 - 53.3s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:30:15,041 - 1:40:19 - 53.4s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:31:08,518 - 1:41:12 - 53.5s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:32:03,610 - 1:42:07 - 55.1s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:32:57,324 - 1:43:01 - 53.7s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:33:50,655 - 1:43:55 - 53.3s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:34:44,466 - 1:44:48 - 53.8s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:35:38,062 - 1:45:42 - 53.6s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:36:32,295 - 1:46:36 - 54.2s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:37:26,347 - 1:47:30 - 54.1s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 11:38:23,131 - 1:48:27 - 56.8s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:48:25
CPU Execution time: 01:34:26
................................................................................................................................
Testing Adapters at rf 4
................................................................................................................................
2023-08-20 11:38:33,853 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[1], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 11:38:47,408 - 0:00:19 - 13.6s - INFO - __main__ - task: sst, epoch: 20
2023-08-20 11:38:47,410 - 0:00:19 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-20 11:38:51,638 - 0:00:23 - 4.2s - INFO - __main__ - len of test dataset: 1821
2023-08-20 11:39:01,787 - 0:00:33 - 10.1s - INFO - __main__ - score: {'sst': OrderedDict([('em', 89.40142778693026), ('nf1', 89.40142778693026), ('nem', 89.40142778693026)]), 'srl': None, 'woz.en': None}
2023-08-20 11:39:14,272 - 0:00:46 - 12.5s - INFO - __main__ - task: srl, epoch: 20
2023-08-20 11:39:14,273 - 0:00:46 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-20 11:39:18,663 - 0:00:50 - 4.4s - INFO - __main__ - len of test dataset: 2201
2023-08-20 12:01:05,356 - 0:22:37 - 1306.7s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 50.34075420263516), ('nf1', 69.13634442616747), ('nem', 55.565651976374376)]), 'woz.en': None}
2023-08-20 12:01:16,188 - 0:22:48 - 10.8s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-20 12:01:16,188 - 0:22:48 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-20 12:01:20,456 - 0:22:52 - 4.3s - INFO - __main__ - len of test dataset: 1646
2023-08-20 12:12:21,600 - 0:33:53 - 661.1s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 16.52490886998785), ('nf1', 93.26538218093503), ('nem', 84.38639125151883), ('joint_goal_em', 80.86269744835965), ('turn_request_em', 91.49453219927096), ('turn_goal_em', 89.30741190765492), ('avg_dialogue', 86.17861482381531)])}
................................................................................................................................
 Training Adapters at rf 4
................................................................................................................................
Available number of GPU = 5 < n_gpus = 12
Continue training with 5 GPUs
2023-08-20 12:12:28,583 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[1, 7, 10, 12, 14], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='8,9,10,11', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[27525.12, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=5, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[9633, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[9633, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 12:12:28,583 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-20 12:12:28,583 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 12:12:31,034 - 0:00:07 - 2.5s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [8, 9, 10, 11]
The Flat  : True

[0]
2023-08-20 12:12:42,854 - 0:00:19 - 11.8s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        4,733,952       3.804       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
2023-08-20 12:14:02,123 - 0:01:38 - 79.3s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.698 , qa loss 2.698 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:14:47,769 - 0:02:24 - 45.6s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.67 , qa loss 1.67 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:16:04,248 - 0:03:40 - 76.5s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.222 , qa loss 0.222 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:16:49,605 - 0:04:26 - 45.4s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:18:06,608 - 0:05:43 - 77.0s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.190 , qa loss 0.190 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:18:52,342 - 0:06:29 - 45.7s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:20:09,668 - 0:07:46 - 77.3s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.161 , qa loss 0.161 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:20:54,784 - 0:08:31 - 45.1s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:22:10,447 - 0:09:47 - 75.7s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.150 , qa loss 0.150 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:22:56,966 - 0:10:33 - 46.5s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:24:14,362 - 0:11:51 - 77.4s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.138 , qa loss 0.138 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:24:59,395 - 0:12:36 - 45.0s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:26:15,427 - 0:13:52 - 76.0s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.116 , qa loss 0.116 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:27:00,295 - 0:14:37 - 44.9s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:28:16,561 - 0:15:53 - 76.3s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.105 , qa loss 0.105 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:28:58,820 - 0:16:35 - 42.3s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:30:11,180 - 0:17:47 - 72.4s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.091 , qa loss 0.091 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:30:52,807 - 0:18:29 - 41.6s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:32:04,614 - 0:19:41 - 71.8s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.085 , qa loss 0.085 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:32:45,724 - 0:20:22 - 41.1s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:33:57,612 - 0:21:34 - 71.9s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.087 , qa loss 0.087 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:34:38,923 - 0:22:15 - 41.3s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:35:51,978 - 0:23:28 - 73.1s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.071 , qa loss 0.071 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:36:32,902 - 0:24:09 - 40.9s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:37:44,121 - 0:25:20 - 71.2s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.065 , qa loss 0.065 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:38:25,258 - 0:26:01 - 41.1s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:39:36,592 - 0:27:13 - 71.3s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.066 , qa loss 0.066 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:40:17,920 - 0:27:54 - 41.3s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:41:29,876 - 0:29:06 - 72.0s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.052 , qa loss 0.052 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:42:11,157 - 0:29:47 - 41.3s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:43:23,381 - 0:31:00 - 72.2s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.050 , qa loss 0.050 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:44:05,031 - 0:31:41 - 41.6s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:45:16,372 - 0:32:53 - 71.3s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.048 , qa loss 0.048 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:45:57,464 - 0:33:34 - 41.1s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:47:08,866 - 0:34:45 - 71.4s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.042 , qa loss 0.042 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:47:50,395 - 0:35:27 - 41.5s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:49:02,167 - 0:36:38 - 71.8s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.033 , qa loss 0.033 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:49:44,543 - 0:37:21 - 42.4s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:50:55,610 - 0:38:32 - 71.1s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.046 , qa loss 0.046 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:51:39,119 - 0:39:15 - 43.5s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:51:42,233 - 0:39:18 - 3.1s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-20 12:51:42,234 - 0:39:18 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 12:51:42,354 - 0:39:19 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        4,733,952       3.804       1       1
srl                      bottleneck        4,733,952       3.804       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 12:51:52,061 - 0:39:28 - 9.7s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-20 12:53:37,326 - 0:41:14 - 105.3s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 4.321 , qa loss 4.321 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:54:30,520 - 0:42:07 - 53.2s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 3.05 , qa loss 3.05 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:56:14,682 - 0:43:51 - 104.2s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.815 , qa loss 0.815 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:57:08,799 - 0:44:45 - 54.1s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.79 , qa loss 0.79 , lm loss 0.00 , avg batch size 4.0
2023-08-20 12:58:51,970 - 0:46:28 - 103.2s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.681 , qa loss 0.681 , lm loss 0.000 , avg batch size 4.0
2023-08-20 12:59:46,432 - 0:47:23 - 54.5s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.66 , qa loss 0.66 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:01:28,315 - 0:49:05 - 101.9s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.605 , qa loss 0.605 , lm loss 0.000 , avg batch size 4.0
2023-08-20 13:02:22,672 - 0:49:59 - 54.4s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.60 , qa loss 0.60 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:04:07,881 - 0:51:44 - 105.2s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.540 , qa loss 0.540 , lm loss 0.000 , avg batch size 4.0
2023-08-20 13:05:01,213 - 0:52:37 - 53.3s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:06:44,356 - 0:54:21 - 103.1s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.512 , qa loss 0.512 , lm loss 0.000 , avg batch size 4.0
2023-08-20 13:07:39,425 - 0:55:16 - 55.1s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:09:24,925 - 0:57:01 - 105.5s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.466 , qa loss 0.466 , lm loss 0.000 , avg batch size 4.0
2023-08-20 13:10:18,334 - 0:57:55 - 53.4s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:12:02,975 - 0:59:39 - 104.6s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.452 , qa loss 0.452 , lm loss 0.000 , avg batch size 4.0
2023-08-20 13:12:55,706 - 1:00:32 - 52.7s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:14:38,514 - 1:02:15 - 102.8s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.413 , qa loss 0.413 , lm loss 0.000 , avg batch size 4.0
2023-08-20 13:15:33,604 - 1:03:10 - 55.1s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:17:18,901 - 1:04:55 - 105.3s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.391 , qa loss 0.391 , lm loss 0.000 , avg batch size 4.0
2023-08-20 13:18:13,994 - 1:05:50 - 55.1s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:19:55,711 - 1:07:32 - 101.7s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.391 , qa loss 0.391 , lm loss 0.000 , avg batch size 4.0
2023-08-20 13:20:52,092 - 1:08:28 - 56.4s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:22:35,547 - 1:10:12 - 103.5s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.367 , qa loss 0.367 , lm loss 0.000 , avg batch size 4.0
2023-08-20 13:23:30,440 - 1:11:07 - 54.9s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:25:14,373 - 1:12:51 - 103.9s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.354 , qa loss 0.354 , lm loss 0.000 , avg batch size 4.0
2023-08-20 13:26:08,098 - 1:13:44 - 53.7s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:27:51,299 - 1:15:28 - 103.2s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.340 , qa loss 0.340 , lm loss 0.000 , avg batch size 4.0
2023-08-20 13:28:45,074 - 1:16:21 - 53.8s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:30:30,106 - 1:18:06 - 105.0s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.334 , qa loss 0.334 , lm loss 0.000 , avg batch size 4.0
2023-08-20 13:31:23,409 - 1:19:00 - 53.3s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:33:06,865 - 1:20:43 - 103.5s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.317 , qa loss 0.317 , lm loss 0.000 , avg batch size 4.0
2023-08-20 13:34:00,166 - 1:21:36 - 53.3s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:35:45,993 - 1:23:22 - 105.8s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.309 , qa loss 0.309 , lm loss 0.000 , avg batch size 4.0
2023-08-20 13:36:38,393 - 1:24:15 - 52.4s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:38:22,129 - 1:25:58 - 103.7s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.300 , qa loss 0.300 , lm loss 0.000 , avg batch size 4.0
2023-08-20 13:39:14,597 - 1:26:51 - 52.5s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:40:55,197 - 1:28:31 - 100.6s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.287 , qa loss 0.287 , lm loss 0.000 , avg batch size 4.0
2023-08-20 13:41:51,712 - 1:29:28 - 56.5s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:43:35,226 - 1:31:11 - 103.5s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.291 , qa loss 0.291 , lm loss 0.000 , avg batch size 4.0
2023-08-20 13:44:31,208 - 1:32:07 - 56.0s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:44:34,577 - 1:32:11 - 3.4s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-20 13:44:34,578 - 1:32:11 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 13:44:34,716 - 1:32:11 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        4,733,952       3.804       0       0
srl                      bottleneck        4,733,952       3.804       1       1
woz_en                   bottleneck        4,733,952       3.804       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 13:44:43,544 - 1:32:20 - 8.8s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-20 13:45:41,203 - 1:33:17 - 57.7s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 5.51 , qa loss 5.51 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:46:39,130 - 1:34:15 - 57.9s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.55 , qa loss 0.55 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:47:36,782 - 1:35:13 - 57.7s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:48:33,837 - 1:36:10 - 57.1s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:49:33,155 - 1:37:09 - 59.3s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:50:34,918 - 1:38:11 - 61.8s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:51:32,711 - 1:39:09 - 57.8s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:52:33,068 - 1:40:09 - 60.4s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:53:30,705 - 1:41:07 - 57.6s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:54:29,122 - 1:42:05 - 58.4s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:55:28,867 - 1:43:05 - 59.7s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:56:27,150 - 1:44:03 - 58.3s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:57:26,234 - 1:45:02 - 59.1s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:58:24,339 - 1:46:01 - 58.1s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 13:59:21,691 - 1:46:58 - 57.4s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 14:00:19,751 - 1:47:56 - 58.1s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 14:01:18,417 - 1:48:55 - 58.7s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 14:02:17,030 - 1:49:53 - 58.6s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 14:03:17,805 - 1:50:54 - 60.8s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 14:04:17,150 - 1:51:53 - 59.3s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:51:52
CPU Execution time: 01:39:17
................................................................................................................................
Testing Adapters at rf 4
................................................................................................................................
2023-08-20 14:04:29,775 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[1], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 14:04:44,311 - 0:00:20 - 14.5s - INFO - __main__ - task: sst, epoch: 20
2023-08-20 14:04:44,311 - 0:00:20 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-20 14:04:48,693 - 0:00:25 - 4.4s - INFO - __main__ - len of test dataset: 1821
2023-08-20 14:04:59,841 - 0:00:36 - 11.1s - INFO - __main__ - score: {'sst': OrderedDict([('em', 90.55464030752334), ('nf1', 90.55464030752334), ('nem', 90.55464030752334)]), 'srl': None, 'woz.en': None}
2023-08-20 14:05:11,752 - 0:00:48 - 11.9s - INFO - __main__ - task: srl, epoch: 20
2023-08-20 14:05:11,753 - 0:00:48 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-20 14:05:16,562 - 0:00:52 - 4.8s - INFO - __main__ - len of test dataset: 2201
2023-08-20 14:30:34,590 - 0:26:11 - 1518.0s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 50.43162199000454), ('nf1', 69.54342365034135), ('nem', 55.8836892321672)]), 'woz.en': None}
2023-08-20 14:30:45,762 - 0:26:22 - 11.2s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-20 14:30:45,762 - 0:26:22 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-20 14:30:50,521 - 0:26:26 - 4.8s - INFO - __main__ - len of test dataset: 1646
2023-08-20 14:42:22,727 - 0:37:59 - 692.2s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 16.767922235722963), ('nf1', 93.52706865161298), ('nem', 85.29769137302551), ('joint_goal_em', 82.01701093560145), ('turn_request_em', 91.49453219927096), ('turn_goal_em', 89.91494532199272), ('avg_dialogue', 86.7557715674362)])}
................................................................................................................................
 Training Adapters at rf 4
................................................................................................................................
Available number of GPU = 5 < n_gpus = 12
Continue training with 5 GPUs
2023-08-20 14:42:30,984 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[1, 7, 10, 12, 14], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='0,2,4,6,8,10', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[27525.12, 34078.72, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=5, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[9633, 11927, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[9633, 11927, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 14:42:30,984 - 0:00:06 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-20 14:42:30,984 - 0:00:06 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 14:42:34,528 - 0:00:09 - 3.5s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [0, 2, 4, 6, 8, 10]
The Flat  : True

[0]
2023-08-20 14:42:46,013 - 0:00:21 - 11.5s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
2023-08-20 14:44:00,668 - 0:01:35 - 74.7s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 3.725 , qa loss 3.725 , lm loss 0.000 , avg batch size 4.0
2023-08-20 14:44:43,088 - 0:02:18 - 42.4s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 2.28 , qa loss 2.28 , lm loss 0.00 , avg batch size 4.0
2023-08-20 14:45:56,543 - 0:03:31 - 73.5s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.202 , qa loss 0.202 , lm loss 0.000 , avg batch size 4.0
2023-08-20 14:46:39,365 - 0:04:14 - 42.8s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-20 14:47:50,638 - 0:05:25 - 71.3s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.176 , qa loss 0.176 , lm loss 0.000 , avg batch size 4.0
2023-08-20 14:48:32,152 - 0:06:07 - 41.5s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-20 14:49:43,548 - 0:07:18 - 71.4s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.151 , qa loss 0.151 , lm loss 0.000 , avg batch size 4.0
2023-08-20 14:50:25,291 - 0:08:00 - 41.7s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 14:51:36,379 - 0:09:11 - 71.1s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.137 , qa loss 0.137 , lm loss 0.000 , avg batch size 4.0
2023-08-20 14:52:17,452 - 0:09:52 - 41.1s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 14:53:29,175 - 0:11:04 - 71.7s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.132 , qa loss 0.132 , lm loss 0.000 , avg batch size 4.0
2023-08-20 14:54:11,992 - 0:11:47 - 42.8s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 14:55:23,867 - 0:12:59 - 71.9s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.125 , qa loss 0.125 , lm loss 0.000 , avg batch size 4.0
2023-08-20 14:56:05,894 - 0:13:41 - 42.0s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 14:57:16,660 - 0:14:51 - 70.8s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.109 , qa loss 0.109 , lm loss 0.000 , avg batch size 4.0
2023-08-20 14:57:57,752 - 0:15:33 - 41.1s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 14:59:08,441 - 0:16:43 - 70.7s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.107 , qa loss 0.107 , lm loss 0.000 , avg batch size 4.0
2023-08-20 14:59:48,112 - 0:17:23 - 39.7s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:01:00,632 - 0:18:35 - 72.5s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.100 , qa loss 0.100 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:01:40,450 - 0:19:15 - 39.8s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:02:51,495 - 0:20:26 - 71.0s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.095 , qa loss 0.095 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:03:31,497 - 0:21:06 - 40.0s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:04:42,185 - 0:22:17 - 70.7s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.089 , qa loss 0.089 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:05:22,790 - 0:22:58 - 40.6s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:06:49,927 - 0:24:25 - 87.1s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.082 , qa loss 0.082 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:07:30,077 - 0:25:05 - 40.2s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:08:50,729 - 0:26:26 - 80.7s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.077 , qa loss 0.077 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:09:32,048 - 0:27:07 - 41.3s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:10:42,340 - 0:28:17 - 70.3s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.079 , qa loss 0.079 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:11:24,301 - 0:28:59 - 42.0s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:12:35,982 - 0:30:11 - 71.7s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.068 , qa loss 0.068 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:13:17,646 - 0:30:52 - 41.7s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:14:27,855 - 0:32:03 - 70.2s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.065 , qa loss 0.065 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:15:08,768 - 0:32:44 - 40.9s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:16:22,848 - 0:33:58 - 74.1s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.061 , qa loss 0.061 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:17:04,305 - 0:34:39 - 41.5s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:18:15,191 - 0:35:50 - 70.9s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.053 , qa loss 0.053 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:18:55,909 - 0:36:31 - 40.7s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:20:07,079 - 0:37:42 - 71.2s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.062 , qa loss 0.062 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:20:50,756 - 0:38:26 - 43.7s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:20:54,703 - 0:38:30 - 3.9s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-20 15:20:54,703 - 0:38:30 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 15:20:54,850 - 0:38:30 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        3,550,464       2.853       1       1
srl                      bottleneck        3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 15:21:04,062 - 0:38:39 - 9.2s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-20 15:22:46,233 - 0:40:21 - 102.2s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 4.370 , qa loss 4.370 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:23:40,039 - 0:41:15 - 53.8s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 3.09 , qa loss 3.09 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:25:23,821 - 0:42:59 - 103.8s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.826 , qa loss 0.826 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:26:18,124 - 0:43:53 - 54.3s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.80 , qa loss 0.80 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:28:03,424 - 0:45:38 - 105.3s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.699 , qa loss 0.699 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:28:56,739 - 0:46:32 - 53.3s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.68 , qa loss 0.68 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:30:39,249 - 0:48:14 - 102.5s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.630 , qa loss 0.630 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:31:31,117 - 0:49:06 - 51.9s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.62 , qa loss 0.62 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:33:14,363 - 0:50:49 - 103.2s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.601 , qa loss 0.601 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:34:06,960 - 0:51:42 - 52.6s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:35:49,456 - 0:53:24 - 102.5s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.539 , qa loss 0.539 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:36:42,659 - 0:54:17 - 53.2s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:38:25,448 - 0:56:00 - 102.8s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.511 , qa loss 0.511 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:39:19,325 - 0:56:54 - 53.9s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:41:00,536 - 0:58:35 - 101.2s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.472 , qa loss 0.472 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:41:54,712 - 0:59:30 - 54.2s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:43:37,714 - 1:01:13 - 103.0s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.454 , qa loss 0.454 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:44:29,855 - 1:02:05 - 52.1s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:46:12,686 - 1:03:47 - 102.8s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.426 , qa loss 0.426 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:47:05,112 - 1:04:40 - 52.4s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:48:46,140 - 1:06:21 - 101.0s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.414 , qa loss 0.414 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:49:39,678 - 1:07:14 - 53.5s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:51:22,536 - 1:08:57 - 102.9s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.385 , qa loss 0.385 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:52:15,935 - 1:09:51 - 53.4s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:53:56,343 - 1:11:31 - 100.4s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.369 , qa loss 0.369 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:54:50,355 - 1:12:25 - 54.0s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:56:36,236 - 1:14:11 - 105.9s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.366 , qa loss 0.366 , lm loss 0.000 , avg batch size 4.0
2023-08-20 15:57:31,350 - 1:15:06 - 55.1s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-08-20 15:59:13,758 - 1:16:49 - 102.4s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.370 , qa loss 0.370 , lm loss 0.000 , avg batch size 4.0
2023-08-20 16:00:08,338 - 1:17:43 - 54.6s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:01:51,864 - 1:19:27 - 103.5s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.331 , qa loss 0.331 , lm loss 0.000 , avg batch size 4.0
2023-08-20 16:02:44,205 - 1:20:19 - 52.3s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:04:26,765 - 1:22:02 - 102.6s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.335 , qa loss 0.335 , lm loss 0.000 , avg batch size 4.0
2023-08-20 16:05:20,947 - 1:22:56 - 54.2s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:07:00,744 - 1:24:36 - 99.8s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.328 , qa loss 0.328 , lm loss 0.000 , avg batch size 4.0
2023-08-20 16:07:54,553 - 1:25:29 - 53.8s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:09:37,371 - 1:27:12 - 102.8s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.313 , qa loss 0.313 , lm loss 0.000 , avg batch size 4.0
2023-08-20 16:10:29,706 - 1:28:05 - 52.3s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:12:11,499 - 1:29:46 - 101.8s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.308 , qa loss 0.308 , lm loss 0.000 , avg batch size 4.0
2023-08-20 16:13:08,679 - 1:30:43 - 57.2s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:13:12,039 - 1:30:47 - 3.4s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-20 16:13:12,040 - 1:30:47 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 16:13:12,169 - 1:30:47 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        3,550,464       2.853       0       0
srl                      bottleneck        3,550,464       2.853       1       1
woz_en                   bottleneck        3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 16:13:20,948 - 1:30:56 - 8.8s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-20 16:14:18,201 - 1:31:53 - 57.3s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 6.17 , qa loss 6.17 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:15:14,570 - 1:32:49 - 56.4s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:16:09,272 - 1:33:44 - 54.7s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:17:04,986 - 1:34:40 - 55.7s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:17:59,478 - 1:35:34 - 54.5s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:18:56,035 - 1:36:31 - 56.6s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:19:53,110 - 1:37:28 - 57.1s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:20:51,323 - 1:38:26 - 58.2s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:21:47,761 - 1:39:23 - 56.4s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:22:45,428 - 1:40:20 - 57.7s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:23:43,874 - 1:41:19 - 58.4s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:24:42,284 - 1:42:17 - 58.4s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:25:39,622 - 1:43:14 - 57.3s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:26:36,134 - 1:44:11 - 56.5s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:27:32,137 - 1:45:07 - 56.0s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:28:28,384 - 1:46:03 - 56.2s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:29:23,878 - 1:46:59 - 55.5s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:30:21,639 - 1:47:56 - 57.8s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:31:18,784 - 1:48:54 - 57.1s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 16:32:17,843 - 1:49:53 - 59.1s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:49:50
CPU Execution time: 01:36:37
................................................................................................................................
Testing Adapters at rf 4
................................................................................................................................
2023-08-20 16:32:28,841 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[1], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 16:32:42,603 - 0:00:19 - 13.8s - INFO - __main__ - task: sst, epoch: 20
2023-08-20 16:32:42,604 - 0:00:19 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-20 16:32:47,073 - 0:00:24 - 4.5s - INFO - __main__ - len of test dataset: 1821
2023-08-20 16:32:58,570 - 0:00:35 - 11.5s - INFO - __main__ - score: {'sst': OrderedDict([('em', 90.60955518945634), ('nf1', 90.60955518945634), ('nem', 90.60955518945634)]), 'srl': None, 'woz.en': None}
2023-08-20 16:33:10,265 - 0:00:47 - 11.7s - INFO - __main__ - task: srl, epoch: 20
2023-08-20 16:33:10,265 - 0:00:47 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-20 16:33:15,136 - 0:00:52 - 4.9s - INFO - __main__ - len of test dataset: 2201
2023-08-20 16:58:30,432 - 0:26:07 - 1515.3s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 46.43343934575193), ('nf1', 66.43028192593633), ('nem', 51.794638800545215)]), 'woz.en': None}
2023-08-20 16:58:41,942 - 0:26:19 - 11.5s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-20 16:58:41,942 - 0:26:19 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-20 16:58:47,028 - 0:26:24 - 5.1s - INFO - __main__ - len of test dataset: 1646
2023-08-20 17:11:33,294 - 0:39:10 - 766.3s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 16.646415552855405), ('nf1', 93.9148781554614), ('nem', 86.02673147023087), ('joint_goal_em', 85.66221142162819), ('turn_request_em', 92.10206561360876), ('turn_goal_em', 90.94775212636695), ('avg_dialogue', 88.88213851761847)])}
................................................................................................................................
 Training Adapters at rf 4
................................................................................................................................
Available number of GPU = 4 < n_gpus = 12
Continue training with 4 GPUs
2023-08-20 17:11:41,080 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[1, 10, 12, 14], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='1,3,5,7,9,11', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[28835.84, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=4, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[10092, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[10092, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 17:11:41,080 - 0:00:06 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-20 17:11:41,080 - 0:00:06 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 17:11:43,769 - 0:00:08 - 2.7s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : [1, 3, 5, 7, 9, 11]
The Flat  : True

[0]
2023-08-20 17:11:54,398 - 0:00:19 - 10.6s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
2023-08-20 17:13:13,661 - 0:01:38 - 79.3s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 3.018 , qa loss 3.018 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:13:59,260 - 0:02:24 - 45.6s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.87 , qa loss 1.87 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:15:14,497 - 0:03:39 - 75.2s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.241 , qa loss 0.241 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:16:00,352 - 0:04:25 - 45.9s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:17:16,059 - 0:05:41 - 75.7s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.184 , qa loss 0.184 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:18:01,278 - 0:06:26 - 45.2s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:19:17,323 - 0:07:42 - 76.0s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.162 , qa loss 0.162 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:20:01,054 - 0:08:26 - 43.7s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:21:15,616 - 0:09:40 - 74.6s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.152 , qa loss 0.152 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:21:58,862 - 0:10:23 - 43.2s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:23:11,812 - 0:11:36 - 72.9s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.139 , qa loss 0.139 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:23:53,347 - 0:12:18 - 41.5s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:25:05,272 - 0:13:30 - 71.9s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.127 , qa loss 0.127 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:25:46,940 - 0:14:11 - 41.7s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:26:59,102 - 0:15:24 - 72.2s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.119 , qa loss 0.119 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:27:41,243 - 0:16:06 - 42.1s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:28:54,538 - 0:17:19 - 73.3s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.120 , qa loss 0.120 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:29:36,289 - 0:18:01 - 41.8s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:30:47,045 - 0:19:12 - 70.8s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.106 , qa loss 0.106 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:31:28,472 - 0:19:53 - 41.4s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:32:40,961 - 0:21:06 - 72.5s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.102 , qa loss 0.102 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:33:23,124 - 0:21:48 - 42.2s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:34:35,775 - 0:23:00 - 72.7s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.097 , qa loss 0.097 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:35:17,103 - 0:23:42 - 41.3s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:36:28,024 - 0:24:53 - 70.9s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.083 , qa loss 0.083 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:37:09,640 - 0:25:34 - 41.6s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:38:22,700 - 0:26:47 - 73.1s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.079 , qa loss 0.079 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:39:04,705 - 0:27:29 - 42.0s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:40:17,297 - 0:28:42 - 72.6s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.074 , qa loss 0.074 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:40:59,141 - 0:29:24 - 41.8s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:42:11,359 - 0:30:36 - 72.2s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.065 , qa loss 0.065 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:42:53,719 - 0:31:18 - 42.4s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:44:05,643 - 0:32:30 - 71.9s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.055 , qa loss 0.055 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:44:48,647 - 0:33:13 - 43.0s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:46:01,435 - 0:34:26 - 72.8s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.061 , qa loss 0.061 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:46:42,908 - 0:35:07 - 41.5s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:47:56,390 - 0:36:21 - 73.5s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.059 , qa loss 0.059 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:48:37,353 - 0:37:02 - 41.0s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:49:49,162 - 0:38:14 - 71.8s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.059 , qa loss 0.059 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:50:34,377 - 0:38:59 - 45.2s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:50:37,539 - 0:39:02 - 3.2s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-20 17:50:37,540 - 0:39:02 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 17:50:37,666 - 0:39:02 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        3,550,464       2.853       1       1
srl                      bottleneck        3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 17:50:45,554 - 0:39:10 - 7.9s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-20 17:52:28,345 - 0:40:53 - 102.8s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 4.444 , qa loss 4.444 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:53:22,682 - 0:41:47 - 54.3s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 3.12 , qa loss 3.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:55:05,445 - 0:43:30 - 102.8s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.795 , qa loss 0.795 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:56:00,807 - 0:44:25 - 55.4s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.77 , qa loss 0.77 , lm loss 0.00 , avg batch size 4.0
2023-08-20 17:57:44,925 - 0:46:09 - 104.1s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.689 , qa loss 0.689 , lm loss 0.000 , avg batch size 4.0
2023-08-20 17:58:37,319 - 0:47:02 - 52.4s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.67 , qa loss 0.67 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:00:20,926 - 0:48:45 - 103.6s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.597 , qa loss 0.597 , lm loss 0.000 , avg batch size 4.0
2023-08-20 18:01:14,846 - 0:49:39 - 53.9s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:02:57,712 - 0:51:22 - 102.9s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.559 , qa loss 0.559 , lm loss 0.000 , avg batch size 4.0
2023-08-20 18:03:51,393 - 0:52:16 - 53.7s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.55 , qa loss 0.55 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:06:05,221 - 0:54:30 - 133.8s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.514 , qa loss 0.514 , lm loss 0.000 , avg batch size 4.0
2023-08-20 18:07:16,529 - 0:55:41 - 71.3s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:09:30,792 - 0:57:55 - 134.3s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.466 , qa loss 0.466 , lm loss 0.000 , avg batch size 4.0
2023-08-20 18:10:44,486 - 0:59:09 - 73.7s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:12:57,969 - 1:01:23 - 133.5s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.444 , qa loss 0.444 , lm loss 0.000 , avg batch size 4.0
2023-08-20 18:14:08,934 - 1:02:33 - 71.0s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:16:17,003 - 1:04:42 - 128.1s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.410 , qa loss 0.410 , lm loss 0.000 , avg batch size 4.0
2023-08-20 18:17:28,269 - 1:05:53 - 71.3s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:19:39,067 - 1:08:04 - 130.8s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.409 , qa loss 0.409 , lm loss 0.000 , avg batch size 4.0
2023-08-20 18:20:49,388 - 1:09:14 - 70.3s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:22:59,176 - 1:11:24 - 129.8s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.385 , qa loss 0.385 , lm loss 0.000 , avg batch size 4.0
2023-08-20 18:24:09,056 - 1:12:34 - 69.9s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:25:58,271 - 1:14:23 - 109.2s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.370 , qa loss 0.370 , lm loss 0.000 , avg batch size 4.0
2023-08-20 18:26:55,326 - 1:15:20 - 57.1s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:28:37,817 - 1:17:02 - 102.5s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.356 , qa loss 0.356 , lm loss 0.000 , avg batch size 4.0
2023-08-20 18:29:31,939 - 1:17:56 - 54.1s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:31:14,583 - 1:19:39 - 102.6s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.343 , qa loss 0.343 , lm loss 0.000 , avg batch size 4.0
2023-08-20 18:32:08,202 - 1:20:33 - 53.6s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:33:52,660 - 1:22:17 - 104.5s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.325 , qa loss 0.325 , lm loss 0.000 , avg batch size 4.0
2023-08-20 18:34:44,083 - 1:23:09 - 51.4s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:36:25,623 - 1:24:50 - 101.5s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.332 , qa loss 0.332 , lm loss 0.000 , avg batch size 4.0
2023-08-20 18:37:19,311 - 1:25:44 - 53.7s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:39:02,634 - 1:27:27 - 103.3s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.299 , qa loss 0.299 , lm loss 0.000 , avg batch size 4.0
2023-08-20 18:39:55,065 - 1:28:20 - 52.4s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:41:37,480 - 1:30:02 - 102.4s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.301 , qa loss 0.301 , lm loss 0.000 , avg batch size 4.0
2023-08-20 18:42:29,903 - 1:30:54 - 52.4s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:44:14,205 - 1:32:39 - 104.3s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.297 , qa loss 0.297 , lm loss 0.000 , avg batch size 4.0
2023-08-20 18:45:06,572 - 1:33:31 - 52.4s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:46:49,515 - 1:35:14 - 102.9s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.293 , qa loss 0.293 , lm loss 0.000 , avg batch size 4.0
2023-08-20 18:47:42,706 - 1:36:07 - 53.2s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:47:43,938 - 1:36:08 - 1.2s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-20 18:47:43,939 - 1:36:08 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 18:47:44,063 - 1:36:09 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        3,550,464       2.853       0       0
srl                      bottleneck        3,550,464       2.853       1       1
woz_en                   bottleneck        3,550,464       2.853       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 18:47:51,315 - 1:36:16 - 7.3s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-20 18:48:54,618 - 1:37:19 - 63.3s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 5.44 , qa loss 5.44 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:50:02,895 - 1:38:27 - 68.3s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:51:06,571 - 1:39:31 - 63.7s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:52:11,359 - 1:40:36 - 64.8s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:53:16,194 - 1:41:41 - 64.8s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:54:22,723 - 1:42:47 - 66.5s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:55:27,482 - 1:43:52 - 64.8s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:56:34,023 - 1:44:59 - 66.5s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:57:42,209 - 1:46:07 - 68.2s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:58:42,612 - 1:47:07 - 60.4s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 18:59:43,064 - 1:48:08 - 60.5s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 19:00:43,161 - 1:49:08 - 60.1s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 19:01:44,724 - 1:50:09 - 61.6s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 19:02:45,358 - 1:51:10 - 60.6s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 19:03:44,038 - 1:52:09 - 58.7s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 19:04:41,510 - 1:53:06 - 57.5s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 19:05:38,309 - 1:54:03 - 56.8s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 19:06:35,637 - 1:55:00 - 57.3s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 19:07:33,917 - 1:55:58 - 58.3s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 19:08:32,673 - 1:56:57 - 58.8s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:56:54
CPU Execution time: 01:39:16
................................................................................................................................
Testing Adapters at rf 4
................................................................................................................................
2023-08-20 19:08:43,783 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[1], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 19:08:58,217 - 0:00:20 - 14.4s - INFO - __main__ - task: sst, epoch: 20
2023-08-20 19:08:58,217 - 0:00:20 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-20 19:09:02,945 - 0:00:25 - 4.7s - INFO - __main__ - len of test dataset: 1821
2023-08-20 19:09:13,068 - 0:00:35 - 10.1s - INFO - __main__ - score: {'sst': OrderedDict([('em', 91.59802306425041), ('nf1', 91.59802306425041), ('nem', 91.59802306425041)]), 'srl': None, 'woz.en': None}
2023-08-20 19:09:25,690 - 0:00:48 - 12.6s - INFO - __main__ - task: srl, epoch: 20
2023-08-20 19:09:25,691 - 0:00:48 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-20 19:09:30,601 - 0:00:53 - 4.9s - INFO - __main__ - len of test dataset: 2201
2023-08-20 19:37:17,144 - 0:28:39 - 1666.5s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 49.88641526578827), ('nf1', 69.36241848750943), ('nem', 55.565651976374376)]), 'woz.en': None}
2023-08-20 19:37:28,685 - 0:28:51 - 11.5s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-20 19:37:28,685 - 0:28:51 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-20 19:37:33,292 - 0:28:55 - 4.6s - INFO - __main__ - len of test dataset: 1646
2023-08-20 19:51:05,826 - 0:42:28 - 812.5s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 16.707168894289186), ('nf1', 93.91557443501551), ('nem', 85.54070473876064), ('joint_goal_em', 85.29769137302551), ('turn_request_em', 91.73754556500607), ('turn_goal_em', 90.46172539489672), ('avg_dialogue', 88.51761846901579)])}
................................................................................................................................
 Training Adapters at rf 4
................................................................................................................................
Available number of GPU = 4 < n_gpus = 12
Continue training with 4 GPUs
2023-08-20 19:51:13,370 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[1, 7, 10, 12], dynamic_epochs=False, flat=True, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[28835.84, 34078.72, 34078.72, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=4, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[10092, 11927, 11927, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[10092, 11927, 11927, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 19:51:13,370 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-08-20 19:51:13,370 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 19:51:16,378 - 0:00:08 - 3.0s - INFO - __main__ - gen token = __gen__ , gen token id = 50260

The Prefix length is : 30
The Reduction Facotr is : 4
The Bottleneck size is : 800
The leaving layers are : []
The Flat  : True

[0]
2023-08-20 19:51:28,213 - 0:00:20 - 11.8s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        7,100,928       5.706       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               1
================================================================================
2023-08-20 19:52:49,888 - 0:01:42 - 81.7s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.510 , qa loss 2.510 , lm loss 0.000 , avg batch size 4.0
2023-08-20 19:53:38,305 - 0:02:30 - 48.4s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.55 , qa loss 1.55 , lm loss 0.00 , avg batch size 4.0
2023-08-20 19:55:01,680 - 0:03:54 - 83.4s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.220 , qa loss 0.220 , lm loss 0.000 , avg batch size 4.0
2023-08-20 19:55:50,679 - 0:04:43 - 49.0s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-20 19:57:11,182 - 0:06:03 - 80.5s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.178 , qa loss 0.178 , lm loss 0.000 , avg batch size 4.0
2023-08-20 19:58:01,260 - 0:06:53 - 50.1s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 19:59:21,571 - 0:08:13 - 80.3s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.152 , qa loss 0.152 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:00:13,092 - 0:09:05 - 51.5s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:01:34,227 - 0:10:26 - 81.1s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.137 , qa loss 0.137 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:02:23,008 - 0:11:15 - 48.8s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:03:42,741 - 0:12:35 - 79.7s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.138 , qa loss 0.138 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:04:30,900 - 0:13:23 - 48.2s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:05:53,564 - 0:14:45 - 82.7s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.110 , qa loss 0.110 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:06:42,605 - 0:15:34 - 49.0s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:08:02,711 - 0:16:55 - 80.1s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.101 , qa loss 0.101 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:08:55,919 - 0:17:48 - 53.2s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:10:18,780 - 0:19:11 - 82.9s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.087 , qa loss 0.087 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:11:07,963 - 0:20:00 - 49.2s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:12:29,106 - 0:21:21 - 81.1s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.080 , qa loss 0.080 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:13:17,589 - 0:22:09 - 48.5s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:14:39,407 - 0:23:31 - 81.8s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.069 , qa loss 0.069 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:15:28,174 - 0:24:20 - 48.8s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:16:50,152 - 0:25:42 - 82.0s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.062 , qa loss 0.062 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:17:38,729 - 0:26:31 - 48.6s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:18:59,383 - 0:27:51 - 80.7s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.055 , qa loss 0.055 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:19:47,529 - 0:28:39 - 48.1s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:21:10,294 - 0:30:02 - 82.8s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.049 , qa loss 0.049 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:21:59,790 - 0:30:52 - 49.5s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:23:21,314 - 0:32:13 - 81.5s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.042 , qa loss 0.042 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:24:09,722 - 0:33:02 - 48.4s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:25:29,641 - 0:34:21 - 79.9s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.043 , qa loss 0.043 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:26:19,462 - 0:35:11 - 49.8s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:27:40,464 - 0:36:32 - 81.0s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.041 , qa loss 0.041 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:28:29,696 - 0:37:22 - 49.2s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:29:51,628 - 0:38:43 - 81.9s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.035 , qa loss 0.035 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:30:40,806 - 0:39:33 - 49.2s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:32:03,741 - 0:40:56 - 82.9s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.028 , qa loss 0.028 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:32:53,069 - 0:41:45 - 49.3s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:34:15,356 - 0:43:07 - 82.3s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.034 , qa loss 0.034 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:35:07,622 - 0:43:59 - 52.3s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:35:10,924 - 0:44:03 - 3.3s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-08-20 20:35:10,925 - 0:44:03 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 20:35:11,047 - 0:44:03 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        7,100,928       5.706       1       1
srl                      bottleneck        7,100,928       5.706       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 20:35:20,337 - 0:44:12 - 9.3s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-08-20 20:37:18,480 - 0:46:10 - 118.1s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 3.467 , qa loss 3.467 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:38:19,637 - 0:47:11 - 61.2s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.48 , qa loss 2.48 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:40:14,060 - 0:49:06 - 114.4s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.742 , qa loss 0.742 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:41:14,917 - 0:50:07 - 60.9s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.73 , qa loss 0.73 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:43:09,792 - 0:52:02 - 114.9s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.630 , qa loss 0.630 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:44:13,079 - 0:53:05 - 63.3s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:47:21,373 - 0:56:13 - 188.3s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.553 , qa loss 0.553 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:49:50,702 - 0:58:43 - 149.3s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:53:41,152 - 1:02:33 - 230.4s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.489 , qa loss 0.489 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:54:43,395 - 1:03:35 - 62.2s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:56:36,653 - 1:05:28 - 113.3s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.441 , qa loss 0.441 , lm loss 0.000 , avg batch size 4.0
2023-08-20 20:57:36,812 - 1:06:29 - 60.2s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-08-20 20:59:32,095 - 1:08:24 - 115.3s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.413 , qa loss 0.413 , lm loss 0.000 , avg batch size 4.0
2023-08-20 21:00:33,222 - 1:09:25 - 61.1s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:02:28,225 - 1:11:20 - 115.0s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.359 , qa loss 0.359 , lm loss 0.000 , avg batch size 4.0
2023-08-20 21:03:29,998 - 1:12:22 - 61.8s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:05:27,862 - 1:14:20 - 117.9s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.335 , qa loss 0.335 , lm loss 0.000 , avg batch size 4.0
2023-08-20 21:06:28,528 - 1:15:20 - 60.7s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:08:22,576 - 1:17:14 - 114.0s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.335 , qa loss 0.335 , lm loss 0.000 , avg batch size 4.0
2023-08-20 21:09:26,669 - 1:18:18 - 64.1s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:11:21,348 - 1:20:13 - 114.7s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.302 , qa loss 0.302 , lm loss 0.000 , avg batch size 4.0
2023-08-20 21:12:22,416 - 1:21:14 - 61.1s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:14:16,577 - 1:23:08 - 114.2s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.288 , qa loss 0.288 , lm loss 0.000 , avg batch size 4.0
2023-08-20 21:15:19,700 - 1:24:12 - 63.1s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:17:14,901 - 1:26:07 - 115.2s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.260 , qa loss 0.260 , lm loss 0.000 , avg batch size 4.0
2023-08-20 21:18:15,524 - 1:27:07 - 60.6s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:20:11,775 - 1:29:04 - 116.3s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.253 , qa loss 0.253 , lm loss 0.000 , avg batch size 4.0
2023-08-20 21:21:11,061 - 1:30:03 - 59.3s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:23:05,877 - 1:31:58 - 114.8s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.229 , qa loss 0.229 , lm loss 0.000 , avg batch size 4.0
2023-08-20 21:24:08,152 - 1:33:00 - 62.3s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:26:03,340 - 1:34:55 - 115.2s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.227 , qa loss 0.227 , lm loss 0.000 , avg batch size 4.0
2023-08-20 21:27:05,469 - 1:35:57 - 62.1s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:28:59,441 - 1:37:51 - 114.0s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.217 , qa loss 0.217 , lm loss 0.000 , avg batch size 4.0
2023-08-20 21:30:00,370 - 1:38:52 - 60.9s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:31:57,814 - 1:40:50 - 117.4s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.209 , qa loss 0.209 , lm loss 0.000 , avg batch size 4.0
2023-08-20 21:33:00,025 - 1:41:52 - 62.2s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:35:00,073 - 1:43:52 - 120.0s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.204 , qa loss 0.204 , lm loss 0.000 , avg batch size 4.0
2023-08-20 21:36:04,538 - 1:44:56 - 64.5s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:38:01,481 - 1:46:53 - 116.9s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.197 , qa loss 0.197 , lm loss 0.000 , avg batch size 4.0
2023-08-20 21:39:07,669 - 1:47:59 - 66.2s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:39:10,848 - 1:48:03 - 3.2s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-08-20 21:39:10,849 - 1:48:03 - 0.0s - INFO - __main__ - extra training data size: 0
2023-08-20 21:39:10,980 - 1:48:03 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
sst                      bottleneck        7,100,928       5.706       0       0
srl                      bottleneck        7,100,928       5.706       1       1
woz_en                   bottleneck        7,100,928       5.706       0       1
--------------------------------------------------------------------------------
Full model                               124,442,880     100.000               0
================================================================================
2023-08-20 21:39:20,172 - 1:48:12 - 9.2s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-08-20 21:40:30,438 - 1:49:22 - 70.3s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 3.94 , qa loss 3.94 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:41:40,669 - 1:50:32 - 70.2s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:42:51,071 - 1:51:43 - 70.4s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:44:01,761 - 1:52:54 - 70.7s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:45:12,890 - 1:54:05 - 71.1s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:46:24,213 - 1:55:16 - 71.3s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:47:34,094 - 1:56:26 - 69.9s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:48:44,055 - 1:57:36 - 70.0s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:49:54,814 - 1:58:47 - 70.8s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:51:04,824 - 1:59:57 - 70.0s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:52:15,960 - 2:01:08 - 71.1s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:53:28,226 - 2:02:20 - 72.3s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:54:38,951 - 2:03:31 - 70.7s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:55:49,727 - 2:04:42 - 70.8s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:57:02,234 - 2:05:54 - 72.5s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:58:15,672 - 2:07:08 - 73.4s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-20 21:59:25,751 - 2:08:18 - 70.1s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-20 22:00:36,729 - 2:09:29 - 71.0s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-08-20 22:01:47,849 - 2:10:40 - 71.1s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-08-20 22:03:02,091 - 2:11:54 - 74.2s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 02:11:51
CPU Execution time: 01:58:10
................................................................................................................................
Testing Adapters at rf 4
................................................................................................................................
2023-08-20 22:03:15,317 - 0:00:09 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, bottle_neck_size=800, data_dir='data', debug=False, decay_style='linear', device_ids=[1], dynamic_epochs=False, flat=False, fp32=False, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, leaveout='', lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32768.0], min_batch_size=4, min_n_steps=1500, model_dir_root='models_ag/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=35, prefixlength=30, qp_margin=0.5, real_sample=False, reg_lambda=1.0, rf=4, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[16056], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[16056], unbound=0, use_sep=False, weight_decay=0.01)
2023-08-20 22:03:30,023 - 0:00:23 - 14.7s - INFO - __main__ - task: sst, epoch: 20
2023-08-20 22:03:30,023 - 0:00:23 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-08-20 22:03:34,358 - 0:00:28 - 4.3s - INFO - __main__ - len of test dataset: 1821
2023-08-20 22:03:44,673 - 0:00:38 - 10.3s - INFO - __main__ - score: {'sst': OrderedDict([('em', 90.82921471718835), ('nf1', 90.82921471718835), ('nem', 90.82921471718835)]), 'srl': None, 'woz.en': None}
2023-08-20 22:03:56,334 - 0:00:50 - 11.7s - INFO - __main__ - task: srl, epoch: 20
2023-08-20 22:03:56,335 - 0:00:50 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-08-20 22:04:01,356 - 0:00:55 - 5.0s - INFO - __main__ - len of test dataset: 2201
2023-08-20 22:29:58,440 - 0:26:52 - 1557.1s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 49.70467969104953), ('nf1', 69.60532471996126), ('nem', 55.0658791458428)]), 'woz.en': None}
2023-08-20 22:30:10,687 - 0:27:04 - 12.2s - INFO - __main__ - task: woz.en, epoch: 20
2023-08-20 22:30:10,687 - 0:27:04 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-08-20 22:30:15,756 - 0:27:09 - 5.1s - INFO - __main__ - len of test dataset: 1646
2023-08-20 22:42:01,372 - 0:38:55 - 705.6s - INFO - __main__ - score: {'sst': None, 'srl': None, 'woz.en': OrderedDict([('em', 16.950182260024302), ('nf1', 93.779032497137), ('nem', 86.2089914945322), ('joint_goal_em', 84.44714459295261), ('turn_request_em', 92.10206561360876), ('turn_goal_em', 90.5224787363305), ('avg_dialogue', 88.27460510328069)])}
--------------------------------------------The End Man!!!------------------------------------------------------
