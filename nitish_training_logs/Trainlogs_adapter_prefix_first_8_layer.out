Available number of GPU = 2 < n_gpus = 12
Continue training with 2 GPUs
2023-07-29 20:30:33,648 - 0:00:07 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[13, 15], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.2, learning_rate=6.25e-05, lm_lambda=0.2, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[31457.28, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='new_models/gpt2/lll/sst_srl_woz.en_0.2', model_name='gpt2', n_gpus=2, n_train_epochs={'sst': 10, 'srl': 10, 'woz.en': 10}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[11010, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[11010, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-07-29 20:30:33,648 - 0:00:07 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-07-29 20:30:33,649 - 0:00:07 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-29 20:30:36,610 - 0:00:10 - 3.0s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
[0]
2023-07-29 20:30:51,541 - 0:00:25 - 14.9s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 69200
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-07-29 20:32:39,049 - 0:02:12 - 107.5s - INFO - __main__ - progress 0.578 , lr 5.9E-05 , loss 1.584 , qa loss 1.584 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:33:36,965 - 0:03:10 - 57.9s - INFO - __main__ - epoch 1/10 done , tot steps 1730 , lr 5.6E-05 , loss 1.04 , qa loss 1.04 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:35:12,988 - 0:04:46 - 96.0s - INFO - __main__ - progress 1.578 , lr 5.3E-05 , loss 0.296 , qa loss 0.296 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:36:13,699 - 0:05:47 - 60.7s - INFO - __main__ - epoch 2/10 done , tot steps 3460 , lr 5.0E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:37:49,976 - 0:07:23 - 96.3s - INFO - __main__ - progress 2.578 , lr 4.6E-05 , loss 0.249 , qa loss 0.249 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:38:53,311 - 0:08:27 - 63.3s - INFO - __main__ - epoch 3/10 done , tot steps 5190 , lr 4.4E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:40:21,141 - 0:09:54 - 87.8s - INFO - __main__ - progress 3.578 , lr 4.0E-05 , loss 0.228 , qa loss 0.228 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:41:13,454 - 0:10:47 - 52.3s - INFO - __main__ - epoch 4/10 done , tot steps 6920 , lr 3.8E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:42:59,731 - 0:12:33 - 106.3s - INFO - __main__ - progress 4.578 , lr 3.4E-05 , loss 0.200 , qa loss 0.200 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:44:06,714 - 0:13:40 - 67.0s - INFO - __main__ - epoch 5/10 done , tot steps 8650 , lr 3.1E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:45:53,631 - 0:15:27 - 106.9s - INFO - __main__ - progress 5.578 , lr 2.8E-05 , loss 0.185 , qa loss 0.185 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:46:59,054 - 0:16:32 - 65.4s - INFO - __main__ - epoch 6/10 done , tot steps 10380 , lr 2.5E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:48:40,318 - 0:18:14 - 101.3s - INFO - __main__ - progress 6.578 , lr 2.1E-05 , loss 0.173 , qa loss 0.173 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:49:47,476 - 0:19:21 - 67.2s - INFO - __main__ - epoch 7/10 done , tot steps 12110 , lr 1.9E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:51:30,811 - 0:21:04 - 103.3s - INFO - __main__ - progress 7.578 , lr 1.5E-05 , loss 0.170 , qa loss 0.170 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:52:34,654 - 0:22:08 - 63.8s - INFO - __main__ - epoch 8/10 done , tot steps 13840 , lr 1.3E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:54:16,104 - 0:23:49 - 101.5s - INFO - __main__ - progress 8.578 , lr 8.9E-06 , loss 0.148 , qa loss 0.148 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:55:21,282 - 0:24:55 - 65.2s - INFO - __main__ - epoch 9/10 done , tot steps 15570 , lr 6.3E-06 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:57:07,689 - 0:26:41 - 106.4s - INFO - __main__ - progress 9.578 , lr 2.7E-06 , loss 0.136 , qa loss 0.136 , lm loss 0.000 , avg batch size 4.0
2023-07-29 20:58:10,494 - 0:27:44 - 62.8s - INFO - __main__ - epoch 10/10 done , tot steps 17300 , lr 3.1E-08 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-29 20:58:12,064 - 0:27:45 - 1.6s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-07-29 20:58:12,065 - 0:27:45 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-29 20:58:12,225 - 0:27:45 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-07-29 20:58:28,960 - 0:28:02 - 16.7s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 64140
2023-07-29 21:00:27,508 - 0:30:01 - 118.5s - INFO - __main__ - progress 0.624 , lr 5.9E-05 , loss 3.821 , qa loss 3.821 , lm loss 0.000 , avg batch size 4.0
2023-07-29 21:01:26,945 - 0:31:00 - 59.4s - INFO - __main__ - epoch 1/10 done , tot steps 1604 , lr 5.6E-05 , loss 2.79 , qa loss 2.79 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:03:40,547 - 0:33:14 - 133.6s - INFO - __main__ - progress 1.624 , lr 5.2E-05 , loss 0.919 , qa loss 0.919 , lm loss 0.000 , avg batch size 4.0
2023-07-29 21:04:54,243 - 0:34:27 - 73.7s - INFO - __main__ - epoch 2/10 done , tot steps 3208 , lr 5.0E-05 , loss 0.88 , qa loss 0.88 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:07:06,517 - 0:36:40 - 132.3s - INFO - __main__ - progress 2.624 , lr 4.6E-05 , loss 0.762 , qa loss 0.762 , lm loss 0.000 , avg batch size 4.0
2023-07-29 21:08:18,870 - 0:37:52 - 72.4s - INFO - __main__ - epoch 3/10 done , tot steps 4812 , lr 4.4E-05 , loss 0.74 , qa loss 0.74 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:10:33,535 - 0:40:07 - 134.7s - INFO - __main__ - progress 3.624 , lr 4.0E-05 , loss 0.665 , qa loss 0.665 , lm loss 0.000 , avg batch size 4.0
2023-07-29 21:11:45,741 - 0:41:19 - 72.2s - INFO - __main__ - epoch 4/10 done , tot steps 6416 , lr 3.8E-05 , loss 0.66 , qa loss 0.66 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:13:58,711 - 0:43:32 - 133.0s - INFO - __main__ - progress 4.624 , lr 3.4E-05 , loss 0.595 , qa loss 0.595 , lm loss 0.000 , avg batch size 4.0
2023-07-29 21:15:11,271 - 0:44:45 - 72.6s - INFO - __main__ - epoch 5/10 done , tot steps 8020 , lr 3.1E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:17:23,371 - 0:46:57 - 132.1s - INFO - __main__ - progress 5.624 , lr 2.7E-05 , loss 0.540 , qa loss 0.540 , lm loss 0.000 , avg batch size 4.0
2023-07-29 21:18:30,883 - 0:48:04 - 67.5s - INFO - __main__ - epoch 6/10 done , tot steps 9624 , lr 2.5E-05 , loss 0.55 , qa loss 0.55 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:20:31,525 - 0:50:05 - 120.6s - INFO - __main__ - progress 6.624 , lr 2.1E-05 , loss 0.524 , qa loss 0.524 , lm loss 0.000 , avg batch size 4.0
2023-07-29 21:21:29,882 - 0:51:03 - 58.4s - INFO - __main__ - epoch 7/10 done , tot steps 11228 , lr 1.9E-05 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:23:40,500 - 0:53:14 - 130.6s - INFO - __main__ - progress 7.624 , lr 1.5E-05 , loss 0.504 , qa loss 0.504 , lm loss 0.000 , avg batch size 4.0
2023-07-29 21:24:54,726 - 0:54:28 - 74.2s - INFO - __main__ - epoch 8/10 done , tot steps 12832 , lr 1.3E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:27:05,682 - 0:56:39 - 131.0s - INFO - __main__ - progress 8.624 , lr 8.6E-06 , loss 0.480 , qa loss 0.480 , lm loss 0.000 , avg batch size 4.0
2023-07-29 21:28:20,806 - 0:57:54 - 75.1s - INFO - __main__ - epoch 9/10 done , tot steps 14436 , lr 6.3E-06 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:30:35,061 - 1:00:08 - 134.3s - INFO - __main__ - progress 9.624 , lr 2.4E-06 , loss 0.458 , qa loss 0.458 , lm loss 0.000 , avg batch size 4.0
2023-07-29 21:31:47,154 - 1:01:20 - 72.1s - INFO - __main__ - epoch 10/10 done , tot steps 16040 , lr 3.1E-08 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:31:48,655 - 1:01:22 - 1.5s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-07-29 21:31:48,656 - 1:01:22 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-29 21:31:48,814 - 1:01:22 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-07-29 21:31:57,344 - 1:01:31 - 8.5s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 25360
2023-07-29 21:33:21,025 - 1:02:54 - 83.7s - INFO - __main__ - epoch 1/10 done , tot steps 634 , lr 5.6E-05 , loss 2.63 , qa loss 2.63 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:34:40,505 - 1:04:14 - 79.5s - INFO - __main__ - epoch 2/10 done , tot steps 1268 , lr 5.0E-05 , loss 0.76 , qa loss 0.76 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:36:02,442 - 1:05:36 - 81.9s - INFO - __main__ - epoch 3/10 done , tot steps 1902 , lr 4.4E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:37:24,237 - 1:06:57 - 81.8s - INFO - __main__ - epoch 4/10 done , tot steps 2536 , lr 3.8E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:38:44,100 - 1:08:17 - 79.9s - INFO - __main__ - epoch 5/10 done , tot steps 3170 , lr 3.1E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:40:02,345 - 1:09:36 - 78.2s - INFO - __main__ - epoch 6/10 done , tot steps 3804 , lr 2.5E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:41:05,785 - 1:10:39 - 63.4s - INFO - __main__ - epoch 7/10 done , tot steps 4438 , lr 1.9E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:42:13,014 - 1:11:46 - 67.2s - INFO - __main__ - epoch 8/10 done , tot steps 5072 , lr 1.3E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:43:31,274 - 1:13:05 - 78.3s - INFO - __main__ - epoch 9/10 done , tot steps 5706 , lr 6.3E-06 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-07-29 21:44:51,520 - 1:14:25 - 80.2s - INFO - __main__ - epoch 10/10 done , tot steps 6340 , lr 3.0E-08 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:14:19
CPU Execution time: 01:07:38
