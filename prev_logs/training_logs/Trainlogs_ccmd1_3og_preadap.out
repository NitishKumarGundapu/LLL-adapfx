2023-06-29 13:00:15,665 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[0, 1], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[31457.28, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='model_ccmd1/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=2, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[11010, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[11010, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-29 13:00:15,666 - 0:00:06 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-06-29 13:00:15,666 - 0:00:06 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 13:00:18,368 - 0:00:08 - 2.7s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
[0]
2023-06-29 13:00:33,421 - 0:00:24 - 15.1s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/amana/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-06-29 13:01:51,748 - 0:01:42 - 78.3s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.657 , qa loss 1.657 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:02:36,819 - 0:02:27 - 45.1s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.10 , qa loss 1.10 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:03:51,539 - 0:03:42 - 74.7s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.267 , qa loss 0.267 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:04:35,677 - 0:04:26 - 44.1s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:05:50,502 - 0:05:41 - 74.8s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.219 , qa loss 0.219 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:06:34,870 - 0:06:25 - 44.4s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:07:48,981 - 0:07:39 - 74.1s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.189 , qa loss 0.189 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:08:33,603 - 0:08:24 - 44.6s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:09:48,346 - 0:09:38 - 74.7s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.178 , qa loss 0.178 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:10:32,747 - 0:10:23 - 44.4s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:11:46,761 - 0:11:37 - 74.0s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.151 , qa loss 0.151 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:12:31,667 - 0:12:22 - 44.9s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:13:47,354 - 0:13:37 - 75.7s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.149 , qa loss 0.149 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:14:31,772 - 0:14:22 - 44.4s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:15:48,091 - 0:15:38 - 76.3s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.139 , qa loss 0.139 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:16:32,259 - 0:16:22 - 44.2s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:17:46,436 - 0:17:37 - 74.2s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.134 , qa loss 0.134 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:18:30,372 - 0:18:20 - 43.9s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:19:43,590 - 0:19:34 - 73.2s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.129 , qa loss 0.129 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:20:29,332 - 0:20:19 - 45.7s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:21:42,702 - 0:21:33 - 73.4s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.114 , qa loss 0.114 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:22:27,109 - 0:22:17 - 44.4s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:23:41,109 - 0:23:31 - 74.0s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.105 , qa loss 0.105 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:24:25,213 - 0:24:15 - 44.1s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:25:40,031 - 0:25:30 - 74.8s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.100 , qa loss 0.100 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:26:26,931 - 0:26:17 - 46.9s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:27:41,444 - 0:27:32 - 74.5s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.094 , qa loss 0.094 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:28:25,363 - 0:28:15 - 43.9s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:29:38,860 - 0:29:29 - 73.5s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.088 , qa loss 0.088 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:30:22,703 - 0:30:13 - 43.8s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:31:37,174 - 0:31:27 - 74.5s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.088 , qa loss 0.088 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:32:21,188 - 0:32:11 - 44.0s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:33:36,514 - 0:33:27 - 75.3s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.090 , qa loss 0.090 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:34:21,382 - 0:34:11 - 44.9s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:35:35,884 - 0:35:26 - 74.5s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.086 , qa loss 0.086 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:36:20,853 - 0:36:11 - 45.0s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:37:35,278 - 0:37:25 - 74.4s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.077 , qa loss 0.077 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:38:20,463 - 0:38:11 - 45.2s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:39:33,463 - 0:39:24 - 73.0s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.077 , qa loss 0.077 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:40:19,080 - 0:40:09 - 45.6s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:40:20,227 - 0:40:10 - 1.1s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-06-29 13:40:20,228 - 0:40:10 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 13:40:20,390 - 0:40:11 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
[1]
2023-06-29 13:40:31,434 - 0:40:22 - 11.0s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-06-29 13:42:18,236 - 0:42:08 - 106.8s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 2.950 , qa loss 2.950 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:43:14,552 - 0:43:05 - 56.3s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.24 , qa loss 2.24 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:45:00,694 - 0:44:51 - 106.1s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.902 , qa loss 0.902 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:45:59,087 - 0:45:49 - 58.4s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.87 , qa loss 0.87 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:47:44,522 - 0:47:35 - 105.4s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.749 , qa loss 0.749 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:48:40,424 - 0:48:31 - 55.9s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.74 , qa loss 0.74 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:50:30,058 - 0:50:20 - 109.6s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.680 , qa loss 0.680 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:51:27,185 - 0:51:17 - 57.1s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:53:19,918 - 0:53:10 - 112.7s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.597 , qa loss 0.597 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:54:17,976 - 0:54:08 - 58.1s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.60 , qa loss 0.60 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:56:05,862 - 0:55:56 - 107.9s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.539 , qa loss 0.539 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:57:06,664 - 0:56:57 - 60.8s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.55 , qa loss 0.55 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:58:49,989 - 0:58:40 - 103.3s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.522 , qa loss 0.522 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:59:50,396 - 0:59:41 - 60.4s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:01:38,852 - 1:01:29 - 108.5s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.490 , qa loss 0.490 , lm loss 0.000 , avg batch size 4.0
2023-06-29 14:02:34,118 - 1:02:24 - 55.3s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:04:20,076 - 1:04:10 - 106.0s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.453 , qa loss 0.453 , lm loss 0.000 , avg batch size 4.0
2023-06-29 14:05:20,861 - 1:05:11 - 60.8s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:07:04,543 - 1:06:55 - 103.7s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.441 , qa loss 0.441 , lm loss 0.000 , avg batch size 4.0
2023-06-29 14:08:00,535 - 1:07:51 - 56.0s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:09:45,198 - 1:09:35 - 104.7s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.428 , qa loss 0.428 , lm loss 0.000 , avg batch size 4.0
2023-06-29 14:10:39,648 - 1:10:30 - 54.5s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:12:21,712 - 1:12:12 - 102.1s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.408 , qa loss 0.408 , lm loss 0.000 , avg batch size 4.0
2023-06-29 14:13:16,454 - 1:13:07 - 54.7s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:14:59,813 - 1:14:50 - 103.4s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.392 , qa loss 0.392 , lm loss 0.000 , avg batch size 4.0
2023-06-29 14:15:55,164 - 1:15:45 - 55.4s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:17:34,775 - 1:17:25 - 99.6s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.374 , qa loss 0.374 , lm loss 0.000 , avg batch size 4.0
2023-06-29 14:18:32,541 - 1:18:23 - 57.8s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:20:16,776 - 1:20:07 - 104.2s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.381 , qa loss 0.381 , lm loss 0.000 , avg batch size 4.0
2023-06-29 14:21:12,181 - 1:21:02 - 55.4s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:22:55,693 - 1:22:46 - 103.5s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.372 , qa loss 0.372 , lm loss 0.000 , avg batch size 4.0
2023-06-29 14:23:52,082 - 1:23:42 - 56.4s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:25:35,764 - 1:25:26 - 103.7s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.357 , qa loss 0.357 , lm loss 0.000 , avg batch size 4.0
2023-06-29 14:26:31,241 - 1:26:21 - 55.5s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:28:14,875 - 1:28:05 - 103.6s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.348 , qa loss 0.348 , lm loss 0.000 , avg batch size 4.0
2023-06-29 14:29:12,290 - 1:29:02 - 57.4s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:30:57,025 - 1:30:47 - 104.7s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.350 , qa loss 0.350 , lm loss 0.000 , avg batch size 4.0
2023-06-29 14:31:51,329 - 1:31:41 - 54.3s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:33:36,074 - 1:33:26 - 104.7s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.339 , qa loss 0.339 , lm loss 0.000 , avg batch size 4.0
2023-06-29 14:34:32,145 - 1:34:22 - 56.1s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:34:33,302 - 1:34:23 - 1.2s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-06-29 14:34:33,303 - 1:34:23 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 14:34:33,462 - 1:34:24 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
[2]
2023-06-29 14:34:42,849 - 1:34:33 - 9.4s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-06-29 14:35:40,620 - 1:35:31 - 57.8s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 2.96 , qa loss 2.96 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:36:38,334 - 1:36:28 - 57.7s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.60 , qa loss 0.60 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:37:36,778 - 1:37:27 - 58.4s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:38:34,203 - 1:38:24 - 57.4s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:39:31,547 - 1:39:22 - 57.3s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:40:26,720 - 1:40:17 - 55.2s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:41:22,510 - 1:41:13 - 55.8s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:42:19,386 - 1:42:09 - 56.9s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:43:16,618 - 1:43:07 - 57.2s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:44:13,907 - 1:44:04 - 57.3s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:45:11,976 - 1:45:02 - 58.1s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:46:07,880 - 1:45:58 - 55.9s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:47:04,191 - 1:46:54 - 56.3s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:47:59,890 - 1:47:50 - 55.7s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:48:55,718 - 1:48:46 - 55.8s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:49:52,742 - 1:49:43 - 57.0s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:50:48,244 - 1:50:38 - 55.5s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:51:45,497 - 1:51:36 - 57.3s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:52:42,683 - 1:52:33 - 57.2s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 14:53:40,851 - 1:53:31 - 58.2s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:53:26
CPU Execution time: 01:41:40
