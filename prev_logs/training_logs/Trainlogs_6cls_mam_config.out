Available number of GPU = 2 < n_gpus = 8
Continue training with 2 GPUs
2023-02-24 12:37:28,701 - 0:00:21 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[11, 13], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[31457.28, 34078.72], min_batch_size=4, min_n_steps=1500, model_dir_root='models/gpt2/lll/dbpedia_amazon_ag_sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=2, n_train_epochs={'dbpedia': 8, 'amazon': 8, 'ag': 8, 'sst': 8, 'srl': 8, 'woz.en': 8}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['dbpedia', 'amazon', 'ag', 'sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[11010, 11927], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[11010, 11927], unbound=0, use_sep=False, weight_decay=0.01)
2023-02-24 12:37:28,702 - 0:00:21 - 0.0s - INFO - __main__ - start to train { task: ['dbpedia'], seq train type: lll }
2023-02-24 12:37:28,702 - 0:00:21 - 0.0s - INFO - __main__ - extra training data size: 0
2023-02-24 12:37:50,733 - 0:00:43 - 22.0s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1024). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1858 > 1024). Running this sequence through the model will result in indexing errors
2023-02-24 12:39:01,774 - 0:01:54 - 71.0s - INFO - __main__ - len of train dataset: 115000 , max train batch size 76 , num of opt steps: 920000
/raid/cs21mtech11006/miniconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/raid/cs21mtech11006/miniconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-02-24 12:58:17,727 - 0:21:10 - 1156.0s - INFO - __main__ - progress 0.466 , lr 5.9E-05 , loss 1.238 , qa loss 1.238 , lm loss 0.000 , avg batch size 53.6
2023-02-24 13:16:18,116 - 0:39:10 - 1080.4s - INFO - __main__ - progress 0.932 , lr 5.5E-05 , loss 0.632 , qa loss 0.632 , lm loss 0.000 , avg batch size 53.6
2023-02-24 13:18:58,731 - 0:41:51 - 160.6s - INFO - __main__ - epoch 1/8 done , tot steps 2148 , lr 5.5E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 53.5
2023-02-24 13:37:22,417 - 1:00:14 - 1103.7s - INFO - __main__ - progress 1.464 , lr 5.1E-05 , loss 0.019 , qa loss 0.019 , lm loss 0.000 , avg batch size 53.4
2023-02-24 13:55:19,309 - 1:18:11 - 1076.9s - INFO - __main__ - progress 1.931 , lr 4.7E-05 , loss 0.019 , qa loss 0.019 , lm loss 0.000 , avg batch size 53.5
2023-02-24 13:57:58,988 - 1:20:51 - 159.7s - INFO - __main__ - epoch 2/8 done , tot steps 4296 , lr 4.7E-05 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 53.5
2023-02-24 14:16:43,886 - 1:39:36 - 1124.9s - INFO - __main__ - progress 2.466 , lr 4.3E-05 , loss 0.015 , qa loss 0.015 , lm loss 0.000 , avg batch size 53.6
2023-02-24 14:35:25,192 - 1:58:17 - 1121.3s - INFO - __main__ - progress 2.930 , lr 4.0E-05 , loss 0.015 , qa loss 0.015 , lm loss 0.000 , avg batch size 53.5
2023-02-24 14:38:19,520 - 2:01:11 - 174.3s - INFO - __main__ - epoch 3/8 done , tot steps 6447 , lr 3.9E-05 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 53.5
2023-02-24 14:57:50,063 - 2:20:42 - 1170.5s - INFO - __main__ - progress 3.467 , lr 3.5E-05 , loss 0.013 , qa loss 0.013 , lm loss 0.000 , avg batch size 53.7
2023-02-24 15:17:10,706 - 2:40:03 - 1160.6s - INFO - __main__ - progress 3.932 , lr 3.2E-05 , loss 0.013 , qa loss 0.013 , lm loss 0.000 , avg batch size 53.6
2023-02-24 15:20:02,643 - 2:42:54 - 171.9s - INFO - __main__ - epoch 4/8 done , tot steps 8594 , lr 3.1E-05 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 53.6
2023-02-24 15:39:06,973 - 3:01:59 - 1144.3s - INFO - __main__ - progress 4.466 , lr 2.8E-05 , loss 0.012 , qa loss 0.012 , lm loss 0.000 , avg batch size 53.6
2023-02-24 15:58:23,806 - 3:21:16 - 1156.8s - INFO - __main__ - progress 4.932 , lr 2.4E-05 , loss 0.011 , qa loss 0.011 , lm loss 0.000 , avg batch size 53.6
2023-02-24 16:01:20,158 - 3:24:12 - 176.4s - INFO - __main__ - epoch 5/8 done , tot steps 10741 , lr 2.3E-05 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 53.6
2023-02-24 16:20:54,658 - 3:43:46 - 1174.5s - INFO - __main__ - progress 5.466 , lr 2.0E-05 , loss 0.010 , qa loss 0.010 , lm loss 0.000 , avg batch size 53.5
2023-02-24 16:39:41,490 - 4:02:33 - 1126.8s - INFO - __main__ - progress 5.932 , lr 1.6E-05 , loss 0.010 , qa loss 0.010 , lm loss 0.000 , avg batch size 53.6
2023-02-24 16:42:34,068 - 4:05:26 - 172.6s - INFO - __main__ - epoch 6/8 done , tot steps 12889 , lr 1.6E-05 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 53.5
2023-02-24 17:02:02,910 - 4:24:55 - 1168.8s - INFO - __main__ - progress 6.466 , lr 1.2E-05 , loss 0.009 , qa loss 0.009 , lm loss 0.000 , avg batch size 53.6
2023-02-24 17:20:51,741 - 4:43:44 - 1128.8s - INFO - __main__ - progress 6.930 , lr 8.4E-06 , loss 0.009 , qa loss 0.009 , lm loss 0.000 , avg batch size 53.5
2023-02-24 17:23:40,774 - 4:46:33 - 169.0s - INFO - __main__ - epoch 7/8 done , tot steps 15041 , lr 7.9E-06 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 53.4
2023-02-24 17:42:49,227 - 5:05:41 - 1148.5s - INFO - __main__ - progress 7.464 , lr 4.2E-06 , loss 0.009 , qa loss 0.009 , lm loss 0.000 , avg batch size 53.4
2023-02-24 18:01:28,566 - 5:24:20 - 1119.3s - INFO - __main__ - progress 7.930 , lr 5.9E-07 , loss 0.008 , qa loss 0.008 , lm loss 0.000 , avg batch size 53.5
2023-02-24 18:04:23,343 - 5:27:15 - 174.8s - INFO - __main__ - epoch 8/8 done , tot steps 17193 , lr 3.9E-08 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 53.4
2023-02-24 18:04:32,371 - 5:27:24 - 9.0s - INFO - __main__ - start to train { task: ['amazon'], seq train type: lll }
2023-02-24 18:04:50,883 - 5:27:43 - 18.5s - INFO - utils - writing extra data in models/gpt2/lll/dbpedia_amazon_ag_sst_srl_woz.en_0.0/dbpedia/lm.csv ...
2023-02-24 18:04:50,883 - 5:27:43 - 0.0s - INFO - __main__ - extra training data size: 0
2023-02-24 18:04:50,995 - 5:27:43 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[dbpedia]
The task with which model is saved dbpedia
2023-02-24 18:06:05,019 - 5:28:57 - 74.0s - INFO - __main__ - len of train dataset: 115000 , max train batch size 76 , num of opt steps: 920000
2023-02-24 18:25:12,547 - 5:48:04 - 1147.5s - INFO - __main__ - progress 0.353 , lr 6.0E-05 , loss 1.955 , qa loss 1.955 , lm loss 0.000 , avg batch size 40.6
2023-02-24 18:43:29,071 - 6:06:21 - 1096.5s - INFO - __main__ - progress 0.703 , lr 5.7E-05 , loss 1.205 , qa loss 1.205 , lm loss 0.000 , avg batch size 40.4
2023-02-24 18:58:54,339 - 6:21:46 - 925.3s - INFO - __main__ - epoch 1/8 done , tot steps 2840 , lr 5.5E-05 , loss 0.98 , qa loss 0.98 , lm loss 0.00 , avg batch size 40.5
2023-02-24 19:17:59,461 - 6:40:51 - 1145.1s - INFO - __main__ - progress 1.352 , lr 5.2E-05 , loss 0.420 , qa loss 0.420 , lm loss 0.000 , avg batch size 40.5
2023-02-24 19:36:01,394 - 6:58:53 - 1081.9s - INFO - __main__ - progress 1.704 , lr 4.9E-05 , loss 0.416 , qa loss 0.416 , lm loss 0.000 , avg batch size 40.5
2023-02-24 19:51:37,324 - 7:14:29 - 935.9s - INFO - __main__ - epoch 2/8 done , tot steps 5682 , lr 4.7E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 40.5
2023-02-24 20:10:49,013 - 7:33:41 - 1151.7s - INFO - __main__ - progress 2.352 , lr 4.4E-05 , loss 0.397 , qa loss 0.397 , lm loss 0.000 , avg batch size 40.5
2023-02-24 20:30:15,496 - 7:53:07 - 1166.5s - INFO - __main__ - progress 2.704 , lr 4.1E-05 , loss 0.396 , qa loss 0.396 , lm loss 0.000 , avg batch size 40.5
2023-02-24 20:46:51,903 - 8:09:44 - 996.4s - INFO - __main__ - epoch 3/8 done , tot steps 8527 , lr 3.9E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 40.4
2023-02-24 21:07:23,816 - 8:30:16 - 1231.9s - INFO - __main__ - progress 3.352 , lr 3.6E-05 , loss 0.383 , qa loss 0.383 , lm loss 0.000 , avg batch size 40.5
2023-02-24 21:26:31,226 - 8:49:23 - 1147.4s - INFO - __main__ - progress 3.705 , lr 3.4E-05 , loss 0.382 , qa loss 0.382 , lm loss 0.000 , avg batch size 40.5
2023-02-24 21:42:56,964 - 9:05:49 - 985.7s - INFO - __main__ - epoch 4/8 done , tot steps 11374 , lr 3.1E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 40.4
2023-02-24 22:03:18,579 - 9:26:10 - 1221.6s - INFO - __main__ - progress 4.352 , lr 2.9E-05 , loss 0.373 , qa loss 0.373 , lm loss 0.000 , avg batch size 40.4
2023-02-24 22:21:29,462 - 9:44:21 - 1090.9s - INFO - __main__ - progress 4.703 , lr 2.6E-05 , loss 0.372 , qa loss 0.372 , lm loss 0.000 , avg batch size 40.4
2023-02-24 22:37:06,804 - 9:59:59 - 937.3s - INFO - __main__ - epoch 5/8 done , tot steps 14217 , lr 2.3E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 40.5
2023-02-24 22:56:08,805 - 10:19:01 - 1142.0s - INFO - __main__ - progress 5.351 , lr 2.1E-05 , loss 0.368 , qa loss 0.368 , lm loss 0.000 , avg batch size 40.4
2023-02-24 23:14:21,439 - 10:37:13 - 1092.6s - INFO - __main__ - progress 5.703 , lr 1.8E-05 , loss 0.366 , qa loss 0.366 , lm loss 0.000 , avg batch size 40.4
2023-02-24 23:29:55,113 - 10:52:47 - 933.7s - INFO - __main__ - epoch 6/8 done , tot steps 17060 , lr 1.6E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 40.5
2023-02-24 23:48:45,007 - 11:11:37 - 1129.9s - INFO - __main__ - progress 6.352 , lr 1.3E-05 , loss 0.362 , qa loss 0.362 , lm loss 0.000 , avg batch size 40.4
2023-02-25 00:07:14,415 - 11:30:06 - 1109.4s - INFO - __main__ - progress 6.702 , lr 1.0E-05 , loss 0.361 , qa loss 0.361 , lm loss 0.000 , avg batch size 40.4
2023-02-25 00:22:50,126 - 11:45:42 - 935.7s - INFO - __main__ - epoch 7/8 done , tot steps 19908 , lr 7.9E-06 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 40.4
2023-02-25 00:42:00,577 - 12:04:52 - 1150.5s - INFO - __main__ - progress 7.352 , lr 5.1E-06 , loss 0.357 , qa loss 0.357 , lm loss 0.000 , avg batch size 40.4
2023-02-25 01:00:18,191 - 12:23:10 - 1097.6s - INFO - __main__ - progress 7.703 , lr 2.4E-06 , loss 0.357 , qa loss 0.357 , lm loss 0.000 , avg batch size 40.4
2023-02-25 01:16:02,907 - 12:38:55 - 944.7s - INFO - __main__ - epoch 8/8 done , tot steps 22755 , lr 3.9E-08 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 40.4
2023-02-25 01:16:15,659 - 12:39:07 - 12.8s - INFO - __main__ - start to train { task: ['ag'], seq train type: lll }
2023-02-25 01:16:35,115 - 12:39:27 - 19.5s - INFO - utils - writing extra data in models/gpt2/lll/dbpedia_amazon_ag_sst_srl_woz.en_0.0/amazon/lm.csv ...
2023-02-25 01:16:35,116 - 12:39:27 - 0.0s - INFO - __main__ - extra training data size: 0
2023-02-25 01:16:35,848 - 12:39:28 - 0.7s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[dbpedia]
The task with which model is saved dbpedia
2023-02-25 01:17:36,366 - 12:40:28 - 60.5s - INFO - __main__ - len of train dataset: 115000 , max train batch size 76 , num of opt steps: 920000
2023-02-25 01:36:23,992 - 12:59:16 - 1127.6s - INFO - __main__ - progress 0.623 , lr 5.8E-05 , loss 1.282 , qa loss 1.282 , lm loss 0.000 , avg batch size 71.7
2023-02-25 01:47:04,539 - 13:09:56 - 640.5s - INFO - __main__ - epoch 1/8 done , tot steps 1603 , lr 5.5E-05 , loss 0.84 , qa loss 0.84 , lm loss 0.00 , avg batch size 71.7
2023-02-25 02:05:36,211 - 13:28:28 - 1111.7s - INFO - __main__ - progress 1.625 , lr 5.0E-05 , loss 0.099 , qa loss 0.099 , lm loss 0.000 , avg batch size 71.8
2023-02-25 02:16:18,544 - 13:39:10 - 642.3s - INFO - __main__ - epoch 2/8 done , tot steps 3207 , lr 4.7E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 71.7
2023-02-25 02:34:44,715 - 13:57:37 - 1106.2s - INFO - __main__ - progress 2.622 , lr 4.2E-05 , loss 0.080 , qa loss 0.080 , lm loss 0.000 , avg batch size 71.6
2023-02-25 02:45:30,108 - 14:08:22 - 645.4s - INFO - __main__ - epoch 3/8 done , tot steps 4811 , lr 3.9E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 71.7
2023-02-25 03:04:08,256 - 14:27:00 - 1118.1s - INFO - __main__ - progress 3.622 , lr 3.4E-05 , loss 0.071 , qa loss 0.071 , lm loss 0.000 , avg batch size 71.5
2023-02-25 03:14:55,174 - 14:37:47 - 646.9s - INFO - __main__ - epoch 4/8 done , tot steps 6416 , lr 3.1E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 71.7
2023-02-25 03:33:30,624 - 14:56:22 - 1115.5s - INFO - __main__ - progress 4.623 , lr 2.6E-05 , loss 0.067 , qa loss 0.067 , lm loss 0.000 , avg batch size 71.7
2023-02-25 03:44:23,521 - 15:07:15 - 652.9s - INFO - __main__ - epoch 5/8 done , tot steps 8020 , lr 2.3E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 71.7
2023-02-25 04:02:54,687 - 15:25:47 - 1111.2s - INFO - __main__ - progress 5.623 , lr 1.9E-05 , loss 0.063 , qa loss 0.063 , lm loss 0.000 , avg batch size 71.6
2023-02-25 04:13:47,425 - 15:36:39 - 652.7s - INFO - __main__ - epoch 6/8 done , tot steps 9626 , lr 1.6E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 71.6
2023-02-25 04:32:11,735 - 15:55:04 - 1104.3s - INFO - __main__ - progress 6.623 , lr 1.1E-05 , loss 0.061 , qa loss 0.061 , lm loss 0.000 , avg batch size 71.7
2023-02-25 04:43:07,377 - 16:05:59 - 655.6s - INFO - __main__ - epoch 7/8 done , tot steps 11231 , lr 7.9E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 71.7
2023-02-25 05:01:33,869 - 16:24:26 - 1106.5s - INFO - __main__ - progress 7.624 , lr 3.0E-06 , loss 0.058 , qa loss 0.058 , lm loss 0.000 , avg batch size 71.8
2023-02-25 05:12:20,063 - 16:35:12 - 646.2s - INFO - __main__ - epoch 8/8 done , tot steps 12838 , lr 3.9E-08 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 71.6
2023-02-25 05:12:28,185 - 16:35:20 - 8.1s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-02-25 05:12:49,784 - 16:35:42 - 21.6s - INFO - utils - writing extra data in models/gpt2/lll/dbpedia_amazon_ag_sst_srl_woz.en_0.0/ag/lm.csv ...
2023-02-25 05:12:49,785 - 16:35:42 - 0.0s - INFO - __main__ - extra training data size: 0
2023-02-25 05:12:50,701 - 16:35:43 - 0.9s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[dbpedia]
The task with which model is saved dbpedia
2023-02-25 05:13:20,826 - 16:36:13 - 30.1s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 55360
2023-02-25 05:20:55,833 - 16:43:48 - 455.0s - INFO - __main__ - progress 0.578 , lr 5.8E-05 , loss 1.988 , qa loss 1.988 , lm loss 0.000 , avg batch size 4.0
2023-02-25 05:26:07,900 - 16:49:00 - 312.1s - INFO - __main__ - epoch 1/8 done , tot steps 1730 , lr 5.5E-05 , loss 1.26 , qa loss 1.26 , lm loss 0.00 , avg batch size 4.0
2023-02-25 05:34:09,163 - 16:57:01 - 481.3s - INFO - __main__ - progress 1.578 , lr 5.0E-05 , loss 0.255 , qa loss 0.255 , lm loss 0.000 , avg batch size 4.0
2023-02-25 05:39:31,225 - 17:02:23 - 322.1s - INFO - __main__ - epoch 2/8 done , tot steps 3460 , lr 4.7E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-02-25 05:47:15,922 - 17:10:08 - 464.7s - INFO - __main__ - progress 2.578 , lr 4.2E-05 , loss 0.206 , qa loss 0.206 , lm loss 0.000 , avg batch size 4.0
2023-02-25 05:52:44,279 - 17:15:36 - 328.4s - INFO - __main__ - epoch 3/8 done , tot steps 5190 , lr 3.9E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-02-25 06:00:22,436 - 17:23:14 - 458.2s - INFO - __main__ - progress 3.578 , lr 3.5E-05 , loss 0.172 , qa loss 0.172 , lm loss 0.000 , avg batch size 4.0
2023-02-25 06:05:46,055 - 17:28:38 - 323.6s - INFO - __main__ - epoch 4/8 done , tot steps 6920 , lr 3.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-02-25 06:14:12,737 - 17:37:05 - 506.7s - INFO - __main__ - progress 4.578 , lr 2.7E-05 , loss 0.151 , qa loss 0.151 , lm loss 0.000 , avg batch size 4.0
2023-02-25 06:19:40,553 - 17:42:32 - 327.8s - INFO - __main__ - epoch 5/8 done , tot steps 8650 , lr 2.3E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-02-25 06:28:07,179 - 17:50:59 - 506.6s - INFO - __main__ - progress 5.578 , lr 1.9E-05 , loss 0.137 , qa loss 0.137 , lm loss 0.000 , avg batch size 4.0
2023-02-25 06:33:27,903 - 17:56:20 - 320.7s - INFO - __main__ - epoch 6/8 done , tot steps 10380 , lr 1.6E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-02-25 06:40:48,266 - 18:03:40 - 440.4s - INFO - __main__ - progress 6.578 , lr 1.1E-05 , loss 0.125 , qa loss 0.125 , lm loss 0.000 , avg batch size 4.0
2023-02-25 06:46:05,126 - 18:08:57 - 316.9s - INFO - __main__ - epoch 7/8 done , tot steps 12110 , lr 7.9E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-02-25 06:53:15,532 - 18:16:07 - 430.4s - INFO - __main__ - progress 7.578 , lr 3.3E-06 , loss 0.103 , qa loss 0.103 , lm loss 0.000 , avg batch size 4.0
2023-02-25 06:58:25,093 - 18:21:17 - 309.6s - INFO - __main__ - epoch 8/8 done , tot steps 13840 , lr 3.8E-08 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-02-25 06:58:29,070 - 18:21:21 - 4.0s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-02-25 06:58:49,690 - 18:21:42 - 20.6s - INFO - utils - writing extra data in models/gpt2/lll/dbpedia_amazon_ag_sst_srl_woz.en_0.0/sst/lm.csv ...
2023-02-25 06:58:49,691 - 18:21:42 - 0.0s - INFO - __main__ - extra training data size: 0
2023-02-25 06:58:50,234 - 18:21:42 - 0.5s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[dbpedia]
The task with which model is saved dbpedia
2023-02-25 06:59:17,174 - 18:22:09 - 26.9s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 51312
2023-02-25 07:06:58,611 - 18:29:50 - 461.4s - INFO - __main__ - progress 0.624 , lr 5.8E-05 , loss 3.066 , qa loss 3.066 , lm loss 0.000 , avg batch size 4.0
2023-02-25 07:11:30,156 - 18:34:22 - 271.5s - INFO - __main__ - epoch 1/8 done , tot steps 1604 , lr 5.5E-05 , loss 2.24 , qa loss 2.24 , lm loss 0.00 , avg batch size 4.0
2023-02-25 07:19:18,669 - 18:42:10 - 468.5s - INFO - __main__ - progress 1.624 , lr 5.0E-05 , loss 0.725 , qa loss 0.725 , lm loss 0.000 , avg batch size 4.0
2023-02-25 07:23:53,462 - 18:46:45 - 274.8s - INFO - __main__ - epoch 2/8 done , tot steps 3208 , lr 4.7E-05 , loss 0.72 , qa loss 0.72 , lm loss 0.00 , avg batch size 4.0
2023-02-25 07:32:05,218 - 18:54:57 - 491.8s - INFO - __main__ - progress 2.624 , lr 4.2E-05 , loss 0.602 , qa loss 0.602 , lm loss 0.000 , avg batch size 4.0
2023-02-25 07:36:32,175 - 18:59:24 - 267.0s - INFO - __main__ - epoch 3/8 done , tot steps 4812 , lr 3.9E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-02-25 07:44:05,066 - 19:06:57 - 452.9s - INFO - __main__ - progress 3.624 , lr 3.4E-05 , loss 0.500 , qa loss 0.500 , lm loss 0.000 , avg batch size 4.0
2023-02-25 07:48:33,978 - 19:11:26 - 268.9s - INFO - __main__ - epoch 4/8 done , tot steps 6416 , lr 3.1E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-02-25 07:56:27,197 - 19:19:19 - 473.2s - INFO - __main__ - progress 4.624 , lr 2.6E-05 , loss 0.463 , qa loss 0.463 , lm loss 0.000 , avg batch size 4.0
2023-02-25 08:01:00,693 - 19:23:53 - 273.5s - INFO - __main__ - epoch 5/8 done , tot steps 8020 , lr 2.3E-05 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-02-25 08:08:48,916 - 19:31:41 - 468.2s - INFO - __main__ - progress 5.624 , lr 1.9E-05 , loss 0.416 , qa loss 0.416 , lm loss 0.000 , avg batch size 4.0
2023-02-25 08:13:28,897 - 19:36:21 - 280.0s - INFO - __main__ - epoch 6/8 done , tot steps 9624 , lr 1.6E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-02-25 08:21:20,317 - 19:44:12 - 471.4s - INFO - __main__ - progress 6.624 , lr 1.1E-05 , loss 0.385 , qa loss 0.385 , lm loss 0.000 , avg batch size 4.0
2023-02-25 08:25:42,990 - 19:48:35 - 262.7s - INFO - __main__ - epoch 7/8 done , tot steps 11228 , lr 7.9E-06 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-02-25 08:33:48,952 - 19:56:41 - 486.0s - INFO - __main__ - progress 7.624 , lr 3.0E-06 , loss 0.364 , qa loss 0.364 , lm loss 0.000 , avg batch size 4.0
2023-02-25 08:38:18,210 - 20:01:10 - 269.3s - INFO - __main__ - epoch 8/8 done , tot steps 12832 , lr 3.9E-08 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-02-25 08:38:22,237 - 20:01:14 - 4.0s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-02-25 08:38:42,590 - 20:01:34 - 20.4s - INFO - utils - writing extra data in models/gpt2/lll/dbpedia_amazon_ag_sst_srl_woz.en_0.0/srl/lm.csv ...
2023-02-25 08:38:42,590 - 20:01:34 - 0.0s - INFO - __main__ - extra training data size: 0
2023-02-25 08:38:43,282 - 20:01:35 - 0.7s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[dbpedia]
The task with which model is saved dbpedia
2023-02-25 08:39:15,130 - 20:02:07 - 31.8s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 20288
2023-02-25 08:44:36,108 - 20:07:28 - 321.0s - INFO - __main__ - epoch 1/8 done , tot steps 634 , lr 5.5E-05 , loss 3.20 , qa loss 3.20 , lm loss 0.00 , avg batch size 4.0
2023-02-25 08:49:49,900 - 20:12:42 - 313.8s - INFO - __main__ - epoch 2/8 done , tot steps 1268 , lr 4.7E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-02-25 08:54:59,834 - 20:17:52 - 309.9s - INFO - __main__ - epoch 3/8 done , tot steps 1902 , lr 3.9E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-02-25 08:59:57,293 - 20:22:49 - 297.5s - INFO - __main__ - epoch 4/8 done , tot steps 2536 , lr 3.1E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-02-25 09:05:05,987 - 20:27:58 - 308.7s - INFO - __main__ - epoch 5/8 done , tot steps 3170 , lr 2.3E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-02-25 09:10:17,541 - 20:33:09 - 311.6s - INFO - __main__ - epoch 6/8 done , tot steps 3804 , lr 1.6E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-02-25 09:15:22,796 - 20:38:15 - 305.3s - INFO - __main__ - epoch 7/8 done , tot steps 4438 , lr 7.8E-06 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-02-25 09:20:22,817 - 20:43:15 - 300.0s - INFO - __main__ - epoch 8/8 done , tot steps 5072 , lr 3.7E-08 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[dbpedia]
The task with which model is saved dbpedia
Wall Execution time: 20:42:59
CPU Execution time: 09:09:45
